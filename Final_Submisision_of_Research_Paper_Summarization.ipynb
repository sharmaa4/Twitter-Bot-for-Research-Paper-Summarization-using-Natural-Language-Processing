{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Submisision of Research Paper Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hOqnek8xB5d7",
        "0SJIUt87PdU7",
        "vrFVaeCu1K_S"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "caeaadb09f824f72bde91e2a4486ac4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_96be88265737406282cb89c8d2c67d5d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9b7445d577b443c1ad4241be62b7fa95",
              "IPY_MODEL_b31f1439fb4f427e815c2bcf04c814a2",
              "IPY_MODEL_608779c715ef4e04896c36b820c7e084"
            ]
          }
        },
        "96be88265737406282cb89c8d2c67d5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9b7445d577b443c1ad4241be62b7fa95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6e8d345bbd1d41bc92ba3792745c5724",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b99f932e4f35409bafdb244b0486003b"
          }
        },
        "b31f1439fb4f427e815c2bcf04c814a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f9972cbf227045239cf7a728cc3a34db",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1009,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1009,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1cd8db5b7934eb886519efdf9e77b70"
          }
        },
        "608779c715ef4e04896c36b820c7e084": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_848881f04a8449379056e87b8a2acd5a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1009/1009 [17:06&lt;00:00,  1.03it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_034bacd0f4d54a0588f47d52d2f5bec3"
          }
        },
        "6e8d345bbd1d41bc92ba3792745c5724": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b99f932e4f35409bafdb244b0486003b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f9972cbf227045239cf7a728cc3a34db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1cd8db5b7934eb886519efdf9e77b70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "848881f04a8449379056e87b8a2acd5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "034bacd0f4d54a0588f47d52d2f5bec3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d76f27eb79524f0abeacb732099e08cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d16fddf0261c483bad5b7ed72afd66b8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dbe21da0cf124579975a44dd5cbc8b85",
              "IPY_MODEL_fff56ae7ca8649faa9ef8a0f920f8f1f"
            ]
          }
        },
        "d16fddf0261c483bad5b7ed72afd66b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dbe21da0cf124579975a44dd5cbc8b85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d12261660a8f4b62a76d0edee9892ba0",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 5,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d08b79ce299145539b4f293e6c174221"
          }
        },
        "fff56ae7ca8649faa9ef8a0f920f8f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1ed545a19c0342b6b155354bf8cd82bb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/5 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_19235ce6da0c49a4b0bc3f26c8f425e6"
          }
        },
        "d12261660a8f4b62a76d0edee9892ba0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d08b79ce299145539b4f293e6c174221": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1ed545a19c0342b6b155354bf8cd82bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "19235ce6da0c49a4b0bc3f26c8f425e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharmaa4/Twitter-Bot-for-Research-Paper-Summarization-using-Natural-Language-Processing/blob/main/Final_Submisision_of_Research_Paper_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVFb1IyfBCN2"
      },
      "source": [
        "## SETUP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B72svn1EoNVg",
        "outputId": "5f96bf53-24ac-4987-af7f-07357fe1e021"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzR4OlOaK4Tg"
      },
      "source": [
        "!pip install -Uq fastbook\n",
        "!rm -rf /content/pdfminer.six\n",
        "!pip install pdfminer.six\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "import tweepy\n",
        "import webbrowser\n",
        "import time\n",
        "\n",
        "from lxml import etree\n",
        "from tqdm.auto import tqdm\n",
        "from fastai.text.all import *\n",
        "!git clone \"https://github.com/pdfminer/pdfminer.six\"\n",
        "!pip install pdfminer.six\n",
        "!pip install -Uq transformers\n",
        "!pip install -r '/content/drive/MyDrive/Colab Notebooks/requirements.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2yi0oSEBYIV"
      },
      "source": [
        "## XML PARSING TO CONVERT INPUT DATA IN XML FORMAT TO STRINGS AND APPENDING THEM INTO A CSV FILE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRkkkdyeMbam"
      },
      "source": [
        "root_path = '/content/drive/MyDrive/Colab Notebooks/scisummnet_release1.1__20190413/top1000_complete'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt1fDtV9MFTV"
      },
      "source": [
        "pathlist = os.listdir(root_path)\n",
        "# for folder in pathlist:\n",
        "#   print(os.listdir(root_path+'/'+folder+'/summary'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q-0lIyTMN07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "caeaadb09f824f72bde91e2a4486ac4d",
            "96be88265737406282cb89c8d2c67d5d",
            "9b7445d577b443c1ad4241be62b7fa95",
            "b31f1439fb4f427e815c2bcf04c814a2",
            "608779c715ef4e04896c36b820c7e084",
            "6e8d345bbd1d41bc92ba3792745c5724",
            "b99f932e4f35409bafdb244b0486003b",
            "f9972cbf227045239cf7a728cc3a34db",
            "e1cd8db5b7934eb886519efdf9e77b70",
            "848881f04a8449379056e87b8a2acd5a",
            "034bacd0f4d54a0588f47d52d2f5bec3"
          ]
        },
        "outputId": "9b3e3a06-da2c-401a-a343-c797b8697ce4"
      },
      "source": [
        "parser = etree.XMLParser(remove_blank_text=True)\n",
        "\n",
        "# papers\n",
        "pap_text = []\n",
        "summaries = []\n",
        "for folder in tqdm(pathlist):\n",
        "  tree = etree.parse(root_path+'/'+folder+'/Documents_xml/'+folder+'.xml', parser)\n",
        "  paper = []\n",
        "  for s in tree.iter():\n",
        "    if s.text is not None: paper.append(s.text)\n",
        "\n",
        "  with open(root_path+'/'+folder+'/summary/'+folder+'.gold.txt') as f:\n",
        "    lines = f.read()\n",
        "\n",
        "  pap_text.append(' '.join(paper))\n",
        "  summaries.append(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "caeaadb09f824f72bde91e2a4486ac4d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1009 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS6Dg7_qzvtJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cace5eb3-0854-4fa7-d26c-6b3ca63599ee"
      },
      "source": [
        "scis_df = pd.DataFrame({'text':pap_text, 'summary':summaries})\n",
        "type(scis_df['text'].iloc[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiVMYFjZ9Wvt"
      },
      "source": [
        "scis_df.to_csv('./scisumm.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO_fJEpUr-wQ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Manual train-test split\n",
        "np.random.seed(10) # seed added \n",
        "msk = np.random.rand(len(scis_df)) < 0.8\n",
        "msk\n",
        "train_df = scis_df[msk]\n",
        "test_df = scis_df[~msk]\n",
        "train_df.to_csv('/content/drive/MyDrive/Colab Notebooks/scisumm_train.csv')\n",
        "test_df.to_csv('/content/drive/MyDrive/Colab Notebooks/scisumm_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tzdTcpaPzxU"
      },
      "source": [
        "## FINE-TUNING BART WITH HUGGINGFACE SCRIPT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7JPDvoJ8-S8",
        "outputId": "73eea2b8-e9bb-4121-e1f0-0d6fc6b92337"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "os.chdir(\"/content/transformers\")\n",
        "!pwd\n",
        "!pip install .\n",
        "\n",
        "!python '/content/drive/MyDrive/Colab Notebooks/run_summarization.py' \\\n",
        "    --model_name_or_path facebook/bart-large-cnn \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --train_file '/content/drive/MyDrive/Colab Notebooks/scisumm_train.csv' \\\n",
        "    --validation_file '/content/drive/MyDrive/Colab Notebooks/scisumm_test.csv' \\\n",
        "    --text_column text \\\n",
        "    --summary_column summary \\\n",
        "    --source_prefix \"summarize: \" \\\n",
        "    --output_dir \"/content/drive/MyDrive/Colab Notebooks/Summarization Output/\" \\\n",
        "    --overwrite_output_dir \\\n",
        "    --per_device_train_batch_size=1 \\\n",
        "    --per_device_eval_batch_size=1 \\\n",
        "    --predict_with_generate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 79107, done.\u001b[K\n",
            "remote: Counting objects: 100% (805/805), done.\u001b[K\n",
            "remote: Compressing objects: 100% (472/472), done.\u001b[K\n",
            "remote: Total 79107 (delta 434), reused 567 (delta 284), pack-reused 78302\u001b[K\n",
            "Receiving objects: 100% (79107/79107), 62.68 MiB | 24.93 MiB/s, done.\n",
            "Resolving deltas: 100% (56408/56408), done.\n",
            "/content/transformers\n",
            "Processing /content/transformers\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (0.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (4.6.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (4.61.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (5.4.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (0.10.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers==4.10.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.10.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.10.0.dev0) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (7.1.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.10.0.dev0-py3-none-any.whl size=2623951 sha256=abe638731363d1ea6ca4d642268f8665020ced39007532797d5c85f94588299f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-m5ek1hor/wheels/49/62/f4/6730819eed4e6468662b1519bf3bf46419b2335990c77f8767\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.10.0.dev0\n",
            "    Uninstalling transformers-4.10.0.dev0:\n",
            "      Successfully uninstalled transformers-4.10.0.dev0\n",
            "Successfully installed transformers-4.10.0.dev0\n",
            "2021-07-28 17:59:43.117617: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "07/28/2021 17:59:45 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/28/2021 17:59:45 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/drive/MyDrive/Colab Notebooks/Summarization Output/runs/Jul28_17-59-45_225ff91bef62,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=/content/drive/MyDrive/Colab Notebooks/Summarization Output/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=1,\n",
            "per_device_train_batch_size=1,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=Summarization Output,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/drive/MyDrive/Colab Notebooks/Summarization Output/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "07/28/2021 17:59:45 - WARNING - datasets.builder - Using custom data configuration default-dc9bfa63384c4a22\n",
            "07/28/2021 17:59:45 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-dc9bfa63384c4a22/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-dc9bfa63384c4a22/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\n",
            "100% 2/2 [00:00<00:00, 4586.45it/s]\n",
            "07/28/2021 17:59:45 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "07/28/2021 17:59:47 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "100% 2/2 [00:00<00:00, 130.57it/s]\n",
            "07/28/2021 17:59:47 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "07/28/2021 17:59:47 - INFO - datasets.builder - Generating split train\n",
            "07/28/2021 17:59:47 - INFO - datasets.builder - Generating split validation\n",
            "07/28/2021 17:59:47 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-dc9bfa63384c4a22/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 532.20it/s]\n",
            "[INFO|file_utils.py:1624] 2021-07-28 17:59:48,272 >> https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_vflo90f\n",
            "Downloading: 100% 1.58k/1.58k [00:00<00:00, 1.61MB/s]\n",
            "[INFO|file_utils.py:1628] 2021-07-28 17:59:48,554 >> storing https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.930264180d256e6fe8e4ba6a728dd80e969493c23d4caa0a6f943614c52d34ab\n",
            "[INFO|file_utils.py:1636] 2021-07-28 17:59:48,554 >> creating metadata file for /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.930264180d256e6fe8e4ba6a728dd80e969493c23d4caa0a6f943614c52d34ab\n",
            "[INFO|configuration_utils.py:545] 2021-07-28 17:59:48,554 >> loading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.930264180d256e6fe8e4ba6a728dd80e969493c23d4caa0a6f943614c52d34ab\n",
            "[INFO|configuration_utils.py:581] 2021-07-28 17:59:48,555 >> Model config BartConfig {\n",
            "  \"_num_labels\": 3,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"force_bos_token_to_be_generated\": true,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \" \",\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.10.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:432] 2021-07-28 17:59:48,846 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:545] 2021-07-28 17:59:49,129 >> loading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.930264180d256e6fe8e4ba6a728dd80e969493c23d4caa0a6f943614c52d34ab\n",
            "[INFO|configuration_utils.py:581] 2021-07-28 17:59:49,129 >> Model config BartConfig {\n",
            "  \"_num_labels\": 3,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"force_bos_token_to_be_generated\": true,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \" \",\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.10.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1624] 2021-07-28 17:59:49,704 >> https://huggingface.co/facebook/bart-large-cnn/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpgl_nd7z1\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 4.52MB/s]\n",
            "[INFO|file_utils.py:1628] 2021-07-28 17:59:50,191 >> storing https://huggingface.co/facebook/bart-large-cnn/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/4d8eeedc3498bc73a4b72411ebb3219209b305663632d77a6f16e60790b18038.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|file_utils.py:1636] 2021-07-28 17:59:50,192 >> creating metadata file for /root/.cache/huggingface/transformers/4d8eeedc3498bc73a4b72411ebb3219209b305663632d77a6f16e60790b18038.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|file_utils.py:1624] 2021-07-28 17:59:50,475 >> https://huggingface.co/facebook/bart-large-cnn/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpsp8gfjgx\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 3.39MB/s]\n",
            "[INFO|file_utils.py:1628] 2021-07-28 17:59:50,900 >> storing https://huggingface.co/facebook/bart-large-cnn/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/0ddddd3ca9e107b17a6901c92543692272af1c3238a8d7549fa937ba0057bbcf.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1636] 2021-07-28 17:59:50,901 >> creating metadata file for /root/.cache/huggingface/transformers/0ddddd3ca9e107b17a6901c92543692272af1c3238a8d7549fa937ba0057bbcf.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1624] 2021-07-28 17:59:51,181 >> https://huggingface.co/facebook/bart-large-cnn/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpni841110\n",
            "Downloading: 100% 1.36M/1.36M [00:00<00:00, 5.24MB/s]\n",
            "[INFO|file_utils.py:1628] 2021-07-28 17:59:51,732 >> storing https://huggingface.co/facebook/bart-large-cnn/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/55c96bd962ce1d360fde4947619318f1b4eb551430de678044699cbfeb99de6a.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|file_utils.py:1636] 2021-07-28 17:59:51,733 >> creating metadata file for /root/.cache/huggingface/transformers/55c96bd962ce1d360fde4947619318f1b4eb551430de678044699cbfeb99de6a.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1730] 2021-07-28 17:59:52,581 >> loading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/4d8eeedc3498bc73a4b72411ebb3219209b305663632d77a6f16e60790b18038.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|tokenization_utils_base.py:1730] 2021-07-28 17:59:52,581 >> loading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/0ddddd3ca9e107b17a6901c92543692272af1c3238a8d7549fa937ba0057bbcf.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1730] 2021-07-28 17:59:52,581 >> loading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/55c96bd962ce1d360fde4947619318f1b4eb551430de678044699cbfeb99de6a.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1730] 2021-07-28 17:59:52,581 >> loading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1730] 2021-07-28 17:59:52,581 >> loading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1730] 2021-07-28 17:59:52,581 >> loading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:545] 2021-07-28 17:59:52,863 >> loading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.930264180d256e6fe8e4ba6a728dd80e969493c23d4caa0a6f943614c52d34ab\n",
            "[INFO|configuration_utils.py:581] 2021-07-28 17:59:52,864 >> Model config BartConfig {\n",
            "  \"_num_labels\": 3,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"force_bos_token_to_be_generated\": true,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \" \",\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.10.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1624] 2021-07-28 17:59:53,209 >> https://huggingface.co/facebook/bart-large-cnn/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9gsz9qgj\n",
            "Downloading: 100% 1.63G/1.63G [00:28<00:00, 57.9MB/s]\n",
            "[INFO|file_utils.py:1628] 2021-07-28 18:00:21,369 >> storing https://huggingface.co/facebook/bart-large-cnn/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300\n",
            "[INFO|file_utils.py:1636] 2021-07-28 18:00:21,370 >> creating metadata file for /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300\n",
            "[INFO|modeling_utils.py:1271] 2021-07-28 18:00:21,370 >> loading weights file https://huggingface.co/facebook/bart-large-cnn/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300\n",
            "[INFO|modeling_utils.py:1510] 2021-07-28 18:00:26,347 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:1519] 2021-07-28 18:00:26,347 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "Running tokenizer on train dataset:   0% 0/1 [00:00<?, ?ba/s]07/28/2021 18:00:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-dc9bfa63384c4a22/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-0609970123606afd.arrow\n",
            "Running tokenizer on train dataset: 100% 1/1 [00:10<00:00, 10.80s/ba]\n",
            "Running tokenizer on validation dataset:   0% 0/1 [00:00<?, ?ba/s]07/28/2021 18:00:40 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-dc9bfa63384c4a22/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-80b9fee2d0325785.arrow\n",
            "Running tokenizer on validation dataset: 100% 1/1 [00:02<00:00,  2.59s/ba]\n",
            "07/28/2021 18:00:41 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.10.2/metrics/rouge/rouge.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpw3gj779x\n",
            "Downloading: 5.61kB [00:00, 4.26MB/s]       \n",
            "07/28/2021 18:00:41 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.10.2/metrics/rouge/rouge.py in cache at /root/.cache/huggingface/datasets/downloads/a08577893ed709b70094fba5c05f36c6447c55ceaeecdf7cd8c21db3e1743909.0176a9ea14f2e537f6dfc6a072a422e7075a0cf475bb3b71e493addc7d8dde62.py\n",
            "07/28/2021 18:00:41 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a08577893ed709b70094fba5c05f36c6447c55ceaeecdf7cd8c21db3e1743909.0176a9ea14f2e537f6dfc6a072a422e7075a0cf475bb3b71e493addc7d8dde62.py\n",
            "07/28/2021 18:00:41 - INFO - datasets.load - Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.10.2/metrics/rouge/rouge.py at /root/.cache/huggingface/modules/datasets_modules/metrics/rouge\n",
            "07/28/2021 18:00:41 - INFO - datasets.load - Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.10.2/metrics/rouge/rouge.py at /root/.cache/huggingface/modules/datasets_modules/metrics/rouge/2b73d5eb463209373e9d21a95decb226d4164bdca4c361b8dfad295ec82bc62e\n",
            "07/28/2021 18:00:41 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.10.2/metrics/rouge/rouge.py to /root/.cache/huggingface/modules/datasets_modules/metrics/rouge/2b73d5eb463209373e9d21a95decb226d4164bdca4c361b8dfad295ec82bc62e/rouge.py\n",
            "07/28/2021 18:00:41 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.10.2/metrics/rouge/dataset_infos.json\n",
            "07/28/2021 18:00:41 - INFO - datasets.load - Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.10.2/metrics/rouge/rouge.py at /root/.cache/huggingface/modules/datasets_modules/metrics/rouge/2b73d5eb463209373e9d21a95decb226d4164bdca4c361b8dfad295ec82bc62e/rouge.json\n",
            "[INFO|trainer.py:1164] 2021-07-28 18:00:53,924 >> ***** Running training *****\n",
            "[INFO|trainer.py:1165] 2021-07-28 18:00:53,924 >>   Num examples = 805\n",
            "[INFO|trainer.py:1166] 2021-07-28 18:00:53,924 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1167] 2021-07-28 18:00:53,924 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:1168] 2021-07-28 18:00:53,924 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:1169] 2021-07-28 18:00:53,924 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1170] 2021-07-28 18:00:53,924 >>   Total optimization steps = 2415\n",
            "{'loss': 0.7611, 'learning_rate': 3.9648033126293996e-05, 'epoch': 0.62}\n",
            " 21% 500/2415 [05:37<22:24,  1.42it/s][INFO|trainer.py:1919] 2021-07-28 18:06:31,858 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Summarization Output/checkpoint-500\n",
            "[INFO|configuration_utils.py:379] 2021-07-28 18:06:31,863 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/Summarization Output/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:997] 2021-07-28 18:06:41,142 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/Summarization Output/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2006] 2021-07-28 18:06:41,147 >> tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/Summarization Output/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2012] 2021-07-28 18:06:41,150 >> Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/Summarization Output/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.5638, 'learning_rate': 2.9296066252587996e-05, 'epoch': 1.24}\n",
            " 41% 1000/2415 [12:58<16:28,  1.43it/s][INFO|trainer.py:1919] 2021-07-28 18:13:52,909 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Summarization Output/checkpoint-1000\n",
            "[INFO|configuration_utils.py:379] 2021-07-28 18:13:52,914 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/Summarization Output/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:997] 2021-07-28 18:14:02,741 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/Summarization Output/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2006] 2021-07-28 18:14:02,788 >> tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/Summarization Output/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2012] 2021-07-28 18:14:02,792 >> Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/Summarization Output/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.4436, 'learning_rate': 1.894409937888199e-05, 'epoch': 1.86}\n",
            " 62% 1500/2415 [20:19<10:40,  1.43it/s][INFO|trainer.py:1919] 2021-07-28 18:21:13,454 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Summarization Output/checkpoint-1500\n",
            "[INFO|configuration_utils.py:379] 2021-07-28 18:21:13,461 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/Summarization Output/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:997] 2021-07-28 18:21:27,832 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/Summarization Output/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2006] 2021-07-28 18:21:28,147 >> tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/Summarization Output/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2012] 2021-07-28 18:21:28,151 >> Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/Summarization Output/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 0.2807, 'learning_rate': 8.592132505175984e-06, 'epoch': 2.48}\n",
            " 83% 2000/2415 [27:37<04:52,  1.42it/s][INFO|trainer.py:1919] 2021-07-28 18:28:31,861 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Summarization Output/checkpoint-2000\n",
            "[INFO|configuration_utils.py:379] 2021-07-28 18:28:31,867 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/Summarization Output/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:997] 2021-07-28 18:28:45,931 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/Summarization Output/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2006] 2021-07-28 18:28:45,938 >> tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/Summarization Output/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2012] 2021-07-28 18:28:45,944 >> Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/Summarization Output/checkpoint-2000/special_tokens_map.json\n",
            "100% 2415/2415 [33:56<00:00,  1.43it/s][INFO|trainer.py:1360] 2021-07-28 18:34:50,859 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2036.9362, 'train_samples_per_second': 1.186, 'train_steps_per_second': 1.186, 'train_loss': 0.46862718412347953, 'epoch': 3.0}\n",
            "100% 2415/2415 [33:56<00:00,  1.19it/s]\n",
            "[INFO|trainer.py:1919] 2021-07-28 18:34:50,892 >> Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Summarization Output/\n",
            "[INFO|configuration_utils.py:379] 2021-07-28 18:34:50,900 >> Configuration saved in /content/drive/MyDrive/Colab Notebooks/Summarization Output/config.json\n",
            "[INFO|modeling_utils.py:997] 2021-07-28 18:35:05,346 >> Model weights saved in /content/drive/MyDrive/Colab Notebooks/Summarization Output/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2006] 2021-07-28 18:35:05,352 >> tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/Summarization Output/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2012] 2021-07-28 18:35:05,357 >> Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/Summarization Output/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.4686\n",
            "  train_runtime            = 0:33:56.93\n",
            "  train_samples            =        805\n",
            "  train_samples_per_second =      1.186\n",
            "  train_steps_per_second   =      1.186\n",
            "07/28/2021 18:35:05 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2165] 2021-07-28 18:35:05,660 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2167] 2021-07-28 18:35:05,661 >>   Num examples = 204\n",
            "[INFO|trainer.py:2170] 2021-07-28 18:35:05,661 >>   Batch size = 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "100% 204/204 [06:56<00:00,  2.03s/it]07/28/2021 18:42:08 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/rouge/default/default_experiment-1-0.arrow\n",
            "100% 204/204 [07:00<00:00,  2.06s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_gen_len            =   122.4853\n",
            "  eval_loss               =     0.6638\n",
            "  eval_rouge1             =    86.8247\n",
            "  eval_rouge2             =    82.5522\n",
            "  eval_rougeL             =    85.4232\n",
            "  eval_rougeLsum          =    86.5051\n",
            "  eval_runtime            = 0:07:02.54\n",
            "  eval_samples            =        204\n",
            "  eval_samples_per_second =      0.483\n",
            "  eval_steps_per_second   =      0.483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOqnek8xB5d7"
      },
      "source": [
        "## MEASURING PERFORMANCE OF FINE-TUNED MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvAg5pG3zCfU"
      },
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
        "\n",
        "PATH = \"/content/drive/MyDrive/Colab Notebooks/Summarization Output/\"\n",
        "#model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "model = BartForConditionalGeneration.from_pretrained(PATH, local_files_only=True)\n",
        "#tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "tokenizer = BartTokenizer.from_pretrained(PATH, local_files_only=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28qOOJfcOS30"
      },
      "source": [
        "gen_summs = []\n",
        "for paper in tqdm(test_df.text.values[:203]):\n",
        "#for paper in tqdm(scis_df.text.values[:5]):\n",
        "\n",
        "  ARTICLE_TO_SUMMARIZE = paper\n",
        "  inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt', truncation=True)\n",
        "\n",
        "  # Generate Summary\n",
        "  summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=500, early_stopping=True)\n",
        "  gen_summs.append([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])\n",
        "\n",
        "gen_df = pd.DataFrame({'generated_summary':gen_summs})\n",
        "gen_df.head()\n",
        "\n",
        "gen_df.generated_summary = gen_df.generated_summary.apply(lambda x: x[0])\n",
        "gen_df.head()\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge2', 'rouge3'])\n",
        "scores = []\n",
        "#mod: sharmaa4 for i in range(500):\n",
        "for i in range(203):\n",
        "  \n",
        "  scores.append(scorer.score(test_df.iloc[i].summary, gen_df.iloc[i].generated_summary))\n",
        "\n",
        "\n",
        "r2r = []\n",
        "r2f = []\n",
        "r3f = []\n",
        "#for i in range(250):\n",
        "for i in range(203):\n",
        "  r2r.append(scores[i]['rouge2'].recall)\n",
        "  r2f.append(scores[i]['rouge2'].fmeasure)\n",
        "  r3f.append(scores[i]['rouge3'].fmeasure)\n",
        "\n",
        "\n",
        "print('rouge2 - recall: '+ str(np.mean(r2r)))\n",
        "print('rouge2 - fmeasure: '+ str(np.mean(r2f)))\n",
        "print('rouge3 - fmeasure: '+ str(np.mean(r3f)))\n",
        "\n",
        "gen_df.to_csv('./nonpt_first500.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU6MRw9KCOCF"
      },
      "source": [
        "## FUNCTION TO GENERATE SUMMARY "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZAjF2U5OG_X"
      },
      "source": [
        "os.chdir(\"/content\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1El7El2NjLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70531804-8ba6-4037-fac0-07c3d7dafd33"
      },
      "source": [
        "!git clone \"https://github.com/pdfminer/pdfminer.six\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'pdfminer.six' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlOn7sCHwRqb"
      },
      "source": [
        "def gen_pdf_summary(url):\n",
        "\n",
        "  print(url)\n",
        "  !curl -o PDF {url}\n",
        "  !python /content/pdfminer.six/tools/pdf2txt.py PDF > ./PDF_text\n",
        "\n",
        "  with open(\"./PDF_text\",'r') as file:\n",
        "    text = file.read()\n",
        "  \n",
        "  inputs_1 = tokenizer(text[0:1024], max_length=1024, return_tensors='pt', truncation=True)\n",
        "  inputs_2 = tokenizer(text[1025:2048], max_length=1024, return_tensors='pt', truncation=True)\n",
        "  inputs_3 = tokenizer(text[2048:3072], max_length=1024, return_tensors='pt', truncation=True)\n",
        "  inputs_4 = tokenizer(text[3072:], max_length=1024, return_tensors='pt', truncation=True)\n",
        "\n",
        "  # Generate Summary\n",
        "  summary_ids_1 = model.generate(inputs_1['input_ids'], num_beams=4, max_length=100, early_stopping=True)\n",
        "  summary_1 = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids_1]\n",
        "\n",
        "  summary_ids_2 = model.generate(inputs_2['input_ids'], num_beams=4, max_length=100, early_stopping=True)\n",
        "  summary_2 = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids_2]\n",
        "\n",
        "  summary_ids_3 = model.generate(inputs_3['input_ids'], num_beams=4, max_length=100, early_stopping=True)\n",
        "  summary_3 = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids_3]\n",
        "\n",
        "  summary_ids_4 = model.generate(inputs_4['input_ids'], num_beams=4, max_length=100, early_stopping=True)\n",
        "  summary_4 = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids_4]\n",
        "  \n",
        "  return summary_1[0]+summary_2[0]+summary_3[0]+summary_4[0]\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "BnRJyOWU5zYM",
        "outputId": "8f3e0a25-a221-48f6-e64c-24ef4bde2b47"
      },
      "source": [
        "#gen_pdf_summary(\"https://arxiv.org/pdf/1909.01716v3.pdf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://arxiv.org/pdf/1909.01716v3.pdf\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  614k  100  614k    0     0   423k      0  0:00:01  0:00:01 --:--:--  423k\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ScisummNet: A Large Annotated Corpus and Content-Impact Models\\nFor Scientiﬁc Paper Summarization with Citation Networks\\nRecent work in automatic summarization has achieved good results for news articles: single-Document Single-Document (SDS) summaries (Parveen, Ramsl, and Strube) and traditional citation-based summaries.\\nOur large annotated corpus and hybrid methods provide a new framework for science-oriented paper summarization research.\\nWe demonstrate the effectiveness of our corpus in producing large-scale manually-annotated summaries and the advantage of'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI26ysHlCQvT"
      },
      "source": [
        "## TWITTER BOT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58beNTqmW3MG"
      },
      "source": [
        "# these are the necessary keys which we got from Twitter\n",
        "##consumer_key = \"Qm4Exmimzawm2WJ1wCIbBPDCK\"\n",
        "##consumer_secret = \"jDW4rd4sqwWM0t0nkEUXoRQnXYfpQjnJt4vqaxVhtfIaUb9PN9\"\n",
        "##access_token = \"1419176160127164417-hiXZsBeeONG7cUKDdkz28fvPlU5J6r\"\n",
        "##access_token_secret = \"kiyMn8EoCM3FbAF0jNHCBA59TtblJndRKmICwFv0ATIKW\"\n",
        "\n",
        "consumer_key = \"VXE0kwXAw2bUJ9dx5gnFNmLEG\"\n",
        "consumer_secret = \"LiebjWsYOcOXHWFsTTcfyxmPf13vWEc7xsgmrkVMJ4uXupkdI4\"\n",
        "access_token = \"1419176160127164417-C4b5obrAO2EpwFnWBKC6XtQ6SJWwrO\"\n",
        "access_token_secret = \"6WvekVxHlnQ6GWKSq1APhVPq3GpQg9gAd0NDBpzfP5k5V\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reJ-4ActW4Mq"
      },
      "source": [
        "# as of now we need to get tweets from this user, so we pick his user ID\n",
        "userID = \"ak92501\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc7n1fLYW7e0"
      },
      "source": [
        "# authorizing our twitter credentials\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth,wait_on_rate_limit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq5RtkFoW_nG"
      },
      "source": [
        "# user_timeline extends tweets from particular user\n",
        "tweets = api.user_timeline(screen_name=userID, \n",
        "                           # 200 is the maximum allowed count\n",
        "                           count=200,\n",
        "                           include_rts = True,\n",
        "                           tweet_mode = 'extended'\n",
        "                           )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "id": "XYqZa2_guJLF",
        "outputId": "e43edd75-ac83-4701-c51e-c29f81939f5a"
      },
      "source": [
        "# these are the recent 10 tweets from AK\n",
        "def extract_pdf_link_from_tweet():\n",
        "\n",
        "  for info in tweets[14:15]:\n",
        "    link = \"\"\n",
        "    print(\"ID: {}\".format(info.id))\n",
        "    print(info.created_at)\n",
        "    print(info.full_text)\n",
        "    print(\"\\n\")\n",
        "    pdf_link = re.search(\"pdf:.*\",info.full_text)\n",
        "     \n",
        "\n",
        "    if(pdf_link != None):\n",
        "      pdf_link_list = pdf_link.string.split(\"\\n\")\n",
        "      for i in pdf_link_list:\n",
        "         if(\"pdf: \" in i) :\n",
        "            print(\"PDF Link: \",i)\n",
        "            link_list = i.split(\" \")\n",
        "            link = link_list[1]\n",
        "          \n",
        "\n",
        "  return link      \n",
        "\n",
        "\n",
        "#extract_pdf_link_from_tweet()      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ID: 1420856997583413262\n",
            "2021-07-29 21:21:21\n",
            "RT @abidlabs: By now, you've probably heard about Hugging Face Spaces, a *free* way to host @Gradio demos, like this cool depth perception…\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Dx8PP9FmgPz"
      },
      "source": [
        "import requests\n",
        "def unshorten_url(url=\"\"):\n",
        "  \n",
        "  session = requests.Session()  # so connections are recycled\n",
        "  resp = session.head(url, allow_redirects=True)\n",
        "  \n",
        "  return resp.url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-8Jmv17XGVg"
      },
      "source": [
        "# Posting on Twitter\n",
        "def tweet_summary(tweet=\"\",user_id=\"\"):\n",
        "  \n",
        "  tweet_list = tweet.split('\\n')\n",
        "  tweet_list\n",
        "  count = 0\n",
        "  my_id=\"@Summarizer19 \"\n",
        "  for i in tweet_list[:-1] :\n",
        "      count = count+1\n",
        "      if(count == 1):\n",
        "        head = \"Requested By \"+\"@\"+user_id+\" \"\n",
        "        print(i)\n",
        "        try:\n",
        "          original_tweet = api.update_status(status=head+i)\n",
        "        except tweepy.TweepError:\n",
        "          print(\"Could not tweet due to some issue\")\n",
        "          break\n",
        "      else:\n",
        "        print(my_id+i)\n",
        "        if(count == 2):\n",
        "          try: \n",
        "            reply_tweet = api.update_status(status=my_id+i, \n",
        "                                 in_reply_to_status_id=original_tweet.id, \n",
        "                                 auto_populate_reply_metadata=False)\n",
        "          except tweepy.TweepError:\n",
        "            print(\"Could not tweet due to some issue\")\n",
        "            break\n",
        "        else:\n",
        "          try:\n",
        "            reply_tweet = api.update_status(status=my_id+i, \n",
        "                                 in_reply_to_status_id=reply_tweet.id, \n",
        "                                 auto_populate_reply_metadata=False)\n",
        "          except tweepy.TweepError:\n",
        "            print(\"Could not tweet due to some issue\")\n",
        "            break\n",
        "        \n",
        "  count = 0      \n",
        "\n",
        "#tweet_summary(gen_pdf_summary(unshorten_url(extract_pdf_link_from_tweet())),user_id=\"ak92501\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0RPcFEsTzAg",
        "outputId": "8de6be0a-67b9-450b-cf89-5a14aa979322"
      },
      "source": [
        "api.list_direct_messages(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[DirectMessage(_api=<tweepy.api.API object at 0x7f7afc6b3190>, _json={'type': 'message_create', 'id': '1421023655350259719', 'created_timestamp': '1627633415435', 'message_create': {'target': {'recipient_id': '1419176160127164417'}, 'sender_id': '1321213011911004161', 'message_data': {'text': 'Hi', 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [], 'urls': []}}}}, type='message_create', id='1421023655350259719', created_timestamp='1627633415435', message_create={'target': {'recipient_id': '1419176160127164417'}, 'sender_id': '1321213011911004161', 'message_data': {'text': 'Hi', 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [], 'urls': []}}})]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swSNt51P9l_v"
      },
      "source": [
        "def lookup_for_dms():\n",
        "  global latest_dm_timestamp\n",
        "  \n",
        "  \n",
        "\n",
        "  messages = api.list_direct_messages(1)\n",
        "\n",
        "  \n",
        "    \n",
        "\n",
        "  output_link = \"\"\n",
        "  output_user_id = \"\"\n",
        "\n",
        "  for message in messages:\n",
        "     if(int(message.created_timestamp) <= int(latest_dm_timestamp)):\n",
        "       break\n",
        "\n",
        "     if(len(message.message_create['message_data']['entities']['urls']) != 0):\n",
        "       print(message.message_create['message_data']['entities']['urls'])\n",
        "       #link = str(message.message_create['message_data']['entities']['urls'])\n",
        "       link = str(message.message_create['message_data']['entities']['urls'])\n",
        "       list_link= link.split(\",\")\n",
        "       #print(list_link[1])\n",
        "       list_expanded_link = list_link[1].split(\": \")\n",
        "       if(\"arxiv.org\" in list_expanded_link[1]):\n",
        "         expanded_arxiv_link = list_expanded_link[1]\n",
        "         print(expanded_arxiv_link)\n",
        "         output_link = expanded_arxiv_link\n",
        "       else:\n",
        "         print(\"Not an arxiv link\\n\")\n",
        "\n",
        "     else:\n",
        "       print(\"DM does not have a link\")\n",
        "\n",
        "\n",
        "     if(len(message.message_create['message_data']['entities']['user_mentions']) != 0):\n",
        "    \n",
        "       user_mentions = str(message.message_create['message_data']['entities']['user_mentions'])\n",
        "\n",
        "       user_mentions_list = user_mentions.split(\",\")\n",
        "       screen_name = user_mentions_list[0].split(\": \")\n",
        "       user_name = screen_name[1]\n",
        "    \n",
        "       print(\"User_id: \",user_name)\n",
        "\n",
        "       output_user_id = user_name\n",
        "  \n",
        "     else:\n",
        "       print(\"No username provided\")\n",
        "\n",
        "  if(int(message.created_timestamp) > int(latest_dm_timestamp)):\n",
        "    output = []  \n",
        "    output.append(output_link) \n",
        "    output.append(output_user_id)\n",
        "  \n",
        "  else:\n",
        "    output = []  \n",
        "    output.append(None) \n",
        "    output.append(None)\n",
        "\n",
        "  latest_dm_timestamp = message.created_timestamp\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "  print(message)\n",
        "\n",
        "  \n",
        "\n",
        "  return output\n",
        "\n",
        "#latest_dm_timestamp = 0\n",
        "#requests = lookup_for_dms() \n",
        "#if(requests[0] != None):\n",
        "#  print(\"LINK:\",requests[0])\n",
        "#  print(\"LAST DM TIMESTAMP\",latest_dm_timestamp)\n",
        "#else:\n",
        "#  print(\"No Link\")\n",
        "#  print(\"LAST DM TIMESTAMP\",latest_dm_timestamp)\n",
        "#tweet_summary(gen_pdf_summary(requests[0]),user_id=requests[1][1:-1])  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0xBgkofqMO0"
      },
      "source": [
        "import time\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger()\n",
        "\n",
        "#latest_dm_timestamp = 0\n",
        "\n",
        "while True:\n",
        "  print(\"Hello\\n\")\n",
        "  requests = lookup_for_dms()\n",
        "  if(requests[0] != None ):\n",
        "    if(requests[0] != \"\"):\n",
        "      print(\"LINK:\",requests[0])\n",
        "      tweet_summary(gen_pdf_summary(requests[0]),user_id=requests[1][1:-1])\n",
        "  else:\n",
        "    print(\"No Link in the latest DM or Summary has been already posted for the current DM\")\n",
        "    \n",
        "\n",
        "  \n",
        "  logger.info(\"Waiting...\")\n",
        "  time.sleep(60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SJIUt87PdU7"
      },
      "source": [
        "## NOT IN USE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbWeKGhypeNq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "6ad016b7-fa2f-437e-ee76-b52ba9f18621"
      },
      "source": [
        "gen_df = pd.DataFrame({'generated_summary':gen_summs})\n",
        "gen_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>generated_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Role of Word Sense Disambiguation i  Lexical Acquisition: Predicting Semantics from Syntactic Cues\\nThis paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the construction of large-scale knowledge sources.\\nWe describe two experiments: one which ignored word- sense distinctions, resulting in 6.3% accuracy for semantic classification of verbs based on (Levin, 1993); and one which exploited word sense distinctions,. resulting in 97.9% accuracy.\\nThese experiments were dual purpose: (1) to validate the central thesis of the work of Levin, 199...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Improving Statistical Machine Translation Using Word Sense Disambiguation\\nWe show for the first time that incorporating the predictions of a word sense disambigua tion system within a typical phrase-based statistical machine translation (SMT) model consistently improves translation quality across all three different NIST Chinese-English test sets, as well as producing statistically significant improvements on the larger NIST-Chinese-English MT task– and moreover never hurts performance on any test set, according not only to BLEU scores but to all eight most commonly used automatic evalua...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing\\nMost current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of–speech tagger before parsing begins.\\nWe present a transition-based system for joint part–of-speech tagging and labeled dependency parsing with non-projective trees.\\nExperimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of‑the-art]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Constraint Grammars As A FAMEWHIST For Parsing\\nWe present a formalism to be used for parsing where the grammar statements are closer to real text sentences and more directly address some notorious parsing problems, especially ambiguity.\\nThe formalism is a linguistic one.\\nIt relies on transitional probabilities in an indirect way. The probabilities are not part of the description. The descriptive statements, constraints, do not have the ordinary task of defining the notion correct sentence in L. They are less categorical in nature, more closely tied to morphological features, and more g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Efficient Parsing Of Highly Ambiguous Context-Free Grammars With Bit Vectors\\nAn efficient bit-vector-based CKY-style parser for context-free parsing is presented.\\nThe parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences.\\nThis paper describes the development of BitPar, a fast and robust parser for highly ambiguous contexts.\\nBitPar is based on a bit vector implementation of the well-known Cocke-Younger-Kasami (CKY) algorithm.\\nIt build a compact]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         generated_summary\n",
              "0  [Role of Word Sense Disambiguation i  Lexical Acquisition: Predicting Semantics from Syntactic Cues\\nThis paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the construction of large-scale knowledge sources.\\nWe describe two experiments: one which ignored word- sense distinctions, resulting in 6.3% accuracy for semantic classification of verbs based on (Levin, 1993); and one which exploited word sense distinctions,. resulting in 97.9% accuracy.\\nThese experiments were dual purpose: (1) to validate the central thesis of the work of Levin, 199...\n",
              "1  [Improving Statistical Machine Translation Using Word Sense Disambiguation\\nWe show for the first time that incorporating the predictions of a word sense disambigua tion system within a typical phrase-based statistical machine translation (SMT) model consistently improves translation quality across all three different NIST Chinese-English test sets, as well as producing statistically significant improvements on the larger NIST-Chinese-English MT task– and moreover never hurts performance on any test set, according not only to BLEU scores but to all eight most commonly used automatic evalua...\n",
              "2      [A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing\\nMost current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of–speech tagger before parsing begins.\\nWe present a transition-based system for joint part–of-speech tagging and labeled dependency parsing with non-projective trees.\\nExperimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of‑the-art]\n",
              "3  [Constraint Grammars As A FAMEWHIST For Parsing\\nWe present a formalism to be used for parsing where the grammar statements are closer to real text sentences and more directly address some notorious parsing problems, especially ambiguity.\\nThe formalism is a linguistic one.\\nIt relies on transitional probabilities in an indirect way. The probabilities are not part of the description. The descriptive statements, constraints, do not have the ordinary task of defining the notion correct sentence in L. They are less categorical in nature, more closely tied to morphological features, and more g...\n",
              "4                                                     [Efficient Parsing Of Highly Ambiguous Context-Free Grammars With Bit Vectors\\nAn efficient bit-vector-based CKY-style parser for context-free parsing is presented.\\nThe parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences.\\nThis paper describes the development of BitPar, a fast and robust parser for highly ambiguous contexts.\\nBitPar is based on a bit vector implementation of the well-known Cocke-Younger-Kasami (CKY) algorithm.\\nIt build a compact]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui4niqVsSmfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "e80d7928-241f-4c1b-f616-79cb7065aaaa"
      },
      "source": [
        "gen_df.generated_summary = gen_df.generated_summary.apply(lambda x: x[0])\n",
        "gen_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>generated_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Role of Word Sense Disambiguation i  Lexical Acquisition: Predicting Semantics from Syntactic Cues\\nThis paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the construction of large-scale knowledge sources.\\nWe describe two experiments: one which ignored word- sense distinctions, resulting in 6.3% accuracy for semantic classification of verbs based on (Levin, 1993); and one which exploited word sense distinctions,. resulting in 97.9% accuracy.\\nThese experiments were dual purpose: (1) to validate the central thesis of the work of Levin, 1993),</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Improving Statistical Machine Translation Using Word Sense Disambiguation\\nWe show for the first time that incorporating the predictions of a word sense disambigua tion system within a typical phrase-based statistical machine translation (SMT) model consistently improves translation quality across all three different NIST Chinese-English test sets, as well as producing statistically significant improvements on the larger NIST-Chinese-English MT task– and moreover never hurts performance on any test set, according not only to BLEU scores but to all eight most commonly used automatic evaluat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing\\nMost current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of–speech tagger before parsing begins.\\nWe present a transition-based system for joint part–of-speech tagging and labeled dependency parsing with non-projective trees.\\nExperimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of‑the-art</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Constraint Grammars As A FAMEWHIST For Parsing\\nWe present a formalism to be used for parsing where the grammar statements are closer to real text sentences and more directly address some notorious parsing problems, especially ambiguity.\\nThe formalism is a linguistic one.\\nIt relies on transitional probabilities in an indirect way. The probabilities are not part of the description. The descriptive statements, constraints, do not have the ordinary task of defining the notion correct sentence in L. They are less categorical in nature, more closely tied to morphological features, and more ge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Efficient Parsing Of Highly Ambiguous Context-Free Grammars With Bit Vectors\\nAn efficient bit-vector-based CKY-style parser for context-free parsing is presented.\\nThe parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences.\\nThis paper describes the development of BitPar, a fast and robust parser for highly ambiguous contexts.\\nBitPar is based on a bit vector implementation of the well-known Cocke-Younger-Kasami (CKY) algorithm.\\nIt build a compact</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         generated_summary\n",
              "0   Role of Word Sense Disambiguation i  Lexical Acquisition: Predicting Semantics from Syntactic Cues\\nThis paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the construction of large-scale knowledge sources.\\nWe describe two experiments: one which ignored word- sense distinctions, resulting in 6.3% accuracy for semantic classification of verbs based on (Levin, 1993); and one which exploited word sense distinctions,. resulting in 97.9% accuracy.\\nThese experiments were dual purpose: (1) to validate the central thesis of the work of Levin, 1993),\n",
              "1  Improving Statistical Machine Translation Using Word Sense Disambiguation\\nWe show for the first time that incorporating the predictions of a word sense disambigua tion system within a typical phrase-based statistical machine translation (SMT) model consistently improves translation quality across all three different NIST Chinese-English test sets, as well as producing statistically significant improvements on the larger NIST-Chinese-English MT task– and moreover never hurts performance on any test set, according not only to BLEU scores but to all eight most commonly used automatic evaluat...\n",
              "2        A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing\\nMost current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of–speech tagger before parsing begins.\\nWe present a transition-based system for joint part–of-speech tagging and labeled dependency parsing with non-projective trees.\\nExperimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of‑the-art\n",
              "3  Constraint Grammars As A FAMEWHIST For Parsing\\nWe present a formalism to be used for parsing where the grammar statements are closer to real text sentences and more directly address some notorious parsing problems, especially ambiguity.\\nThe formalism is a linguistic one.\\nIt relies on transitional probabilities in an indirect way. The probabilities are not part of the description. The descriptive statements, constraints, do not have the ordinary task of defining the notion correct sentence in L. They are less categorical in nature, more closely tied to morphological features, and more ge...\n",
              "4                                                       Efficient Parsing Of Highly Ambiguous Context-Free Grammars With Bit Vectors\\nAn efficient bit-vector-based CKY-style parser for context-free parsing is presented.\\nThe parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences.\\nThis paper describes the development of BitPar, a fast and robust parser for highly ambiguous contexts.\\nBitPar is based on a bit vector implementation of the well-known Cocke-Younger-Kasami (CKY) algorithm.\\nIt build a compact"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zzbRkPnS14Y"
      },
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge2', 'rouge3'])\n",
        "scores = []\n",
        "#mod: sharmaa4 for i in range(500):\n",
        "for i in range(5):\n",
        "  \n",
        "  scores.append(scorer.score(scis_df.iloc[i].summary, gen_df.iloc[i].generated_summary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWNZPk0XS66I"
      },
      "source": [
        "r2r = []\n",
        "r2f = []\n",
        "r3f = []\n",
        "#for i in range(250):\n",
        "for i in range(5):\n",
        "  r2r.append(scores[i]['rouge2'].recall)\n",
        "  r2f.append(scores[i]['rouge2'].fmeasure)\n",
        "  r3f.append(scores[i]['rouge3'].fmeasure)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZs6zLiXUY3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8366ccc-1667-451a-8038-4d622c055d9e"
      },
      "source": [
        "print('rouge2 - recall: '+ str(np.mean(r2r)))\n",
        "print('rouge2 - fmeasure: '+ str(np.mean(r2f)))\n",
        "print('rouge3 - fmeasure: '+ str(np.mean(r3f)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rouge2 - recall: 0.028551457669011026\n",
            "rouge2 - fmeasure: 0.03360346967623687\n",
            "rouge3 - fmeasure: 0.007523001813298525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxGaf590U0v3"
      },
      "source": [
        "gen_df.to_csv('./nonpt_first500.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrFVaeCu1K_S"
      },
      "source": [
        "## NOT IN USE "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p36AOpm4QA-Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bdf5bbd1-235f-4a55-9889-8b02d12196a5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Manual train-test split\n",
        "np.random.seed(10) # seed added \n",
        "msk = np.random.rand(len(scis_df)) < 0.8\n",
        "msk\n",
        "train_df = scis_df[msk]\n",
        "test_df = scis_df[~msk]\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bayesian Unsupervised Topic Segmentation This paper describes a novel Bayesian approach to unsupervised topic segmentation. Unsupervised systems for this task are driven the tendency of wellformed segments to induce a compact and consistent lexical distribution. We show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation. This contrasts with previous approaches, which relied o...</td>\n",
              "      <td>Bayesian Unsupervised Topic Segmentation\\nThis paper describes a novel Bayesian approach to unsupervised topic segmentation.\\nUnsupervised systems for this task are driven by lexical cohesion: the tendency of well-formed segments to induce a compact and consistent lexical distribution.\\nWe show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation.\\nThis contrasts with previous ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The Alignment Template Approach To Statistical Machine Translation A phrase-based statistical machine translation approach — the alignment template approach — is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source–channel approach. Thereby, the model is easier to extend tha...</td>\n",
              "      <td>The Alignment Template Approach To Statistical Machine Translation\\nA phrase-based statistical machine translation approach — the alignment template approach — is described.\\nThis translation approach allows for general many-to-many relations between words.\\nThereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly.\\nThe model is described using a log-linear modeling approach, which is a generalization of the often used source–channel approach.\\nThereby, the model is easier to exten...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A Prosodic Analysis Of Discourse Segments In Direction-Giving Monologues This paper reports on corpus-based research into the relationship between intonational variation and discourse structure. We examine the effects of speaking style (read versus spontaneous) and of discourse segmentation method (text-alone versus text-and-speech) on the nature of this relationship. We also compare the acoustic-prosodic features of initial, medial, and final utterances in a discourse segment. This paper presents empirical support for the assumption long held by computational linguists, that intonation ca...</td>\n",
              "      <td>A Prosodic Analysis Of Discourse Segments In Direction-Giving Monologues\\nThis paper reports on corpus-based research into the relationship between intonational variation and discourse structure.\\nWe examine the effects of speaking style (read versus spontaneous) and of discourse segmentation method (text-alone versus text-and-speech) on the nature of this relationship.\\nWe also compare the acoustic-prosodic features of initial, medial, and final utterances in a discourse segment.\\nWe find that speech is able to improve inter-annotator agreement in discourse segmentation of monologues.\\nWe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Named Entity Transliteration With Comparable Corpora In this paper we investigate Chinesename transliteration using compacorpora where texts in the two languages deal in some of the same topics — and therefore share references to named entities — but are not translations of each other. We present two distinct methods for transliteration, one approach using phonetic transliteration, and the second using the temporal distribution of candidate pairs. Each of these approaches works quite well, but by combining the approaches one can achieve even better results. We then propose a novel score pr...</td>\n",
              "      <td>Named Entity Transliteration With Comparable Corpora\\nIn this paper we investigate Chinese-English name transliteration using comparable corpora, corpora where texts in the two languages deal in some of the same topics - and therefore share references to named entities - but are not translations of each other.\\nWe present two distinct methods for transliteration, one approach using phonetic transliteration, and the second using the temporal distribution of candidate pairs.\\nEach of these approaches works quite well, but by combining the approaches one can achieve even better results.\\nWe t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation chl, dozhang@microsoft.com mhli@insun.hit.edu.cn muli, mingzhou@microsoft.com guanyi@insun.hit.edu.cn Abstract Inspired by previous preprocessing approaches to SMT, this paper proposes a novel, probabilistic approach to reordering which combines the merits of syntax and phrase-based SMT. Given a source sentence and its parse tree, our method generates, tree operations, an list of reordered inputs, which are then fed to standard phrase-based decoder to produce the optimal translation. Experiments show th...</td>\n",
              "      <td>A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation\\nInspired by previous preprocessing approaches to SMT, this paper proposes a novel, probabilistic approach to reordering which combines the merits of syntax and phrase-based SMT.\\nGiven a source sentence and its parse tree, our method generates, by tree operations, an n-best list of reordered inputs, which are then fed to standard phrase-based decoder to produce the optimal translation.\\nExperiments show that, for the NIST MT-05 task of Chinese-to-English translation, the proposal leads to BLEU improvem...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  summary\n",
              "0  Bayesian Unsupervised Topic Segmentation This paper describes a novel Bayesian approach to unsupervised topic segmentation. Unsupervised systems for this task are driven the tendency of wellformed segments to induce a compact and consistent lexical distribution. We show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation. This contrasts with previous approaches, which relied o...  Bayesian Unsupervised Topic Segmentation\\nThis paper describes a novel Bayesian approach to unsupervised topic segmentation.\\nUnsupervised systems for this task are driven by lexical cohesion: the tendency of well-formed segments to induce a compact and consistent lexical distribution.\\nWe show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation.\\nThis contrasts with previous ...\n",
              "1  The Alignment Template Approach To Statistical Machine Translation A phrase-based statistical machine translation approach — the alignment template approach — is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source–channel approach. Thereby, the model is easier to extend tha...  The Alignment Template Approach To Statistical Machine Translation\\nA phrase-based statistical machine translation approach — the alignment template approach — is described.\\nThis translation approach allows for general many-to-many relations between words.\\nThereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly.\\nThe model is described using a log-linear modeling approach, which is a generalization of the often used source–channel approach.\\nThereby, the model is easier to exten...\n",
              "2  A Prosodic Analysis Of Discourse Segments In Direction-Giving Monologues This paper reports on corpus-based research into the relationship between intonational variation and discourse structure. We examine the effects of speaking style (read versus spontaneous) and of discourse segmentation method (text-alone versus text-and-speech) on the nature of this relationship. We also compare the acoustic-prosodic features of initial, medial, and final utterances in a discourse segment. This paper presents empirical support for the assumption long held by computational linguists, that intonation ca...  A Prosodic Analysis Of Discourse Segments In Direction-Giving Monologues\\nThis paper reports on corpus-based research into the relationship between intonational variation and discourse structure.\\nWe examine the effects of speaking style (read versus spontaneous) and of discourse segmentation method (text-alone versus text-and-speech) on the nature of this relationship.\\nWe also compare the acoustic-prosodic features of initial, medial, and final utterances in a discourse segment.\\nWe find that speech is able to improve inter-annotator agreement in discourse segmentation of monologues.\\nWe...\n",
              "3  Named Entity Transliteration With Comparable Corpora In this paper we investigate Chinesename transliteration using compacorpora where texts in the two languages deal in some of the same topics — and therefore share references to named entities — but are not translations of each other. We present two distinct methods for transliteration, one approach using phonetic transliteration, and the second using the temporal distribution of candidate pairs. Each of these approaches works quite well, but by combining the approaches one can achieve even better results. We then propose a novel score pr...  Named Entity Transliteration With Comparable Corpora\\nIn this paper we investigate Chinese-English name transliteration using comparable corpora, corpora where texts in the two languages deal in some of the same topics - and therefore share references to named entities - but are not translations of each other.\\nWe present two distinct methods for transliteration, one approach using phonetic transliteration, and the second using the temporal distribution of candidate pairs.\\nEach of these approaches works quite well, but by combining the approaches one can achieve even better results.\\nWe t...\n",
              "4  A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation chl, dozhang@microsoft.com mhli@insun.hit.edu.cn muli, mingzhou@microsoft.com guanyi@insun.hit.edu.cn Abstract Inspired by previous preprocessing approaches to SMT, this paper proposes a novel, probabilistic approach to reordering which combines the merits of syntax and phrase-based SMT. Given a source sentence and its parse tree, our method generates, tree operations, an list of reordered inputs, which are then fed to standard phrase-based decoder to produce the optimal translation. Experiments show th...  A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation\\nInspired by previous preprocessing approaches to SMT, this paper proposes a novel, probabilistic approach to reordering which combines the merits of syntax and phrase-based SMT.\\nGiven a source sentence and its parse tree, our method generates, by tree operations, an n-best list of reordered inputs, which are then fed to standard phrase-based decoder to produce the optimal translation.\\nExperiments show that, for the NIST MT-05 task of Chinese-to-English translation, the proposal leads to BLEU improvem..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAFIFuR93KOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f362da43-ac15-4971-b73d-6f1c51c9ffca"
      },
      "source": [
        "tokenizer.all_special_tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', '</s>', '<unk>', '<pad>', '<mask>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1JLzUgt3T9G"
      },
      "source": [
        "sl = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx15rFoj2FJW"
      },
      "source": [
        "#from fastai.text.all import *\n",
        "\n",
        "#class TransformersTokenizer(Transform):\n",
        "#    def __init__(self, tokenizer): self.tokenizer = tokenizer\n",
        "#    def encodes(self, x): \n",
        "#        #mod: sharmaa4 x = ' '.join(x.split()[:sl])\n",
        "#        #x = [] \n",
        "#        #x = x.append(x.split()[:sl])\n",
        "#        #mod:sharmaa4 x_toks = self.tokenizer.tokenize(x['text'])\n",
        "#        x_toks = self.tokenizer.tokenize(x)\n",
        "#        #mod: sharmaa4 y_toks = self.tokenizer.tokenize(x['summary'].iloc[:])\n",
        "#        #y_toks = self.tokenizer.tokenize(x['summary'])\n",
        "#        x_out = tensor(self.tokenizer.convert_tokens_to_ids(x_toks))# Conversion into word embeddings\n",
        "#        #y_out = tensor(self.tokenizer.convert_tokens_to_ids(y_toks))# Conversion into word embeddings\n",
        "#        if len(x_out) > sl:\n",
        "#          #return (x_out[:sl], y_out)\n",
        "#          return x_out[:sl]\n",
        "#        #if y_out\n",
        "#        else: #return (x_out, y_out)\n",
        "#            return x_out\n",
        "#        # return tensor(self.tokenizer.convert_tokens_to_ids(toks))[:sl]\n",
        "#    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSpxQCqq1w6o"
      },
      "source": [
        "# df = pd.DataFrame(dict(a=[1,2,3],b=[2,3,4]))\n",
        "# tl = TfmdLists(df, lambda o: o.a+1, splits=[[0],[1,2]])\n",
        "# test_eq(tl[1,2], [3,4])\n",
        "# tr = tl.subset(0)\n",
        "# test_eq(tr[:], [2])\n",
        "# val = tl.subset(1)\n",
        "# test_eq(val[:], [3,4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6j7bxwjK7Md"
      },
      "source": [
        "??BartTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4AeeBbG2mgO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9000940-3700-4711-9d33-8b08c812bb5c"
      },
      "source": [
        "#mod:sharmaa4 all_texts = np.concatenate([train_df['text'].values, test_df['text'].values])\n",
        "all_texts = np.concatenate([train_df['text'].values, test_df['text'].values])\n",
        "all_summs = np.concatenate([train_df['summary'].values, test_df['summary'].values])\n",
        "type(all_texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iDuPcLsOyP9"
      },
      "source": [
        "def tokenize(text):\n",
        "    toks = tokenizer.tokenize(text)\n",
        "    xout = tensor(tokenizer.convert_tokens_to_ids(toks))\n",
        "    if (len(xout)) > 1024:\n",
        "      return xout[:1024]\n",
        "    else:\n",
        "      return tensor(tokenizer.convert_tokens_to_ids(toks))\n",
        "\n",
        "tokenized = [tokenize(t) for t in all_texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8A0UTxuO104"
      },
      "source": [
        "class TransformersTokenizer(Transform):\n",
        "    def __init__(self, tokenizer): self.tokenizer = tokenizer\n",
        "    def encodes(self, x): \n",
        "        return x if isinstance(x, Tensor) else tokenize(x)\n",
        "        \n",
        "    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR4lqcWn2I5u"
      },
      "source": [
        "splits = [range_of(train_df), list(range(len(train_df), len(all_texts)))]\n",
        "#text_tls = TfmdLists(all_texts, TransformersTokenizer(tokenizer) , splits=splits, dl_type=TfmdDL)\n",
        "#sum_tls = TfmdLists(all_summs, TransformersTokenizer(tokenizer) , splits=splits, dl_type=TfmdDL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsbcGSpW2BYw"
      },
      "source": [
        "#mod:sharmaa4 tls = TfmdLists(scis_df, TransformersTokenizer(tokenizer), splits=splits, dl_type=TfmdDL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W40w-swDRuDF"
      },
      "source": [
        "scis_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZofersiRNHZW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26693e72-ed00-4628-9449-df3313bf3065"
      },
      "source": [
        "#tls = TfmdLists(all_texts, TransformersTokenizer(tokenizer), splits=splits, dl_type=TfmdDL)\n",
        "tls = TfmdLists(all_texts, TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader)\n",
        "type(tls)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "fastai.data.core.TfmdLists"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRhflPvIh76b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d4ba6c7-f6d2-4b54-c3f9-7dfdf2c3cb40"
      },
      "source": [
        "#show_at(tls.train, 0)\n",
        "tls.train[0],tls.valid[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([20861, 44871,  1890,  ...,  7905,     6,    52]),\n",
              " tensor([  530,    12,   846,  ..., 25173,    16,    45]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwjkPYl0r-IA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04e58f59-715b-4e8b-f4c7-804c430a1bb2"
      },
      "source": [
        "#tls.train[1].shape, tls.valid[1].shape\n",
        "tls.dataloaders"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method FilteredBase.dataloaders of TfmdLists: [\"Bayesian Unsupervised Topic Segmentation This paper describes a novel Bayesian approach to unsupervised topic segmentation. Unsupervised systems for this task are driven the tendency of wellformed segments to induce a compact and consistent lexical distribution. We show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation. This contrasts with previous approaches, which relied on hand-crafted cohesion metrics. The Bayesian framework provides a principled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems. Our model yields consistent improvements over an array of state-of-the-art systems on both text and speech datasets. We also show that both an entropy-based analysis and a well-known previous technique can be de Topic segmentation is one of the fundamental problems in discourse analysis, where the task is to divide a text into a linear sequence of topicallycoherent segments. Hearst’s TEXTTILING (1994) introduced the idea that unsupervised segmentation can be driven by lexical cohesion, as high-quality segmentations feature homogeneous lexical distributions within each topic segment. Lexical cohesion has provided the inspiration for several successful systems (e.g., Utiyama and Isahara, 2001; Galley et al.2003; Malioutov and Barzilay, 2006), and is currently the dominant approach to unsupervised topic segmentation. But despite the effectiveness of lexical cohesion for unsupervised topic segmentation, it is clear that there are other important indicators that are ignored by the current generation of unsupervised systems. For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau and Litman, 1993; Galley et al., 2003), but cannot be incorporated by current unsupervised models. One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (Hearst, 1994; Malioutov and Barzilay, 2006). Without supervision, it is not possible to combine such metrics with additional sources of information. Moreover, such hand-crafted metrics may not generalize well across multiple datasets, and often include parameters which must be tuned on development sets (Malioutov and Barzilay, 2006; Galley et al., 2003). In this paper, we situate lexical cohesion in a Bayesian framework, allowing other sources of information to be incorporated without the need for labeled data. We formalize lexical cohesion in a generative model in which the text for each segment is produced by a distinct lexical distribution. Lexically-consistent segments are favored by this model because probability mass is conserved for a narrow subset of words. Thus, lexical cohesion arises naturally through the generative process, and other sources of information – such as cue words – can easily be incorporated as emissions from the segment boundaries. More formally, we treat the words in each sentence as draws from a language model associated with the topic segment. This is related to topicmodeling methods such as latent Dirichlet allocation (LDA; Blei et al. 2003), but here the induced topics are tied to a linear discourse structure. This property enables a dynamic programming solution to find the exact maximum-likelihood segmentation. We consider two approaches to handling the language models: estimating them explicitly, and integrating them out, using the Dirichlet Compound Multinomial distribution (also known as the multivariate Polya distribution). We model cue phrases as generated from a separate multinomial that is shared across all topics and documents in the dataset; a high-likelihood model will obtain a compact set of cue phrases. The addition of cue phrases renders our dynamic programming-based inference inapplicable, so we design a sampling-based inference technique. This algorithm can learn in a completely unsupervised fashion, but it also provides a principled mechanism to improve search through the addition of declarative linguistic knowledge. This is achieved by biasing the selection of samples towards boundaries with known cue phrases; this does not change the underlying probabilistic model, but guides search in the direction of linguistically-plausible segmentations. We evaluate our algorithm on corpora of spoken and written language, including the benchmark ICSI meeting dataset (Janin et al., 2003) and a new textual corpus constructed from the contents of a medical textbook. In both cases our model achieves performance surpassing multiple state-of-the-art baselines. Moreover, we demonstrate that the addition of cue phrases can further improve segmentation performance over cohesion-based methods. In addition to the practical advantages demonstrated by these experimental results, our model reveals interesting theoretical properties. Other researchers have observed relationships between discourse structure and entropy (e.g., Genzel and Charniak, 2002). We show that in a special case of our model, the segmentation objective is equal to a weighted sum of the negative entropies for each topic segment. This finding demonstrates that a relationship between discourse segmentation and entropy is a natural consequence of modeling topic structure in a generative Bayesian framework. In addition, we show that the benchmark segmentation system of Utiyama and Isahara (2001) can be viewed as another special case of our Bayesian model. Existing unsupervised cohesion-based approaches can be characterized in terms of the metric used to quantify cohesion and the search technique. Galley et al. (2003) characterize cohesion in terms of lexical chains – repetitions of a given lexical item over some fixed-length window of sentences. In their unsupervised model, inference is performed by selecting segmentation points at the local maxima of the cohesion function. Malioutov and Barzilay (2006) optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences. Most similar to our work is the approach of Utiyama and Isahara (2001), who search for segmentations with compact language models; as shown in Section 3.1.1, this can be viewed as a special case of our model. Both of these last two systems use dynamic programming to search the space of segmentations. An alternative Bayesian approach to segmentation was proposed by Purver et al. (2006). They assume a set of documents that is characterized by some number of hidden topics that are shared across multiple documents. They then build a linear segmentation by adding a switching variable to indicate whether the topic distribution for each sentence is identical to that of its predecessor. Unlike Purver et al., we do not assume a dataset in which topics are shared across multiple documents; indeed, our model can be applied to single documents individually. Additionally, the inference procedure of Purver et al. requires sampling multiple layers of hidden variables. In contrast, our inference procedure leverages the nature of linear segmentation to search only in the space of segmentation points. The relationship between discourse structure and cue phrases has been studied extensively; for an early example of computational work on this topic, see (Grosz, 1977). Passonneau and Litman (1993) were the first to investigate the relationship between cue phrases and linear segmentation. More recently, cue phrases have been applied to topic segmentation in the supervised setting. In a supervised system that is distinct from the unsupervised model described above, Galley et al. (2003) automatically identify candidate cue phrases by mining labeled data for words that are especially likely to appear at segment boundaries; the presence of cue phrases is then used as a feature in a rule-based classifier for linear topic segmentation. Elsner and Charniak (2008) specify a list of cue phrases by hand; the cue phrases are used as a feature in a maximum-entropy classifier for conversation disentanglement. Unlike these approaches, we identify candidate cue phrases automatically from unlabeled data and incorporate them in the topic segmentation task without supervision. The core idea of lexical cohesion is that topicallycoherent segments demonstrate compact and consistent lexical distributions (Halliday and Hasan, 1976). Lexical cohesion can be placed in a probabilistic context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment. Formally, if sentence t is in segment j, then the bag of words xt is drawn from the multinomial language model θj. This is similar in spirit to hidden topic models such as latent Dirichlet allocation (Blei et al., 2003), but rather than assigning a hidden topic to each word, we constrain the topics to yield a linear segmentation of the document. We will assume that topic breaks occur at sentence boundaries, and write zt to indicate the topic assignment for sentence t. The observation likelihood is, where X is the set of all T sentences, z is the vector of segment assignments for each sentence, and Θ is the set of all K language models.2 A linear segmentation is ensured by the additional constraint that zt must be equal to either zt−1 (the previous sentence’s segment) or zt−1 + 1 (the next segment). To obtain a high likelihood, the language models associated with each segment should concentrate their probability mass on a compact subset of words. Language models that spread their probability mass over a broad set of words will induce a lower likelihood. This is consistent with the principle of lexical cohesion. Thus far, we have described a segmentation in terms of two parameters: the segment indices z, and the set of language models Θ. For the task of segmenting documents, we are interested only in the segment indices, and would prefer not to have to search in the space of language models as well. We consider two alternatives: taking point estimates of the language models (Section 3.1), and analytically marginalizing them out (Section 3.2). One way to handle the language models is to choose a single point estimate for each set of segmentation points z. Suppose that each language model is drawn from a symmetric Dirichlet prior: θj — Dir(θ0). Let nj be a vector in which each element is the sum of the lexical counts over all the sentences in segment j: nj,i = E{t:zt=j} mt,i, where mt,i is the count of word i in sentence t. Assuming that each xt — θj, then the posterior distribution for θj is Dirichlet with vector parameter nj +θ0 (Bernardo and Smith, 2000). The expected value of this distribution is the multinomial distribution ˆθj, where, In this equation, W indicates the number of words in the vocabulary. Having obtained an estimate for the language model ˆθj, the observed data likelihood for segment j is a product over each sentence in the segment, 2Our experiments will assume that the number of topics K is known. This is common practice for this task, as the desired number of segments may be determined by the user (Malioutov and Barzilay, 2006). By viewing the likelihood as a product over all terms in the vocabulary, we observe interesting connections with prior work on segmentation and information theory. In this section, we explain how our model generalizes the well-known method of Utiyama and Isahara (2001; hereafter U&I). As in our work, Utiyama and Isahara propose a probabilistic framework based on maximizing the compactness of the language models induced for each segment. Their likelihood equation is identical to our equations 3-5. They then define the language models for each segment as �Bj,i = nj,iW1 , without rigorous justifiW+Ei nj,i cation. This form is equivalent to Laplacian smoothing (Manning and Sch¨utze, 1999), and is a special case of our equation 2, with B0 = 1. Thus, the language models in U&I can be viewed as the expectation of the posterior distribution p(Bj|{xt : zt = j}, B0), in the special case that B0 = 1. Our approach generalizes U&I and provides a Bayesian justification for the language models that they apply. The remainder of the paper further extends this work by marginalizing out the language model, and by adding cue phrases. We empirically demonstrate that these extensions substantially improve performance. Our model also has a connection to entropy, and situates entropy-based segmentation within a Bayesian framework. Equation 1 defines the objective function as a product across sentences; using equations 3-5 we can decompose this across segments instead. Working in logarithms, The last line substitutes in the logarithm of equation 5. Setting B0 = 0 and rearranging equation 2, we obtain nj,i = Nj�Bj,i, with Nj = PW i nj,i, the total number of words in segment j. Substituting this into equation 6, we obtain where H(Bj) is the negative entropy of the multinomial Bj. Thus, with B0 = 0, the log conditional probability in equation 6 is optimized by a segmentation that minimizes the weighted sum of entropies per segment, where the weights are equal to the segment lengths. This result suggests intriguing connections with prior work on the relationship between entropy and discourse structure (e.g., Genzel and Charniak, 2002; Sporleder and Lapata, 2006). The previous subsection uses point estimates of the language models to reveal connections to entropy and prior work on segmentation. However, point estimates are theoretically unsatisfying from a Bayesian perspective, and better performance may be obtained by marginalizing over all possible laneach segment, so the overall likelihood for the pointestimate version also decomposes across segments. Any objective function that can be decomposed into a product across segments can be maximized using dynamic programming. We define B(t) as the value of the objective function for the optimal segmentation up to sentence t. The contribution to the objective function from a single segment between sentences t' and t is written, b(t', t) = p({xt, ... xt}|zt-...t = j) where pdcm refers to the Dirichlet compound multinomial distribution (DCM), also known as the multivariate Polya distribution (Johnson et al., 1997). The DCM distribution expresses the expectation over all multinomial language models, when conditioning on the Dirichlet prior θ0. When θ0 is a symmetric Dirichlet prior, where nj,i is the count of word i in segment j, and Nj = PWi nj,i, the total number of words in the segment. The symbol F refers to the Gamma function, an extension of the factorial function to real numbers. Using the DCM distribution, we can compute the data likelihood for each segment from the lexical counts over the entire segment. The overall observation likelihood is a product across the likelihoods for each segment. The optimal segmentation maximizes the joint probability, p(X, z|θ0) = p(X|z, θ0)p(z). We assume that p(z) is a uniform distribution over valid segmentations, and assigns no probability mass to invalid segmentations. The data likelihood is defined for point estimate language models in equation 5 and for marginalized language models in equation 7. Note that equation 7 is written as a product over segments. The point estimates for the language models depend only on the counts within The maximum value of the objective function is then given by the recurrence relation, B(t) = maxt,<t B(t')b(t'+1, t), with the base case B(0) = 1. These values can be stored in a table of size T (equal to the number of sentences); this admits a dynamic program that performs inference in polynomial time.3 If the number of segments is specified in advance, the dynamic program is slightly more complex, with a table of size TK. The Dirichlet compound multinomial integrates over language models, but we must still set the prior θ0. We can re-estimate this prior based on the observed data by interleaving gradient-based search in a Viterbi expectation-maximization framework (Gauvain and Lee, 1994). In the E-step, we estimate a segmentation z� of the dataset, as described in Section 3.3. In the M-step, we maximize p(θ0|X, z) ∝ p(X|θ0, z)p(θ0). Assuming a non-informative hyperprior p(θ0), we maximize the likelihood in Equation 7 across all documents. The maximization is performed using a gradient-based search; the gradients are dervied by Minka (2003). This procedure is iterated until convergence or a maximum of twenty iterations. One of the key advantages of a Bayesian framework for topic segmentation is that it permits the principled combination of multiple data sources, even without labeled data. We are especially interested in cue phrases, which are explicit markers for discourse structure, such as “now” or “first” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have previously been used in supervised topic segmentation (e.g., Galley et al. 2003); we show how they can be used in an unsupervised setting. The previous section modeled lexical cohesion by treating the bag of words in each sentence as a series of draws from a multinomial language model indexed by the topic segment. To incorporate cue phrases, this generative model is modified to reflect the idea that some of the text will be topic-specific, but other terms will be topic-neutral cue phrases that express discourse structure. This idea is implemented by drawing the text at each topic boundary from a special language model φ, which is shared across all topics and all documents in the dataset. For sentences that are not at segment boundaries, the likelihood is as before: p(xt|z, o, φ) = Q i∈xt θzt,i. For sentences that immediately follow segment boundaries, we draw the first ` words from φ instead. Writing x�`) t for the ` cue words in xt, and Rt for the remaining words, the likelihood for a segment-initial sentence is, We draw φ from a symmetric Dirichlet prior φ0. Following prior work (Galley et al., 2003; Litman and Passonneau, 1995), we consider only the first word of each sentence as a potential cue phrase; thus, we set ` = 1 in all experiments. To estimate or marginalize the language models o and φ, it is necessary to maintain lexical counts for each segment and for the segment boundaries. The counts for φ are summed across every segment in the entire dataset, so shifting a boundary will affect the probability of every segment, not only the adjacent segments as before. Thus, the factorization that enabled dynamic programming inference in Section 3.3 is no longer applicable. Instead, we must resort to approximate inference. Sampling-based inference is frequently used in related Bayesian models. Such approaches build a stationary Markov chain by repeatedly sampling among the hidden variables in the model. The most commonly-used sampling-based technique is Gibbs sampling, which iteratively samples from the conditional distribution of each hidden variable (Bishop, 2006). However, Gibbs sampling is slow to converge to a stationary distribution when the hidden variables are tightly coupled. This is the case in linear topic segmentation, due to the constraint that zt E {zt−1, zt−1 + 11 (see Section 3). For this reason, we apply the more general Metropolis-Hastings algorithm, which permits sampling arbitrary transformations of the latent variables. In our framework, such transformations correspond to moves through the space of possible segmentations. A new segmentation z0 is drawn from the previous hypothesized segmentation z based on a proposal distribution q(z0|z).4 The probability of accepting a proposed transformation depends on the ratio of the joint probabilities and a correction term for asymmetries in the proposal distribution: The Metropolis-Hastings algorithm guarantees that by accepting samples at this ratio, our sampling procedure will converge to the stationary distribution for the hidden variables z. When cue phrases are included, the observation likelihood is written: As in Section 3.2, we can marginalize over the language models. We obtain a product of DCM distributions: one for each segment, and one for all cue phrases in the dataset. Metropolis-Hastings requires a proposal distribution to sample new configurations. The proposal distri4Because the cue phrase language model 0 is used across the entire dataset, transformations affect the likelihood of all documents in the corpus. For clarity, our exposition will focus on the single-document case. bution does not affect the underlying probabilistic model – Metropolis-Hastings will converge to the same underlying distribution for any non-degenerate proposal. However, a well-chosen proposal distribution can substantially speed convergence. Our basic proposal distribution selects an existing segmentation point with uniform probability, and considers a set of local moves. The proposal is constructed so that no probability mass is allocated to moves that change the order of segment boundaries, or merge two segments; one consequence of this restriction is that moves cannot add or remove segments.5 We set the proposal distribution to decrease exponentially with the move distance, thus favoring incremental transformations to the segmentation. More formally, let d(z —* z') > 0 equal the distance that the selected segmentation point is moved when we transform the segmentation from z to z'. We can write the proposal distribution q(z'  |z) a c(z —* z')d(z —* z')A, where A < 0 sets the rate of exponential decay and c is an indicator function enforcing the constraint that the moves do not reach or cross existing segmentation points.6 We can also incorporate declarative linguistic knowledge by biasing the proposal distribution in favor of moves that place boundaries near known cue phrase markers. We multiply the unnormalized chance of proposing a move to location z —* z' by a term equal to one plus the number of candidate cue phrases in the segment-initial sentences in the new configuration z', written num-cue(z'). Formally, qling(z'  |z') a (1 + num-cue(z'))q(z'  |z). We use a list of cue phrases identified by Hirschberg and Litman (1993). We evaluate our model with both the basic and linguistically-enhanced proposal distributions. As in section 3.4, we set the priors 00 and 00 using gradient-based search. In this case, we perform gradient-based optimization after epochs of 1000 max-move, where max-move is the maximum move-length, set to 5 in our experiments. These parameters affect the rate of convergence but are unrelated to the underlying probability model. In the limit of enough samples, all nonpathological settings will yield the same segmentation results. Metropolis-Hasting steps. Interleaving samplingbased inference with direct optimization of parameters can be considered a form of Monte Carlo Expectation-Maximization (MCEM; Wei and Tanner, 1990). Corpora We evaluate our approach on corpora from two different domains: transcribed meetings and written text. For multi-speaker meetings, we use the ICSI corpus of meeting transcripts (Janin et al., 2003), which is becoming a standard for speech segmentation (e.g., Galley et al. 2003; Purver et al. 2006). This dataset includes transcripts of 75 multi-party meetings, of which 25 are annotated for segment boundaries. For text, we introduce a dataset in which each document is a chapter selected from a medical textbook (Walker et al., 1990).7 The task is to divide each chapter into the sections indicated by the author. This dataset contains 227 chapters, with 1136 sections (an average of 5.00 per chapter). Each chapter contains an average of 140 sentences, giving an average of 28 sentences per segment. Metrics All experiments are evaluated in terms of the commonly-used Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) scores. Both metrics pass a window through the document, and assess whether the sentences on the edges of the window are properly segmented with respect to each other. WindowDiff is stricter in that it requires that the number of intervening segments between the two sentences be identical in the hypothesized and the reference segmentations, while Pk only asks whether the two sentences are in the same segment or not. Pk and WindowDiff are penalties, so lower values indicate better segmentations. We use the evaluation source code provided by Malioutov and Barzilay (2006). System configuration We evaluate our Bayesian approach both with and without cue phrases. Without cue phrases, we use the dynamic programming inference described in section 3.3. This system is referred to as BAYESSEG in Table 1. When adding cue phrases, we use the Metropolis-Hastings model described in 4.1. Both basic and linguisticallymotivated proposal distributions are evaluated (see Section 4.2); these are referred to as BAYESSEGCUE and BAYESSEG-CUE-PROP in the table. For the sampling-based systems, results are averaged over five runs. The initial configuration is obtained from the dynamic programming inference, and then 100,000 sampling iterations are performed. The final segmentation is obtained by annealing the last 25,000 iterations to a temperature of zero. The use of annealing to obtain a maximum a posteriori (MAP) configuration from sampling-based inference is common (e.g., Finkel 2005; Goldwater 2007). The total running time of our system is on the order of three minutes per document. Due to memory constraints, we divide the textbook dataset into ten parts, and perform inference in each part separately. We may achieve better results by performing inference over the entire dataset simultaneously, due to pooling counts for cue phrases across all documents. Baselines We compare against three competitive alternative systems from the literature: U&I (Utiyama and Isahara, 2001); LCSEG (Galley et al., 2003); MCS (Malioutov and Barzilay, 2006). All three systems are described in the related work (Section 2). In all cases, we use the publicly available executables provided by the authors. Parameter settings For LCSEG, we use the parameter values specified in the paper (Galley et al., 2003). MCS requires parameter settings to be tuned on a development set. Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006). Our system does not require parameter tuning; priors are re-estimated as described in Sections 3.4 and 4.3. U&I requires no parameter tuning, and is used “out of the box.” In all experiments, we assume that the number of desired segments is provided. Preprocessing Standard preprocessing techniques are applied to the text for all comparisons. The Porter (1980) stemming algorithm is applied to group equivalent lexical items. A set of stop-words is also removed, using the same list originally employed by several competitive systems (Choi, 2000; ter performance. BAYESSEG is the cohesion-only Bayesian system with marginalized language models. BAYESSEG-CUE is the Bayesian system with cue phrases. BAYESSEG-CUE-PROP adds the linguisticallymotivated proposal distribution. Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006). Table 1 presents the performance results for three instantiations of our Bayesian framework and three competitive alternative systems. As shown in the table, the Bayesian models achieve the best results on both metrics for both corpora. On the medical textbook corpus, the Bayesian systems achieve a raw performance gain of 2-3% with respect to all baselines on both metrics. On the ICSI meeting corpus, the Bayesian systems perform 4-5% better than the best baseline on the Pk metric, and achieve smaller improvement on the WindowDiff metric. The results on the meeting corpus also compare favorably with the topic-modeling method of Purver et al. (2006), who report a Pk of .289 and a WindowDiff of .329. Another observation from Table 1 is that the contribution of cue phrases depends on the dataset. Cue phrases improve performance on the meeting corpus, but not on the textbook corpus. The effectiveness of cue phrases as a feature depends on whether the writer or speaker uses them consistently. At the same time, the addition of cue phrases prevents the use of exact inference techniques, which may explain the decline in results for the meetings dataset. To investigate the quality of the cue phrases that our model extracts, we list its top ten cue phrases for each dataset in Table 2. Cue phrases are ranked by their chi-squared value, which is computed based on the number of occurrences for each word at the beginning of a hypothesized segment, as compared to the expectation. For cue phrases listed in bold, the chi-squared value is statistically significant at the level of p < .01, indicating that the frequency with which the cue phrase appears at the beginning of segments is unlikely to be a chance phenomenon. As shown in the left column of the table, our model has identified several strong cue phrases from the meeting dataset which appear to be linguistically plausible. Galley et al. (2003) performed a similar chi-squared analysis, but used the true segment boundaries in the labeled data; this can be thought of as a sort of ground truth. Four of the ten cue phrases identified by our system overlap with their analysis; these are indicated with asterisks. In contrast to our model’s success at extracting cue phrases from the meeting dataset, only very common words are selected for the textbook dataset. This may help to explain why cue phrases improve performance for meeting transcripts, but not for the textbook. This paper presents a novel Bayesian approach to unsupervised topic segmentation. Our algorithm is capable of incorporating both lexical cohesion and cue phrase features in a principled manner, and outperforms state-of-the-art baselines on text and transcribed speech corpora. We have developed exact and sampling-based inference techniques, both of which search only over the space of segmentations and marginalize out the associated language models. Finally, we have shown that our model provides a theoretical framework with connections to information theory, while also generalizing and justifying prior work. In the future, we hope to explore the use of similar Bayesian techniques for hierarchical segmentation, and to incorporate additional features such as prosody and speaker change information. The authors acknowledge the support of the National Science Foundation (CAREER grant IIS0448168) and the Microsoft Research Faculty Fellowship. Thanks to Aaron Adler, S. R. K. Branavan, Harr Chen, Michael Collins, Randall Davis, Dan Roy, David Sontag and the anonymous reviewers for helpful comments and suggestions. We also thank Michel Galley, Igor Malioutov, and Masao Utiyama for making their topic segmentation code publically available. Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the NSF.\"\n",
              " \"The Alignment Template Approach To Statistical Machine Translation A phrase-based statistical machine translation approach — the alignment template approach — is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source–channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different For the German–English speech we analyze the effect of various syscomponents. On the French–English Canadian the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese–English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems. A phrase-based statistical machine translation approach — the alignment template approach — is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source–channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German–English speech VERBMOBiL task, we analyze the effect of various system components. On the French–English Canadian HANSARDS task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese–English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems. Machine translation (MT) is a hard problem, because natural languages are highly complex, many words have various meanings and different possible translations, sentences might have various readings, and the relationships between linguistic entities are often vague. In addition, it is sometimes necessary to take world knowledge into account. The number of relevant dependencies is much too large and those dependencies are too complex to take them all into account in a machine translation system. Given these boundary conditions, a machine translation system has to make decisions (produce translations) given incomplete knowledge. In such a case, a principled approach to solving that problem is to use the concepts of statistical decision theory to try to make optimal decisions given incomplete knowledge. This is the goal of statistical machine translation. The use of statistical techniques in machine translation has led to dramatic improvements in the quality of research systems in recent years. For example, the statistical approaches of the VERBMOBiL evaluations (Wahlster 2000) or the U.S. National Institute of Standards and Technology (NIST)/TIDES MT evaluations 2001 through 20031 obtain the best results. In addition, the field of statistical machine translation is rapidly progressing, and the quality of systems is getting better and better. An important factor in these improvements is definitely the availability of large amounts of data for training statistical models. Yet the modeling, training, and search methods have also improved since the field of statistical machine translation was pioneered by IBM in the late 1980s and early 1990s (Brown et al. 1990; Brown et al. 1993; Berger et al. 1994). This article focuses on an important improvement, namely, the use of (generalized) phrases instead of just single words as the core elements of the statistical translation model. We describe in Section 2 the basics of our statistical translation model. We suggest the use of a log-linear model to incorporate the various knowledge sources into an overall translation system and to perform discriminative training of the free model parameters. This approach can be seen as a generalization of the originally suggested source–channel modeling framework for statistical machine translation. In Section 3, we describe the statistical alignment models used to obtain a word alignment and techniques for learning phrase translations from word alignments. Here, the term phrase just refers to a consecutive sequence of words occurring in text and has to be distinguished from the use of the term in a linguistic sense. The learned bilingual phrases are not constrained by linguistic phrase boundaries. Compared to the word-based statistical translation models in Brown et al. (1993), this model is based on a (statistical) phrase lexicon instead of a single-word-based lexicon. Looking at the results of the recent machine translation evaluations, this approach seems currently to give the best results, and an increasing number of researchers are working on different methods for learning phrase translation lexica for machine translation purposes (Marcu and Wong 2002; Venugopal, Vogel, and Waibel 2003; Tillmann 2003; Koehn, Och, and Marcu 2003). Our approach to learning a phrase translation lexicon works in two stages: In the first stage, we compute an alignment between words, and in the second stage, we extract the aligned phrase pairs. In our machine translation system, we then use generalized versions of these phrases, called alignment templates, that also include the word alignment and use word classes instead of the words themselves. In Section 4, we describe the various components of the statistical translation model. The backbone of the translation model is the alignment template feature function, which requires that a translation of a new sentence be composed of a set of alignment templates that covers the source sentence and the produced translation. Other feature functions score the well-formedness of the produced target language sentence (i.e., language model feature functions), the number of produced words, or the order of the alignment templates. Note that all components of our statistical machine translation model are purely data-driven and that there is no need for linguistically annotated corpora. This is an important advantage compared to syntax-based translation models (Yamada and Knight 2001; Gildea 2003; Charniak, Knight, and Yamada 2003) that require a parser for source or target language. In Section 5, we describe in detail our search algorithm and discuss an efficient implementation. We use a dynamic-programming-based beam search algorithm that allows a trade-off between efficiency and quality. We also discuss the use of heuristic functions to reduce the number of search errors for a fixed beam size. In Section 6, we describe various results obtained on different tasks. For the German–English VERBMOBiL task, we analyze the effect of various system compoArchitecture of the translation approach based on a log-linear modeling approach. nents. On the French–English Canadian HANSARDS task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese–English 2002 NIST machine translation evaluation it yields results that are significantly better statistically than all competing research and commercial translation systems. We are given a source (French) sentence f =f1J = f1, ... ,fj, ... , fJ, which is to be translated into a target (English) sentence e = eI1 = e1, ... , ei, ... , eI. Among all possible target sentences, we will choose the sentence with the highest probability:2 The argmax operation denotes the search problem, that is, the generation of the output sentence in the target language. As an alternative to the often used source–channel approach (Brown et al. 1993), we directly model the posterior probability Pr(eI1  |f J1) (Och and Ney 2002). An especially well-founded framework for doing this is the maximum-entropy framework (Berger, Della Pietra, and Della Pietra 1996). In this framework, we have a set of M feature functions hm(eI1,fJ1), m = 1, ... , M. For each feature function, there exists a model 2 The notational convention employed in this article is as follows. We use the symbol Pr(·) to denote general probability distributions with (nearly) no specific assumptions. In contrast, for model-based probability distributions, we use the generic symbol p(·). This approach has been suggested by Papineni, Roukos, and Ward (1997, 1998) for a natural language understanding task. We obtain the following decision rule: Hence, the time-consuming renormalization in equation (3) is not needed in search. The overall architecture of the log-linear modeling approach is summarized in Figure 1. A standard criterion on a parallel training corpus consisting of S sentence pairs {(fs, es): s = 1,. . . , S} for log-linear models is the maximum class posterior probability criterion, which can be derived from the maximum-entropy principle: This corresponds to maximizing the equivocation or maximizing the likelihood of the direct-translation model. This direct optimization of the posterior probability in Bayes’ decision rule is referred to as discriminative training (Ney 1995) because we directly take into account the overlap in the probability distributions. The optimization problem under this criterion has very nice properties: There is one unique global optimum, and there are algorithms (e.g. gradient descent) that are guaranteed to converge to the global optimum. Yet the ultimate goal is to obtain good translation quality on unseen test data. An alternative training criterion therefore directly optimizes translation quality as measured by an automatic evaluation criterion (Och 2003). Typically, the translation probability Pr(eI1  |f J1) is decomposed via additional hidden variables. To include these dependencies in our log-linear model, we extend the feature functions to include the dependence on the additional hidden variable. Using for example the alignment aJ1 as hidden variable, we obtain M feature functions of the form hm(eI1,f J 1, aJ1), m = 1, ... , M and the following model: Obviously, we can perform the same step for translation models with an even richer set of hidden variables than only the alignment aJ1. In this section, we describe methods for learning the single-word and phrase-based translation lexica that are the basis of the machine translation system described in Section 4. First, we introduce the basic concepts of statistical alignment models, which are used to learn word alignment. Then, we describe how these alignments can be used to learn bilingual phrasal translations. In (statistical) alignment models Pr(f J 1,aJ 1 eI1), a “hidden” alignment a = aJ1 is introduced that describes a mapping from a source position j to a target position aj. The relationship between the translation model and the alignment model is given by The alignment aJ1 may contain alignments aj = 0 with the “empty” word e0 to account for source words that are not aligned with any target word. In general, the statistical model depends on a set of unknown parameters θ that is learned from training data. To express the dependence of the model on the parameter set, we use the following notation: A detailed description of different specific statistical alignment models can be found in Brown et al. (1993) and Och and Ney (2003). Here, we use the hidden Markov model (HMM) alignment model (Vogel, Ney, and Tillmann 1996) and Model 4 of Brown et al. (1993) to compute the word alignment for the parallel training corpus. To train the unknown parameters θ, we are given a parallel training corpus consisting of S sentence pairs j(fs, es): s = 1, ... , Sj. For each sentence pair (fs, es), the alignment variable is denoted by a = aJ1. The unknown parameters θ are determined by maximizing the likelihood on the parallel training corpus: This optimization can be performed using the expectation maximization (EM) algorithm (Dempster, Laird, and Rubin 1977). For a given sentence pair there are a large number of alignments. The alignment ˆaJ1 that has the highest probability (under a certain model) is also called the Viterbi alignment (of that model): A detailed comparison of the quality of these Viterbi alignments for various statistical alignment models compared to human-made word alignments can be found in Och and Ney (2003). The baseline alignment model does not allow a source word to be aligned with two or more target words. Therefore, lexical correspondences like the German compound word Zahnarzttermin for dentist’s appointment cause problems because a single source word must be mapped onto two or more target words. Therefore, the resulting Viterbi alignment of the standard alignment models has a systematic loss in recall. Here, we Example of a (symmetrized) word alignment (VERBMOBIL task). describe various methods for performing a symmetrization of our directed statistical alignment models by applying a heuristic postprocessing step that combines the alignments in both translation directions (source to target, target to source). Figure 2 shows an example of a symmetrized alignment. To solve this problem, we train in both translation directions. For each sentence pair, we compute two Viterbi alignments aJ1 and bI1. Let A1 = f(aj, j)  |aj > 01 and A2 = f(i, bi)  |bi > 01 denote the sets of alignments in the two Viterbi alignments. To increase the quality of the alignments, we can combine (symmetrize) A1 and A2 into one alignment matrix A using one of the following combination methods: alignment A1 or in the alignment A2 if neither fj nor ei have an alignment in A, or if the following conditions both hold: Obviously, the intersection yields an alignment consisting of only one-to-one alignments with a higher precision and a lower recall. The union yields a higher recall and a lower precision of the combined alignment. The refined alignment method is often able to improve precision and recall compared to the nonsymmetrized alignments. Whether a higher precision or a higher recall is preferred depends on the final application of the word alignment. For the purpose of statistical MT, it seems that a higher recall is more important. Therefore, we use the union or the refined combination method to obtain a symmetrized alignment matrix. The resulting symmetrized alignments are then used to train single-word-based translation lexica p(e  |f) by computing relative frequencies using the count N(e,f) of how many times e and f are aligned divided by the count N(f ) of how many times the word f occurs: In this section, we present a method for learning relationships between whole phrases of m source language words and n target language words. This algorithm, which will be called phrase-extract, takes as input a general word alignment matrix (Section 3.2). The output is a set of bilingual phrases. In the following, we describe the criterion that defines the set of phrases that is consistent with the word alignment matrix: Hence, the set of all bilingual phrases that are consistent with the alignment is constituted by all bilingual phrase pairs in which all words within the source language phrase are aligned only with the words of the target language phrase and the words of the target language phrase are aligned only with the words of the source language phrase. Note that we require that at least one word in the source language phrase be aligned with at least one word of the target language phrase. As a result there are no empty source or target language phrases that would correspond to the “empty word” of the word-based statistical alignment models. These phrases can be computed straightforwardly by enumerating all possible phrases in one language and checking whether the aligned words in the other language are consecutive, with the possible exception of words that are not aligned at all. Figure 3 gives the algorithm phrase-extract that computes the phrases. The algorithm takes into account possibly unaligned words at the boundaries of the source or target language phrases. Table 1 shows the bilingual phrases containing between two and seven words that result from the application of this algorithm to the alignment of Figure 2. Examples of two- to seven-word bilingual phrases obtained by applying the algorithm phrase-extract to the alignment of Figure 2. ja , yes , ja , ich yes , I ja , ich denke mal yes , I think ja , ich denke mal , yes , I think , ja , ich denke mal , also yes, I think, well ,ich , I , ich denke mal , I think , ich denke mal, , I think , , ich denke mal, also , I think, well , ich denke mal, also wir , I think, well we ich denke mal I think ich denke mal, I think, ich denke mal, also I think, well ich denke mal, also wir I think, well we ich denke mal , also wir wollten I think, well we plan to denke mal, think , denke mal , also think, well denke mal , also wir think, well we denke mal, also wir wollten think, well we plan to , also , well , also wir , well we , also wir wollten , well we plan to also wir well we also wir wollten well we plan to wir wollten we plan to in unserer in our in unserer Abteilung in our department in unserer Abteilung ein neues Netzwerk a new network in our department in unserer Abteilung ein neues Netzwerk set up a new network in our department aufbauen unserer Abteilung our department ein neues a new ein neues Netzwerk a new network ein neues Netzwerk aufbauen set up a new network neues Netzwerk new network It should be emphasized that this constraint to consecutive phrases limits the expressive power. If a consecutive phrase in one language is translated into two or three nonconsecutive phrases in the other language, there is no corresponding bilingual phrase pair learned by this approach. In principle, this approach to learning phrases from a word-aligned corpus could be extended straightforwardly to handle nonconsecutive phrases in source and target language as well. Informal experiments have shown that allowing for nonconsecutive phrases significantly increases the number of extracted phrases and especially increases the percentage of wrong phrases. Therefore, we consider only consecutive phrases. In the following, we add generalization capability to the bilingual phrase lexicon by replacing words with word classes and also by storing the alignment information for each phrase pair. These generalized and alignment-annotated phrase pairs are called alignment templates. Formally, an alignment template z is a triple (FJy1 , EI�1 , ˜A) Algorithm phrase-extract for extracting phrases from a word-aligned sentence pair. Here quasi-consecutive(TP) is a predicate that tests whether the set of words TP is consecutive, with the possible exception of words that are not aligned. that describes the alignment A˜ between a source class sequence FJy1 and a target class sequence EI�1 . If each word corresponds to one class, an alignment template corresponds to a bilingual phrase together with an alignment within this phrase. Figure 4 shows examples of alignment templates. The alignment A˜ is represented as a matrix with J' · (I' + 1) binary elements. A matrix element with value 1 means that the words at the corresponding positions are aligned, and the value 0 means that the words are not aligned. If a source word is not aligned with a target word, then it is aligned with the empty word e0, which is at the imaginary position i = 0. The classes used in FJy1 and EI�1 are automatically trained bilingual classes using the method described in Och (1999) and constitute a partition of the vocabulary of source and target language. In general, we are not limited to disjoint classes as long as each specific instance of a word is disambiguated, that is, uniquely belongs to a specific class. In the following, we use the class function C to map words to their classes. Hence, it would be possible to employ parts-of-speech or semantic categories instead of the automatically trained word classes used here. The use of classes instead of the words themselves has the advantage of better generalization. For example, if there exist classes in source and target language that contain town names, it is possible that an alignment template learned using a specific town name can be generalized to other town names. In the following, e˜ and f˜ denote target and source phrases, respectively. To train the probability of applying an alignment template p(z = (FJy1 , EI~1 , ˜A)  |f˜), we use an extended version of the algorithm phrase-extract from Section 3.3. All bilingual phrases that are consistent with the alignment are extracted together with the alignment within this bilingual phrase. Thus, we obtain a count N(z) of how often an alignment template occurred in the aligned training corpus. The probability of using an alignment template to translate a specific source language phrase f˜ is estimated by means of relative frequency: To reduce the memory requirement of the alignment templates, we compute these probabilities only for phrases up to a certain maximal length in the source language. Depending on the size of the corpus, the maximal length in the experiments is between four and seven words. In addition, we remove alignment templates that have a probability lower than a certain threshold. In the experiments, we use a threshold of 0.01. It should be emphasized that this algorithm for computing aligned phrase pairs and their associated probabilities is very easy to implement. The joint translation model suggested by Marcu and Wong (2002) tries to learn phrases as part of a full EM algorithm, which leads to very large memory requirements and a rather complicated training algorithm. A comparison of the two approaches can be found in Koehn, Och, and Marcu (2003). To describe our translation model based on the alignment templates described in the previous section in a formal way, we first decompose both the source sentence f1J and the target sentence eI1 into a sequence of phrases (k = 1,...,K): Note that there are a large number of possible segmentations of a sentence pair into K phrase pairs. In the following, we will describe the model for a specific segmentation. Eventually, however, a model can be described in which the specific segmentation is not known when new text is translated. Hence, as part of the overall search process (Section 5), we also search for the optimal segmentation. To allow possible reordering of phrases, we introduce an alignment on the phrase level πK1 between the source phrases f˜1K and the target phrases ˜eK1. Hence, πK1 is a permutation of the phrase positions 1, ... , K and indicates that the phrases ˜ek and ˜fπk are translations of one another. We assume that for the translation between these phrases a specific alignment template zk is used: ˜ek zk ˜fπk ←→ Hence, our model has the following hidden variables: Figure 5 gives an example of the word alignment and phrase alignment of a German–English sentence pair. We describe our model using a log-linear modeling approach. Hence, all knowledge sources are described as feature functions that include the given source language string f J1, the target language string eI1, and the above-stated hidden variables. Hence, we have the following functional form of all feature functions: Figure 6 gives an overview of the decisions made in the alignment template model. First, the source sentence words fJ1 are grouped into phrases f˜1K. For each phrase f˜ an alignment template z is chosen and the sequence of chosen alignment templates is reordered (according to πK1 ). Then, every phrase f˜ produces its translation e˜ (using the corresponding alignment template z). Finally, the sequence of phrases ˜eK1 constitutes the sequence of words eI1. Dependencies in the alignment template model. Och and Ney The Alignment Template Approach to Statistical Machine Translation 4.1.1 Alignment Template Selection. To score the use of an alignment template, we use the probability p(z  |f˜) defined in Section 3. We establish a corresponding feature Here, jπk−1 + 1 is the position of the first word of alignment template zk in the source language sentence and jπk is the position of the last word of that alignment template. Note that this feature function requires that a translation of a new sentence be composed of a set of alignment templates that covers both the source sentence and the produced translation. There is no notion of “empty phrase” that corresponds to the “empty word” in word-based statistical alignment models. The alignment on the phrase level is actually a permutation, and no insertions or deletions are allowed. 4.1.2 Word Selection. For scoring the use of target language words, we use a lexicon probability p(e  |f), which is estimated using relative frequencies as described in Section 3.2. The target word e depends on the aligned source words. If we denote the resulting word alignment matrix by A := AπKAK and the predicted word class for word For p(ei  |{fj  |(i,j) ∈ A}) we use a uniform mixture of a single-word model p(e  |f), which is constrained to predict only words that are in the predicted word class Ei: A disadvantage of this model is that the word order is ignored in the translation model. The translations the day after tomorrow or after the day tomorrow for the German word ¨ubermorgen receive an identical probability. Yet the first one should obtain a significantly higher probability. Hence, we also include a dependence on the word positions in the lexicon model p(e  |f, i, j): Here, [(i', j) ∈ A] is 1 if (i', j) ∈ A and 0 otherwise. As a result, the word ei depends not only on the aligned French word fj, but also on the number of preceding French words aligned with ei and on the number of the preceding English words aligned with fj. This model distinguishes the positions within a phrasal translation. The number of parameters of p(e  |f,i,j) is significantly higher than that of p(e  |f) alone. Hence, there is a data estimation problem especially for words that rarely occur. Therefore, we linearly interpolate the models p(e  |f) and p(e  |f, i, j). very often a monotone alignment is a correct alignment. Hence, the feature function hAL measures the “amount of nonmonotonicity” by summing over the distance (in the source language) of alignment templates that are consecutive in the target language: Here, jπ0 is defined to equal 0 and jπK+1−1 is defined to equal J. The above-stated sum includes k = K + 1 to include the distance from the end position of the last phrase to the end of sentence. The sequence of K = 6 alignment templates in Figure 5 corresponds to the following sum of seven jump distances: 0 + 0 + 1 + 3 + 2 + 0 + 0 = 6. 4.1.4 Language Model Features. As a default language model feature, we use a standard backing-off word-based trigram language model (Ney, Generet, and Wessel 1995): The use of the language model feature in equation (18) helps take long-range dependencies better into account. Without this feature, we typically observe that the produced sentences tend to be too short. 4.1.6 Conventional Lexicon. We also use a feature that counts how many entries of a conventional lexicon co-occur in the given sentence pair. Therefore, the weight for the provided conventional dictionary can be learned: The intuition is that the conventional dictionary LEX is more reliable than the automatically trained lexicon and therefore should get a larger weight. 4.1.7 Additional Features. A major advantage of the log-linear modeling approach used is that we can add numerous features that deal with specific problems of the baseline statistical MT system. Here, we will restrict ourselves to the described set of features. Yet we could use grammatical features that relate certain grammatical dependencies of source and target language. For example, using a function k(·) that counts how many arguments the main verb of a sentence has in the source or target sentence, we can define the following feature, which has a nonzero value if the verb in each of the two sentences has the same number of arguments: In the same way, we can introduce semantic features or pragmatic features such as the dialogue act classification. For the three different tasks on which we report results, we use two different training approaches. For the VERBMOBiL task, we train the model parameters λM1 according to the maximum class posterior probability criterion (equation (4)). For the French– English HANSARDS task and the Chinese–English NIST task, we simply tune the model parameters by coordinate descent on held-out data with respect to the automatic evaluation metric employed, using as a starting point the model parameters obtained on the VERBMOBiL task. Note that this tuning depends on the starting point of the model parameters and is not guaranteed to converge to the global optimum on the training data. As a result, this approach is limited to a very small number of model parameters. An efficient algorithm for performing this tuning for a larger number of model parameters can be found in Och (2003). A standard approach to training the log-linear model parameters of the maximum class posterior probability criterion is the GIS (Generalized Iterative Scaling) algorithm (Darroch and Ratcliff 1972). To apply this algorithm, we have to solve various practical problems. The renormalization needed in equation (3) requires a sum over many possible sentences, for which we do not know of an efficient algorithm. Hence, we approximate this sum by extracting a large set of highly probable sentences as a sample from the space of all possible sentences (n-best approximation). The set of considered sentences is computed by means of an appropriately extended version of the search algorithm described in Section 5. Using an n-best approximation, we might face the problem that the parameters trained with the GIS algorithm yield worse translation results even on the training corpus. This can happen because with the modified model scaling factors, the n-best list can change significantly and can include sentences that have not been taken into account in training. Using these sentences, the new model parameters might perform worse than the old model parameters. To avoid this problem, we proceed as follows. In a first step, we perform a search, compute an n-best list, and use this n-best list to train the model parameters. Second, we use the new model parameters in a new search and compute a new n-best list, which is combined with the existing n-best list. Third, using this extended n-best list, new model parameters are computed. This process is iterated until the resulting n-best list does not change. In this algorithm, convergence is guaranteed, as in the limit the n-best list will contain all possible translations. In practice, the algorithm converges after five to seven iterations. In our experiments this final n-best list contains about 500–1000 alternative translations. We might have the problem that none of the given reference translations is part of the n-best list because the n-best list is too small or because the search algorithm performs pruning which in principle limits the possible translations that can be produced given a certain input sentence. To solve this problem, we define as reference translation for maximum-entropy training each sentence that has the minimal number of word errors with respect to any of the reference translations in the n-best list. More details of the training procedure can be found in Och and Ney (2002). In this section, we describe an efficient search architecture for the alignment template model. In general, the search problem for statistical MT even using only Model 1 of Brown et al. (1993) is NP-complete (Knight 1999). Therefore, we cannot expect to develop efficient search algorithms that are guaranteed to solve the problem without search errors. Yet for practical applications it is acceptable to commit some search errors (Section 6.1.2). Hence, the art of developing a search algorithm lies in finding suitable approximations and heuristics that allow an efficient search without committing too many search errors. In the development of the search algorithm described in this section, our main aim is that the search algorithm should be efficient. It should be possible to translate a sentence of reasonable length within a few seconds of computing time. We accept that the search algorithm sometimes results in search errors, as long as the impact on translation quality is minor. Yet it should be possible to reduce the number of search errors by increasing computing time. In the limit, it should be possible to search without search errors. The search algorithm should not impose any principal limitations. We also expect that the search algorithm be able to scale up to very long sentences with an acceptable computing time. To meet these aims, it is necessary to have a mechanism that restricts the search effort. We accomplish such a restriction by searching in a breadth-first manner with pruning: beam search. In pruning, we constrain the set of considered translation candidates (the “beam”) only to the promising ones. We compare in beam search those hypotheses that cover different parts of the input sentence. This makes the comparison of the probabilities problematic. Therefore, we integrate an admissible estimation of the remaining probabilities to arrive at a complete translation (Section 5.6) (Garcia-Varea, Casacuberta, and Ney 1998; Garcia-Varea et al. 2001), as does the original IBM stack search decoder (Berger et al. 1994). All these simplifications ultimately make the search problem simpler but introduce fundamental search errors. In the following, we describe our search algorithm based on the concept of beam search, which allows a trade-off between efficiency and quality by adjusting the size of the beam. The search algorithm can be easily adapted to other phrase-based translation models. For single-word-based search in MT, a similar algorithm has been described in Tillmann and Ney (2003). Putting everything together and performing search in maximum approximation, we obtain the following decision rule: Using the four feature functions AT, AL, WRD, and LM, we obtain the following decision rule:3 Here, we have grouped the contributions of the various feature functions into those for each word (from LM and WRD, expression (24)), those for every alignment template (from AT and AL, expression (25)), and those for the end of sentence (expression (26)), which includes a term logp(EOS  |eI−1,eI) for the end-of-sentence language model probability. To extend this decision rule for the word penalty (WP) feature function, we simply obtain an additional term AWP for each word. The class-based 5-gram language model (CLM) can be included like the trigram language model. Note that all these feature functions decompose nicely into contributions for each produced target language word or for each covered source language word. This makes it possible to develop an efficient dynamic programming search algorithm. Not all feature functions have this nice property: For the conventional lexicon feature function (LEX), we obtain an additional term in our decision rule which depends on the full sentence. Therefore, this feature function will not be integrated in the dynamic programming search but instead will be used to rerank the set of candidate translations produced by the search. We have to structure the search space in a suitable way to search efficiently. In our search algorithm, we generate search hypotheses that correspond to prefixes of target language sentences. Each hypothesis is the translation of a part of the source language sentence. A hypothesis is extended by appending one target word. The set of all hypotheses can be structured as a graph with a source node representing the sentence start, goal nodes representing complete translations, and intermediate nodes representing partial translations. There is a directed edge between hypotheses n1 and n2 if the hypothesis n2 is obtained by appending one word to hypothesis n1. Each edge has associated costs resulting from the contributions of all feature functions. Finally, our search problem can be reformulated as finding the optimal path through this graph. ˜ In the first step, we determine the set of all source phrases in f for which an applicable alignment template exists. Every possible application of an alignment template z = (FJy1 , EI~1 , ˜A) to a subsequence f j+J�−1 of the source sentence is called an alignment j template instantiation Z = (z, j). Hence, the set of all alignment template instantiations for the source sentence fJ1 is If the source sentence contains words that have not been seen in the training data, we introduce a new alignment template that performs a one-to-one translation of each of these words by itself. In the second step, we determine a set of probable target language words for each target word position in the alignment template instantiation. Only these words are then hypothesized in the search. We call this selection of highly probable words observation pruning (Tillmann and Ney 2000). As a criterion for a word e at position i in the alignment template instantiation, we use In our experiments, we hypothesize only the five best-scoring words. A decision is a triple d = (Z, e,l) consisting of an alignment template instantiation Z, the generated word e, and the index l of the generated word in Z. A hypothesis n corresponds to a valid sequence of decisions di1. The possible decisions are as follows: The resulting decision score corresponds to the contribution of expression (26). Any valid and complete sequence of decisions dI+1 1 uniquely corresponds to a certain translation eI1, a segmentation into K phrases, a phrase alignment πK1 , and a sequence of alignment template instantiations zK1 . The sum of the decision scores is equal to the corresponding score described in expressions (24)–(26). A straightforward representation of all hypotheses would be the prefix tree of all possible sequences of decisions. Obviously, there would be a large redundancy in this search space representation, because there are many search nodes that are indistinguishable in the sense that the subtrees following these search nodes are identical. We can recombine these identical search nodes; that is, we have to maintain only the most probable hypothesis (Bellman 1957). In general, the criterion for recombining a set of nodes is that the hypotheses can be distinguished by neither language nor translation model. In performing recombination, Algorithm for breadth-first search with pruning. we obtain a search graph instead of a search tree. The exact criterion for performing recombination for the alignment templates is described in Section 5.5. Theoretically, we could use any graph search algorithm to search the optimal path in the search space. We use a breadth-first search algorithm with pruning. This approach offers very good possibilities for adjusting the trade-off between quality and efficiency. In pruning, we always compare hypotheses that have produced the same number of target words. Figure 7 shows a structogram of the algorithm. As the search space increases exponentially, it is not possible to explicitly represent it. Therefore, we represent the search space implicitly, using the functions Extend and Recombine. The function Extend produces new hypotheses extending the current hypothesis by one word. Some hypotheses might be identical or indistinguishable by the language and translation models. These are recombined by the function Recombine. We expand the search space such that only hypotheses with the same number of target language words are recombined. In the pruning step, we use two different types of pruning. First, we perform pruning relative to the score Qˆ of the current best hypothesis. We ignore all hypotheses that have a probability lower than log(tp)+ˆQ, where tp is an adjustable pruning parameter. This type of pruning can be performed when the hypothesis extensions are computed. Second, in histogram pruning (Steinbiss, Tran, and Ney 1994), we maintain only the best Np hypotheses. The two pruning parameters tp and Np have to be optimized with respect to the trade-off between efficiency and quality. In this section, we describe various issues involved in performing an efficient implementation of a search algorithm for the alignment template approach. A very important design decision in the implementation is the representation of a hypothesis. Theoretically, it would be possible to represent search hypotheses only by the associated decision and a back-pointer to the previous hypothesis. Yet this would be a very inefficient representation for the implementation of the operations that have to be performed in the search. The hypothesis representation should contain all information required to perform efficiently the computations needed in the search but should contain no more information than that, to keep the memory consumption small. In search, we produce hypotheses n, each of which contains the following information: We compare in beam search those hypotheses that cover different parts of the input sentence. This makes the comparison of the probabilities problematic. Therefore, we integrate an admissible estimation of the remaining probabilities to arrive at a complete translation. Details of the heuristic function for the alignment templates are provided in the next section. To improve the comparability of search hypotheses, we introduce heuristic functions. A heuristic function estimates the probabilities of reaching the goal node from a certain search node. An admissible heuristic function is always an optimistic estimate; that is, for each search node, the product of edge probabilities of reaching a goal node is always equal to or smaller than the estimated probability. For an A*-based search algorithm, a good heuristic function is crucial to being able to translate long sentences. For a beam search algorithm, the heuristic function has a different motivation. It is used to improve the scoring of search hypotheses. The goal is to make the probabilities of all hypotheses more comparable, in order to minimize the chance that the hypothesis leading to the optimal translation is pruned away. Heuristic functions for search in statistical MT have been used in Wang and Waibel (1997) and Och, Ueffing, and Ney (2001). Wang and Waibel (1997) have described a simple heuristic function for Model 2 of Brown et al. (1993) that was not admissible. Och, Ueffing, and Ney (2001) have described an admissible heuristic function for Model 4 of Brown et al. (1993) and an almost-admissible heuristic function that is empirically obtained. We have to keep in mind that a heuristic function is helpful only if the overhead introduced in computing the heuristic function is more than compensated for by the gain obtained through a better pruning of search hypotheses. The heuristic functions described in the following are designed such that their computation can be performed efficiently. The basic idea for developing a heuristic function for an alignment model is that all source sentence positions that have not been covered so far still have to be translated to complete the sentence. If we have an estimation rX(j) of the optimal score for translating position j, then the value of the heuristic function RX(n) for a node n can be inferred by summing over the contribution for every position j that is not in the coverage vector c(n) (here X denotes different possibilities to choose the heuristic The situation in the case of the alignment template approach is more complicated, as not every word is translated alone, but typically the words are translated in context. Therefore, the basic quantity for the heuristic function in the case of the alignment template approach is a function r(Z) that assigns to every alignment template instantiation Z a maximal probability. Using r(Z), we can induce a position-dependent heuristic function r(j): Here, J(Z) denotes the number of source language words produced by the alignment template instantiation Z and j(Z) denotes the position of the first source language word. It can be easily shown that if r(Z) is admissible, then r(j) is also admissible. We have to show that for all nonoverlapping sequences ZK1 the following holds: Here, k(j) denotes the phrase index k that includes the target language word position j. In the following, we develop various heuristic functions r(Z) of increasing complexity. The simplest realization of a heuristic function r(Z) takes into account only the prior probability of an alignment template instantiation: The language model can be incorporated by considering that for each target word there exists an optimal language model probability: Here, we assume a trigram language model. In general, it is necessary to maximize over all possible different language model histories. We can also combine the language model and the lexicon model into one heuristic function: To include the phrase alignment probability in the heuristic function, we compute the minimum sum of all jump widths that is needed to complete the translation. This sum can be computed efficiently using the algorithm shown in Figure 8. Then, an admissible heuristic function for the jump width is obtained by Combining all the heuristic functions for the various models, we obtain as final heuristic function for a search hypothesis n We present results on the VERBMOBiL task, which is a speech translation task in the domain of appointment scheduling, travel planning, and hotel reservation (Wahlster 2000). Table 2 shows the corpus statistics for this task. We use a training corpus, which is used to train the alignment template model and the language models, a development corpus, which is used to estimate the model scaling factors, and a test corpus. On average, 3.32 reference translations for the development corpus and 5.14 reference translations for the test corpus are used. A standard vocabulary had been defined for the various speech recognizers used in VERBMOBiL. However, not all words of this vocabulary were observed in the training corpus. Therefore, the translation vocabulary was extended semiautomatically by adding about 13,000 German–English entries from an online bilingual lexicon available on the Web. The resulting lexicon contained not only word-word entries, but also multi-word translations, especially for the large number of German compound words. To counteract the sparseness of the training data, a couple of straightforward rule-based preprocessing steps were applied before any other type of processing: So far, in machine translation research there is no generally accepted criterion for the evaluation of experimental results. Therefore, we use various criteria. In the following experiments, we use: In the following, we analyze the effect of various system components: alignment template length, search pruning, and language model n-gram size. A systematic evaluation of the alignment template system comparing it with other translation approaches (e.g., rule-based) has been performed in the VERBMOBiL project and is described in Tessiore and von Hahn (2000). There, the alignment-template-based system achieved a significantly larger number of “approximately correct” translations than the competing translation systems (Ney, Och, and Vogel 2001). 6.1.1 Effect of Alignment Template Length. Table 3 shows the effect of constraining the maximum length of the alignment templates in the source language. Typically, it is necessary to restrict the alignment template length to keep memory requirements low. We see that using alignment templates with only one or two words in the source languages results in very bad translation quality. Yet using alignment templates with lengths as small as three words yields optimal results. algorithm misses the most probable translation and produces a translation which is less probable. As we typically cannot efficiently compute the probability of the optimal translation, we cannot efficiently compute the number of search errors. Yet we can compute a lower bound on the number of search errors by comparing the translation found under specific pruning thresholds with the best translation that we have found using very conservative pruning thresholds. Tables 4 and 5 show the effect of the pruning parameter tp with the histogram pruning parameter Np = 50,000. Tables 6 and 7 show the effect of the pruning parameter Np with the pruning parameter tp = 10−12. In all four tables, we provide the results for using no heuristic functions and three variants of an increasingly informative heuristic function. The first is an estimate of the alignment template and the lexicon probability (AT+WRD), the second adds an estimate of the language model (+LM) probability, and the third also adds the alignment probability (+AL). These heuristic functions are described in Section 5.6. Without a heuristic function, even more than a hundred seconds per sentence cannot guarantee search-error-free translation. We draw the conclusion that a good heuristic function is very important to obtaining an efficient search algorithm. In addition, the search errors have a more severe effect on the error rates if we do not use a heuristic function. If we compare the error rates in Table 7, which correspond to about 55 search errors in Table 6, we obtain an mWER of 36.7% (53 search errors) using no heuristic function and an mWER of 32.6% (57 search errors) using the combined heuristic function. The reason is that without a heuristic function, often the “easy” part of the input sentence is translated first. This yields severe reordering errors. n-gram-based language models. Ideally, we would like to take into account long-range dependencies. Yet long n-grams are seen rarely and are therefore rarely used on unseen data. Therefore, we expect that extending the history length will at some point not improve further translation quality. Table 8 shows the effect of the length of the language model history on translation quality. We see that the language model perplexity improves from 4,781 for a unigram model to 29.9 for a trigram model. The corresponding translation quality improves from an mWER of 45.9% to an mWER of 31.8%. The largest effect seems to come from taking into account the bigram dependence, which achieves an mWER of 32.9%. If we perform log-linear interpolation of a trigram model with a class-based 5-gram model, we observe an additional small improvement in translation quality to an mWER of 30.9%. The HANSARDS task involves the proceedings of the Canadian parliament, which are kept by law in both French and English. About three million parallel sentences of this bilingual data have been made available by the Linguistic Data Consortium (LDC). Here, we use a subset of the data containing only sentences of up to 30 words. Table 9 shows the training and test corpus statistics. The results for French to English and for English to French are shown in Table 10. Because of memory limitations, the maximum alignment template length has been restricted to four words. We compare here against the single-word-based search for Model 4 described in Tillmann (2001). We see that the alignment template approach obtains significantly better results than the single-word-based search. Various statistical, example-based, and rule-based MT systems for a Chinese–English news domain were evaluated in the NIST 2002 MT evaluation.4 Using the alignment template approach described in this article, we participated in these evaluations. The problem domain is the translation of Chinese news text into English. Table 11 gives an overview on the training and test data. The English vocabulary consists of fullform words that have been converted to lowercase letters. The number of sentences has been artificially increased by adding certain parts of the original training material more than once to the training corpus, in order to give larger weight to those parts of the training corpus that consist of high-quality aligned Chinese news text and are therefore expected to be especially helpful for the translation of the test data. The Chinese language poses special problems because the boundaries of Chinese words are not marked. Chinese text is provided as a sequence of characters, and it is unclear which characters have to be grouped together to obtain entities that can be interpreted as words. For statistical MT, it would be possible to ignore this fact and treat the Chinese characters as elementary units and translate them into English. Yet preliminary experiments showed that the existing alignment models produce better results if the Chinese characters are segmented in a preprocessing step into single words. We use the LDC segmentation tool.5 For the English corpus, the following preprocessing steps are applied. First, the corpus is tokenized; it is then segmented into sentences, and all uppercase characters are converted to lowercase. As the final evaluation criterion does not distinguish case, it is not necessary to deal with the case information. Then, the preprocessed Chinese and English corpora are sentence aligned in which the lengths of the source and target sentences are significantly different. From the resulting corpus, we automatically replace translations. In addition, only sentences with less than 60 words in English and Chinese are used. To improve the translation of Chinese numbers, we use a categorization of Chinese number and date expressions. For the statistical learning, all number and date expressions are replaced with one of two generic symbols, $number or $date. The number and date expressions are subjected to a rule-based translation by simple lexicon lookup. The translation of the number and date expressions is inserted into the output using the alignment information. For Chinese and English, this categorization is implemented independently of the other language. To evaluate MT quality on this task, NIST made available the NIST-09 evaluation tool. This tool provides a modified BLEU score by computing a weighted precision of n-grams modified by a length penalty for very short translations. Table 12 shows the results of the official evaluation performed by NIST in June 2002. With a score of 7.65, the results obtained were statistically significantly better than any other competing approach. Differences in the NIST score larger than 0.12 are statistically significant at the 95% level. We conclude that the developed alignment template approach is also applicable to unrelated language pairs such as Chinese–English and that the developed statistical models indeed seem to be largely language-independent. Table 13 shows various example translations. We have presented a framework for statistical MT for natural languages which is more general than the widely used source–channel approach. It allows a baseline MT been achieved in 1995 in the economic construction of China’s fourteen border cities open to foreigners. Translation Xinhua News Agency, Beijing, February 12—China’s opening up to the outside world of the 1995 in the fourteen border pleased to obtain the construction of the economy. Reference Foreign Investment in Jiangsu’s Agriculture on the Increase Translation To increase the operation of foreign investment in Jiangsu agriculture Reference According to the data provided today by the Ministry of Foreign Trade and Economic Cooperation, as of November this year, China has actually utilized 46.959 billion US dollars of foreign capital, including 40.007 billion US dollars of direct investment from foreign businessmen. Translation The external economic and trade cooperation Department today provided that this year, the foreign capital actually utilized by China on November to US $46.959 billion, including of foreign company direct investment was US $40.007 billion. Reference According to officials from the Provincial Department of Agriculture and Forestry of Jiangsu, the ”Three-Capital” ventures approved by agencies within the agricultural system of Jiangsu Province since 1994 have numbered more than 500 and have utilized over 700 million US dollars worth of foreign capital, respectively three times and seven times more than in 1993. Translation Jiangsu Province for the Secretaries said that, from the 1994 years, Jiangsu Province system the approval of the “three-funded” enterprises, there are more than 500, foreign investment utilization rate of more than US $700 million, 1993 years before three and seven. Reference The actual amount of foreign capital has also increased more than 30% as compared with the same period last year. Translation The actual amount of foreign investment has increased by more than 30% compared with the same period last year. Reference Import and Export in Pudong New District Exceeding 9 billion US dollars This Year Translation Foreign trade imports and exports of this year to the Pudong new Region exceeds US $9 billion system to be extended easily by adding new feature functions. We have described the alignment template approach for statistical machine translation, which uses two different alignment levels: a phrase-level alignment between phrases and a wordlevel alignment between single words. As a result the context of words has a greater influence, and the changes in word order from source to target language can be learned explicitly. An advantage of this method is that machine translation is learned fully automatically through the use of a bilingual training corpus. We have shown that the presented approach is capable of achieving better translation results on various tasks compared to other statistical, example-based, or rule-based translation systems. This is especially interesting, as our system is structured simpler than many competing systems. We expect that better translation can be achieved by using models that go beyond the flat phrase segmentation that we perform in our model. A promising avenue is to gradually extend the model to take into account to some extent the recursive structure of natural languages using ideas from Wu and Wong (1998) or Alshawi, Bangalore, and Douglas (2000). We expect other improvements as well from learning nonconsecutive phrases in source or target language and from better generalization methods for the learned-phrase pairs. The work reported here was carried out while the first author was with the Lehrstuhl f¨ur Informatik VI, Computer Science Department, RWTH Aachen–University of Technology.\"\n",
              " \"A Prosodic Analysis Of Discourse Segments In Direction-Giving Monologues This paper reports on corpus-based research into the relationship between intonational variation and discourse structure. We examine the effects of speaking style (read versus spontaneous) and of discourse segmentation method (text-alone versus text-and-speech) on the nature of this relationship. We also compare the acoustic-prosodic features of initial, medial, and final utterances in a discourse segment. This paper presents empirical support for the assumption long held by computational linguists, that intonation can provide valuable cues for discourse processing. The relationship between intonational variation and discourse structure has been explored in a new corpus of direction-giving monologues. We examine the effects of speaking style (read versus spontaneous) and of discourse segmentation method (text-alone versus text-and-speech) on the nature of this relationship. We also compare the acousticprosodic features of initial, medial, and final utterances in a discourse segment. A better understanding of the role of intonation in conveying discourse structure will enable improvements in the naturalness of intonational variation in text-to-speech systems as well as in algorithms for recognizing discourse structure in speech-understanding systems. It has long been assumed in computational linguistics that discourse structure plays an important role in Natural Language Understanding tasks such as identifying speaker intentions and resolving anaphoric reference. Previous research has found *The second author was partially supported by NSF Grants No. IRI-90-09018, No. IRI-93-08173, and No. CDA-94-01024 at Harvard University and by AT&T Bell Laboratories. that discourse structural information can be inferred from orthographic cues in text, such as paragraphing and punctuation; from linguistic cues in text or speech, such as CUE PHRASES1 (Cohen, 1984; Reichman, 1985; Grosz and Sidner, 1986; Passonneau and Litman, 1993; Passonneau and Litman, to appear) and other lexical cues (Hinkelman and Allen, 1989); from variation in referring expressions (Linde, 1979; Levy, 1984; Grosz and Sidner, 1986; Webber, 1988; Song and Cohen, 1991; Passonneau and Litman, 1993), tense, and aspect (Schubert and Hwang, 1990; Song and Cohen, 1991); from knowledge of the domain, especially for taskoriented discourses (Grosz, 1978); and from speaker intentions (Carberry, 1990; Litman and Hirschberg, 1990; Lochbaum, 1994). Recent methods for automatic recognition of discourse structure from text have incorporated thesaurus-based and other information retrieval techniques to identify changes in topic (Morris and Hirst, 1991; Yarowsky, 1991; Iwanska et al., 1991; Hearst, 1994; Reynar, 1994). Parallel investigations on prosodic/acoustic cues to discourse structure have investigated the contributions of features such as pitch range, pausal duration, amplitude, speaking rate, and intonational contour to signaling topic change. Variation in pitch range has often been seen as conveying 'topic structure' in discourse. Brown et al. (1980) found that subjects typically started new topics relatively high in their pitch range and finished topics by compressing their range. Silverman (1987) found that manipulation of pitch range alone, or in conjunction with pausal duration between utterances, facilitated the disambiguation of ambiguous topic structures. Avesani and Vayra (1988) also found variation in pitch range in professional recordings which appeared to correlate with topic structure, and Ayers (1992) found that pitch range correlates with hierarchical topic structure more closely in read than spontaneous conversational speech. Duration of pause between utterances or phrases has also been identi'Also called DISCOURSE MARKERS or DISCOURSE PARTICLES, these are items such as now, first, and by the way, which explicitly mark discourse structure. fled as an indicator of topic structure, with longer pauses marking major topic shifts (Lehiste, 1979; Brown, Currie, and Kenworthy, 1980; Avesani and Vayra, 1988; Passonneau and Litman, 1993); Woodbury (1987), however, found no such correlation in his data. Amplitude was also found to increase at the start of a new topic and decrease at the end (Brown, Currie, and Kenworthy, 1980). Swerts and colleagues (1992) found that melody and duration can pre-signal the end of a discourse unit, in addition to marking the discourse-unit-final utterance itself. And speaking rate has been found to correlate with structural variation; in several studies (Lehiste, 1980; Brubaker, 1972; Butterworth, 1975) segment-initial utterances exhibited slower rates, and segment-final, faster rates. Swerts and Ostendorf (1995), however, report negative rate results. In general, these studies have lacked an independently-motivated notion of discourse structure. With few exceptions, they rely on intuitive analyses of topic structure; operational definitions of discourse-level properties (e.g., interpreting paragraph breaks as discourse segment boundaries); or 'theory-neutral' discourse segmentations, where subjects are given instructions to simply mark changes in topic. Recent studies have focused on the question of whether discourse structure itself can be empirically determined in a reliable manner, a prerequisite to investigating linguistic cues to its existence. An intention-based theory of discourse was used in (Hirschberg and Grosz, 1992; Grosz and Hirschberg, 1992) to identify intonational correlates of discourse structure in news stories read by a professional speaker. Discourse structural elements were determined by experts in the Grosz and Sidner (1986) theory of discourse structure, based on either text alone or text and speech. This study revealed strong correlations of aspects of pitch range, amplitude, and timing with features of global and local structure for both segmentation methods. Passonneau and Litman (to appear) analyzed correlations of pause, as well as cue phrases and referential relations, with discourse structure; their segmenters were asked to identify speakers' communicative &quot;actions&quot;. The present study addresses issues of speaking style and segmentation method while exploring in more detail than previous studies the prosodic parameters that characterize initial, medial, and final utterances in a discourse segment. The current investigation of discourse and intonation is based on analysis of a corpus of spontaneous and read speech, the Boston Directions Corpus.' This 'The Boston Directions Corpus was designed and collected in collaboration with Barbara Grosz. corpus comprises elicited monologues produced by multiple non-professional speakers, who were given written instructions to perform a series of nine increasingly complex direction-giving tasks. Speakers first explained simple routes such as getting from one station to another on the subway, and progressed gradually to the most complex task of planning a round-trip journey from Harvard Square to several Boston tourist sights. Thus, the tasks were designed to require increasing levels of planning complexity. The speakers were provided with various maps, and could write notes to themselves as well as trace routes on the maps. For the duration of the experiment, the speakers were in face-to-face contact with a silent partner (a confederate) who traced on her map the routes described by the speakers. The speech was subsequently orthographically transcribed, with false starts and other speech errors repaired or omitted; subjects returned several weeks after their first recording to read aloud from transcriptions of their own directions. For this paper, the spontaneous and read recordings for one male speaker were acoustically analyzed; fundamental frequency and energy were calculated using Entropic speech analysis software. The prosodic transcription, a more abstract representation of the intonational prominences, phrasing, and melodic contours, was obtained by hand-labeling. We employed the ToBI standard for prosodic transcription (Pitrelli, Beckman, and Hirschberg, 1994), which is based upon Pierrehumbert's theory of American English intonation (Pierrehumbert, 1980). The ToBI transcription provided us with a breakdown of the speech sample into minor or INTERMEDIATE PHRASES (Pierrehumbert, 1980; Beckman and Pierrehumbert, 1986). This level of prosodic phrase served as our primary unit of analysis for measuring both speech and discourse properties. The portion of the corpus we report on consists of 494 and 552 intermediate phrases for read and spontaneous speech, respectively. In our research, the Grosz and Sidner (1986) theory of discourse structure, hereafter G&S, provides a foundation for segmenting discourses into constituent parts. According to this model, at least three components of discourse structure must be distinguished. The utterances composing the discourse divide into segments that may be embedded relative to one another. These segments and the embedding relationships between them form the LINGUISTIC STRUCTURE. The embedding relationships reflect changes in the ATTENTIONAL STATE, the dynamic record of the entities and attributes that are salient during a particular part of the discourse. Changes in linguistic structure, and hence attentional state, depend on the discourse's INTENTIONAL STRUCTURE; this structure comprises the intentions or DISCOURSE SEGMENT PURPOSES (DSPs) underlying the discourse and relations between DSPs. Two methods of discourse segmentation were employed by subjects who had expertise in the G&S theory. Following Hirschberg and Grosz (1992), three subjects labeled from text alone (group T) and three labeled from text and speech (group S). Other than this difference in input modality, all subjects received identical written instructions. The text for each task was presented with line breaks corresponding to intermediate phrase boundaries (i.e., ToBI BREAK INDICES of level 3 or higher (Pitrelli, Beckman, and Hirschberg, 1994)). In the instructions, subjects were essentially asked to analyze the linguistic and intentional structures by segmenting the discourse, identifying the DSPs, and specifying the hierarchical relationships among segments. Labels on which all labelers in the group agreed are termed the CONSENSUS LABELS.3 The consensus labels for segment-initial (SBEG), segment-final (SF), and segment-medial (SCONT, defined as neither SBEG nor SF) phrase labels are given in Table 1.4 Note that group T and group S segmentations differ significantly, in contrast to earlier findings of Hirschberg and Grosz (1992) on a corpus of readaloud news stories and in support of informal findings of Swerts (1995). Table 1 shows that group S produced significantly more consensus boundaries for both read (p<.001, x=58.8, df=1) and spontaneous (p<.001, x=55.4, df=1) speech than did 3Use of consensus labels is a conservative measure of labeler agreement. Results in (Passonneau and Litman, 1993) and (Swerts, 1995) show that with a larger number of labelers, notions of BOUNDARY STRENGTH can be employed. 4Consensus percentages for the three types in Table 1 do not necessarily sum to the total consensus agreement percentage, since a phrase is both segment-initial and segment-final when it makes up a segment by itself. group T. When the read and spontaneous data are pooled, group S agreed upon significantly more SBEG boundaries (p<.05, x=4.7, df=1) as well as SF boundaries (p<.05, x=4.4, df=1) than did group T. Further, it is not the case that text-alone segmenters simply chose to place fewer boundaries in the discourse; if this were so, then we would expect a high percentage of SCONT consensus labels where no SBEGs or SFs were identified. Instead, we find that the number of consensus SCONTs was significantly higher for text-and-speech labelings than for text-alone (p<.001, x=49.1, df=1). It appears that the speech signal can help disambiguate among alternate segmentations of the same text. Finally, the data in Table 1 show that spontaneous speech can be segmented as reliably as its read counterpart, contrary to Ayer's results (1992). Comparisons of inter-labeler reliability, that is, the reproducibility of a coding scheme given multiple labelers, provide another perspective on the segmentation data. How best to measure inter-labeler reliability for discourse segmentation tasks, especially for hierarchical segmentation, is an open research question (Passonneau and Litman, to appear; Carletta, 1995; Flammia and Zue, 1995; Rotondo, 1984; Swerts, 1995). For comparative purposes, we explored several measures proposed in the literature, namely, COCHRAN'S Q and the KAPPA (K) COEFFICIENT (Siegel and Castellan, 1988). Cochran's Q, originally proposed in (Hirschberg and Grosz, 1992) to measure the likelihood that similarity among labelings was due to chance, was not useful in the current study; all tests of similarity using this metric (pairwise, or comparing all labelers) gave probability near zero. We concluded that this statistic did not serve, for example, to capture the differences observed between labelings from text alone versus labelings from text and speech. Recent discourse annotation studies (Isard and Carletta, 1995; Flammia and Zue, 1995) have measured reliability using the K coefficient, which factors out chance agreement taking the expected distribution of categories into account. This coefficient is defined as where Po represents the observed agreement and PE represents the expected agreement. Typically, values of .7 or higher for this measure provide evidence of good reliability, while values of .8 or greater indicate high reliability. Isard and Carletta (1995) report pairwise K scores ranging from .43 to .68 in a study of naive and expert classifications of types of 'moves' in the Map Task dialogues. For theory-neutral discourse segmentations of information-seeking dialogues, Flammia (Flammia and Zue, 1995) reports an average pairwise K of .45 for five labelers and of .68 for the three most similar labelers. An important issue in applying the 1£ coefficient is how one calculates the expected agreement using prior distributions of categories. We first calculated the prior probabilities for our data based simply on the distribution of SBEG versus non-SBEG labels for all labelers on one of the nine directiongiving tasks in this study, with separate calculations for the read and spontaneous versions. This task, which represented about 8% of the data for both speaking styles, was chosen because it was midway in planning complexity and in length among all the tasks. Using these distributions, we calculated tz coefficients for each pair of labelers in each condition for the remaining eight tasks in our corpus. The observed percentage of SBEG labels, prior distribution for SBEG, average of the pairwise n scores, and standard deviations for those scores are presented in The average K scores for group T segmenters indicate weak inter-labeler reliability. In contrast, average i scores for group S are .8 or better, indicating a high degree of inter-labeler reliability. Thus, application of this somewhat stricter reliability metric confirms that the availability of speech critically influences how listeners perceive discourse structure. The calculation of reliability for SBEG versus nonSBEG labeling in effect tests the similarity of linearized segmentations and does not speak to the issue of how similar the labelings are in terms of hierarchical structure. Flammia has proposed a method for generalizing the use of the K coefficient for hierarchical segmentation that gives an upper-bound estimate on inter-labeler agreement.5 We applied this metric to our segmentation data, calculating weighted averages for pairwise n scores averaged for each task. Results for each condition, together with the lowest and highest average n scores over the tasks, are presented in Table 3. Once again, averaged scores of .7 or better for text-and-speech labelings, for both speaking styles, indicate markedly higher inter-labeler reliability than do scores for text-alone labelings, which averaged .51 and .53. For purposes of intonational analysis, we take advantage of the high degree of agreement among our discourse labelers and include in each segment boundary class (SBEG, SF, and SCONT) only the phrases whose classification all subjects agreed upon. We term these the CONSENSUS-LABELED PHRASES, and compare their features to those of all phrases not in the relevant class (i.e., non-consensuslabeled phrases and consensus-labeled phrases of the other types). Note that there were one-third fewer consensus-labeled phrases for text-alone labelings than for text-and-speech (see Table 1). We examined the following acoustic and prosodic features of SBEG, SCONT, and SF consensus-labeled phrases: f0 maximum and f0 average;6 rms (energy) maximum and rms average; speaking rate (measured in syllables per second); and duration of preceding and subsequent silent pauses. As for the segmentation analyses, we compared intonational correlates of segment boundary types not only for group S versus group T, but also for spontaneous versus read speech. While correlates have been identified in read speech, they have been observed in spontaneous speech only rarely and descriptively. 'We calculated 10 maximum in two ways: as simple f0 peak within the intermediate phrase and also as f0 maximum measured at the rms maximum of the sonorant portion of the nuclear-accented syllable in the intermediate phrase (HIGH FO in the ToBI framework (Pitre Beckman, and Hirschberg, 1994)). The latter measure proved more robust, so we report results based on this metric. The same applies to measurement of rms maximum. Average f0 and rms were calculated over the entire intermediate phrase. We found strong correlations for consensus SBEG, SCONT, and SF phrases for all conditions. Results for group T are given in Table 4, and for group S, in Table 5.7 Consensus SBEG phrases in all conditions possess significantly higher maximum and average fO, higher maximum and average rms, shorter subsequent pause, and longer preceding pause. For consensus SCONT phrases, we found some differences between read and spontaneous speech for both labeling methods. Features for group T included significantly lower f0 maximum and average and lower rms maximum and average for read speech, but only lower f0 maximum for the spontaneous condition. Group S features for SCONT were identical to group T except for the absence of a correlation for maximum rms. While SCONT phrases for both speaking styles exhibited significantly shorter preceding and subsequent pauses than other phrases, only the spontaneous condition showed a significantly slower rate. For consensus SF phrases, we again found similar patterns for both speaking styles and both label7T-tests were used to test for statistical significance of difference in the means of two classes of phrases. Results reported are significant at the .005 level or better, except where '*' indicates significance at the .03 level or better. Results were calculated using one-tailed t-tests, except where 'f' indicates a two-tailed test. ing methods, namely lower f0 maximum and average, lower rms maximum and average, faster speaking rate, shorter preceding pauses, and longer subsequent pauses. While it may appear somewhat surprising that results for both labeling methods match so closely, in fact, correlations for text-and-speech labels presented in Table 5 were almost invariably statistically stronger than those for text-alone labels presented in Table 4. These more robust results for text-andspeech labelings occur even though the data set of consensus labels is considerably larger than the data set of consensus text-alone labelings. With a view toward automatically segmenting a spoken discourse, we would like to directly classify phrases of all three discourse categories. But SCONT and SF phrases exhibit similar prominence features and appear distinct from each other only in terms of timing differences. A second issue is whether such classification can be done con-line.' To address both of these issues, we made pairwise comparisons of consensus-labeled phrase groups using measures of relative change in acoustic-prosodic parameters over a local window of two consecutive phrases. Table 6 presents significant findings on relative changes in fO, loudness (measured in decibels), and speaking rate, from prior to current intermediate phrase.8 First, note that SBEG is distinguished from both SCONT and SF in terms of f0 change and db change from prior phrase; that is, while SBEG phrases are distinguished on a variety of measures from all other phrases (including non-consensus-labeled phrases) in Table 5, this table shows that SBEGs are also distinguishable directly from each of the other consensuslabeled categories. Second, while SCONT and SF appear to share prominence features in Table 5, Table 6 reveals differences between SCONT and SF in amount of f0 and db change. Thus, in addition to lending themselves to on-line processing, local measures may also capture valuable prominence cues to distinguish between segment-medial and segmentfinal phrases. Although this paper reports results from only a single speaker, the findings are promising. We have demonstrated that a theory-based method for discourse analysis can provide reliable segmentations of spontaneous as well as read speech. In addition, the availability of speech in the text-and-speech labeling method led to significantly higher reliability scores. The stronger correlations found for intonational features of the text-and-speech labelings suggest not only that discourse labelers make use of prosody in their analyses, but also that obtaining such data can lead to more robust modeling of the relationship between intonation and discourse structure. The following preliminary results can be considered for incorporation in such a model. First, segment-initial utterances differ from medial and final utterances in both prominence and rhythmic properties. Segment-medial and segment-final utterances are distinguished more clearly by rhythmic features, primarily pause. Finally, all correlations found for global parameters can also be computed based on relative change in acoustic-prosodic parameters in a window of two phrases. Ongoing research is addressing the development of automatic classification algorithms for discourse boundary type; the role of prosody in conveying hierarchical relationships among discourse segments; individual speaker differences; and discourse segmentation methods that can be used by naive subjects.\"\n",
              " ...\n",
              " \"Word Sense Disambiguation Improves Statistical Machine Translation Recent research presents conflicting evidence on whether word sense disambiguation (WSD) systems can help to improve the performance of statistical machine translation (MT) systems. In this paper, we successfully integrate a state-of-the-art WSD system into a state-of-the-art hierarchical phrase-based MT system, Hiero. We show for the first time that integrating a WSD system improves the performance of a state-ofthe-art statistical MT system on an actual translation task. Furthermore, the improvement is statistically significant. Many words have multiple meanings, depending on the context in which they are used. Word sense disambiguation (WSD) is the task of determining the correct meaning or sense of a word in context. WSD is regarded as an important research problem and is assumed to be helpful for applications such as machine translation (MT) and information retrieval. In translation, different senses of a word w in a source language may have different translations in a target language, depending on the particular meaning of w in context. Hence, the assumption is that in resolving sense ambiguity, a WSD system will be able to help an MT system to determine the correct translation for an ambiguous word. To determine the correct sense of a word, WSD systems typically use a wide array of features that are not limited to the local context of w, and some of these features may not be used by state-of-the-art statistical MT systems. 33 To perform translation, state-of-the-art MT systems use a statistical phrase-based approach (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by treating phrases as the basic units of translation. In this approach, a phrase can be any sequence of consecutive words and is not necessarily linguistically meaningful. Capitalizing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system. Recently, some researchers investigated whether performing WSD will help to improve the performance of an MT system. Carpuat and Wu (2005) integrated the translation predictions from a Chinese WSD system (Carpuat et al., 2004) into a ChineseEnglish word-based statistical MT system using the ISI ReWrite decoder (Germann, 2003). Though they acknowledged that directly using English translations as word senses would be ideal, they instead predicted the HowNet sense of a word and then used the English gloss of the HowNet sense as the WSD model’s predicted translation. They did not incorporate their WSD model or its predictions into their translation model; rather, they used the WSD predictions either to constrain the options available to their decoder, or to postedit the output of their decoder. They reported the negative result that WSD decreased the performance of MT based on their experiments. In another work (Vickrey et al., 2005), the WSD problem was recast as a word translation task. The translation choices for a word w were defined as the Then, in Section 3, we describe the Hiero MT sysset of words or phrases aligned to w, as gathered tem and introduce the two new features used to intefrom a word-aligned parallel corpus. The authors grate the WSD system into Hiero. In Section 4, we showed that they were able to improve their model’s describe the training data used by the WSD system. accuracy on two simplified translation tasks: word In Section 5, we describe how the WSD translations translation and blank-filling. provided are used by the decoder of the MT system. Recently, Cabezas and Resnik (2005) experi- In Section 6 and 7, we present and analyze our exmented with incorporating WSD translations into perimental results, before concluding in Section 8. Pharaoh, a state-of-the-art phrase-based MT sys- 2 Word Sense Disambiguation tem (Koehn et al., 2003). Their WSD system pro- Prior research has shown that using Support Vector vided additional translations to the phrase table of Machines (SVM) as the learning algorithm for WSD Pharaoh, which fired a new model feature, so that achieves good results (Lee and Ng, 2002). For our the decoder could weigh the additional alternative experiments, we use the SVM implementation of translations against its own. However, they could (Chang and Lin, 2001) as it is able to work on multinot automatically tune the weight of this feature in class problems to output the classification probabilthe same way as the others. They obtained a rela- ity for each class. tively small improvement, and no statistical signifi- Our implemented WSD classifier uses the knowlcance test was reported to determine if the improve- edge sources of local collocations, parts-of-speech ment was statistically significant. (POS), and surrounding words, following the sucNote that the experiments in (Carpuat and Wu, cessful approach of (Lee and Ng, 2002). For local 2005) did not use a state-of-the-art MT system, collocations, we use 3 features, w_1w+1, w_1, and while the experiments in (Vickrey et al., 2005) were w+1, where w_1 (w+1) is the token immediately to not done using a full-fledged MT system and the the left (right) of the current ambiguous word ocevaluation was not on how well each source sentence currence w. For parts-of-speech, we use 3 features, was translated as a whole. The relatively small im- P_1, P0, and P+1, where P0 is the POS of w, and provement reported by Cabezas and Resnik (2005) P_1 (P+1) is the POS of w_1 (w+1). For surroundwithout a statistical significance test appears to be ing words, we consider all unigrams (single words) inconclusive. Considering the conflicting results re- in the surrounding context of w. These unigrams can ported by prior work, it is not clear whether a WSD be in a different sentence from w. We perform feasystem can help to improve the performance of a ture selection on surrounding words by including a state-of-the-art statistical MT system. unigram only if it occurs 3 or more times in some In this paper, we successfully integrate a state- sense of w in the training data. of-the-art WSD system into the state-of-the-art hi- To measure the accuracy of our WSD classifier, erarchical phrase-based MT system, Hiero (Chiang, we evaluate it on the test data of SENSEVAL-3 Chi2005). The integration is accomplished by introduc- nese lexical-sample task. We obtain accuracy that ing two additional features into the MT model which compares favorably to the best participating system operate on the existing rules of the grammar, with- in the task (Carpuat et al., 2004). out introducing competing rules. These features are 3 Hiero treated, both in feature-weight tuning and in decod- Hiero (Chiang, 2005) is a hierarchical phrase-based ing, on the same footing as the rest of the model, model for statistical machine translation, based on allowing it to weigh the WSD model predictions weighted synchronous context-free grammar (CFG) against other pieces of evidence so as to optimize (Lewis and Stearns, 1968). A synchronous CFG translation accuracy (as measured by BLEU). The consists of rewrite rules such as the following: contribution of our work lies in showing for the first X —* ('y, α) (1) time that integrating a WSD system significantly improves the performance of a state-of-the-art statistical MT system on an actual translation task. In the next section, we describe our WSD system. 34 where X is a non-terminal symbol, -y (α) is a string of terminal and non-terminal symbols in the source (target) language, and there is a one-to-one correspondence between the non-terminals in -y and α indicated by co-indexation. Hence, -y and α always have the same number of non-terminal symbols. For instance, we could have the following grammar rule: where boxed indices represent the correspondences between non-terminal symbols. Hiero extracts the synchronous CFG rules automatically from a word-aligned parallel corpus. To translate a source sentence, the goal is to find its most probable derivation using the extracted grammar rules. Hiero uses a general log-linear model (Och and Ney, 2002) where the weight of a derivation D for a particular source sentence and its translation is where Oi is a feature function and Ai is the weight for feature Oi. To ensure efficient decoding, the Oi are subject to certain locality restrictions. Essentially, they should be defined as products of functions defined on isolated synchronous CGF rules; however, it is possible to extend the domain of locality of the features somewhat. A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult but still polynomial; in this paper, we add features that depend on the neighboring source-side words, which does not affect decoding complexity at all because the source string is fixed. In principle we could add features that depend on arbitrary source-side context. To incorporate WSD into Hiero, we use the translations proposed by the WSD system to help Hiero obtain a better or more probable derivation during the translation of each source sentence. To achieve this, when a grammar rule R is considered during decoding, and we recognize that some of the terminal symbols (words) in α are also chosen by the WSD system as translations for some terminal symbols (words) in -y, we compute the following features: a negative weight, rewards rules that use translations suggested by the WSD module. Note that we can take the negative logarithm of the rule/derivation weights and think of them as costs rather than probabilities. Our experiments were for Chinese to English translation. Hence, in the context of our work, a synchronous CFG grammar rule X —* (-y, α) gathered by Hiero consists of a Chinese portion -y and a corresponding English portion α, where each portion is a sequence of words and non-terminal symbols. Our WSD classifier suggests a list of English phrases (where each phrase consists of one or more English words) with associated contextual probabilities as possible translations for each particular Chinese phrase. In general, the Chinese phrase may consist of k Chinese words, where k = 1, 2, 3, .... However, we limit k to 1 or 2 for experiments reported in this paper. Future work can explore enlarging k. Whenever Hiero is about to extract a grammar rule where its Chinese portion is a phrase of one or two Chinese words with no non-terminal symbols, we note the location (sentence and token offset) in the Chinese half of the parallel corpus from which the Chinese portion of the rule is extracted. The actual sentence in the corpus containing the Chinese phrase, and the one sentence before and the one sentence after that actual sentence, will serve as the context for one training example for the Chinese phrase, with the corresponding English phrase of the grammar rule as its translation. Hence, unlike traditional WSD where the sense classes are tied to a specific sense inventory, our “senses” here consist of the English phrases extracted as translations for each Chinese phrase. Since the extracted training data may be noisy, for each Chinese phrase, we remove English translations that occur only once. Furthermore, we only attempt WSD classification for those Chinese phrases with at least 10 training examples. Using the WSD classifier described in Section 2, we classified the words in each Chinese source sentence to be translated. We first performed WSD on all single Chinese words which are either noun, verb, or adjective. Next, we classified the Chinese phrases consisting of 2 consecutive Chinese words by simply treating the phrase as a single unit. When performing classification, we give as output the set of English translations with associated context-dependent probabilities, which are the probabilities of a Chinese word (phrase) translating into each English phrase, depending on the context of the Chinese word (phrase). After WSD, the ith word ci in every Chinese sentence may have up to 3 sets of associated translations provided by the WSD system: a set of translations for ci as a single word, a second set of translations for ci_1ci considered as a single unit, and a third set of translations for cici+1 considered as a single unit. The following tasks are done for each rule that is considered during decoding: The WSD system is able to predict translations only for a subset of Chinese words or phrases. Hence, we must first identify which parts of the Chinese side of the rule have suggested translations available. Here, we consider substrings of length up to two, and we give priority to longer substrings. Next, we want to know, for each Chinese substring considered, whether the WSD system supports the Chinese-English translation represented by the rule. If the rule is finally chosen as part of the best derivation for translating the Chinese sentence, then all the words in the English side of the rule will appear in the translated English sentence. Hence, we need to match the translations suggested by the WSD system against the English side of the rule. It is for these matching rules that the WSD features will apply. The translations proposed by the WSD system may be more than one word long. In order for a proposed translation to match the rule, we require two conditions. First, the proposed translation must be a substring of the English side of the rule. For example, the proposed translation “every to” would not match the chunk “every month to”. Second, the match must contain at least one aligned ChineseEnglish word pair, but we do not make any other requirements about the alignment of the other Chinese or English words.1 If there are multiple possible matches, we choose the longest proposed translation; in the case of a tie, we choose the proposed translation with the highest score according to the WSD model. Define a chunk of a rule to be a maximal substring of terminal symbols on the English side of the rule. For example, in Rule (2), the chunks would be “go to” and “every month to”. Whenever we find a matching WSD translation, we mark the whole chunk on the English side as consumed. Finally, we compute the feature values for the rule. The feature P,sd(t  |s) is the sum of the costs (according to the WSD model) of all the matched translations, and the feature Pty,sd is the sum of the lengths of all the matched translations. Figure 1 shows the pseudocode for the rule scoring algorithm in more detail, particularly with regards to resolving conflicts between overlapping matches. To illustrate the algorithm given in Figure 1, consider Rule (2). Hereafter, we will use symbols to represent the Chinese and English words in the rule: c1, c2, and c3 will represent the Chinese words “4”, “�”, and “YI” respectively. Similarly, e1, e2, e3, e4, and e5 will represent the English words go, to, every, month, and to respectively. Hence, Rule (2) has two chunks: e1e2 and e3e4e5. When the rule is extracted from the parallel corpus, it has these alignments between the words of its Chinese and English portion: {c1–e3,c2–e4,c3–e1,c3–e2,c3–e5}, which means that c1 is aligned to e3, c2 is aligned to Input: rule R considered during decoding with its own associated cOstR e4, and c3 is aligned to e1, e2, and e5. Although all words are aligned here, in general for a rule, some of its Chinese or English words may not be associated with any alignments. In our experiment, c1c2 as a phrase has a list of translations proposed by the WSD system, including the English phrase “every month”. matchWSD will first be invoked for c1, which is aligned to only one chunk e3e4e5 via its alignment with e3. Since “every month” is a sub-sequence of the chunk and also contains the word e3 (“every”), it is noted as a candidate translation. Later, it is determined that the most number of words any candidate translation has is two words. Since among all the 2-word candidate translations, the translation “every month” has the highest translation probability as assigned by the WSD classifier, it is chosen as the best matching translation for the chunk. matchWSD is then invoked for c2, which is aligned to only one chunk e3e4e5. However, since this chunk has already been examined by c1 with which it is considered as a phrase, no further matching is done for c2. Next, matchWSD is invoked for c3, which is aligned to both chunks of R. The English phrases “go to” and “to” are among the list of translations proposed by the WSD system for c3, and they are eventually chosen as the best matching translations for the chunks e1e2 and e3e4e5, respectively. As mentioned, our experiments were on Chinese to English translation. Similar to (Chiang, 2005), we trained the Hiero system on the FBIS corpus, used the NIST MT 2002 evaluation test set as our development set to tune the feature weights, and the NIST MT 2003 evaluation test set as our test data. Using the English portion of the FBIS corpus and the Xinhua portion of the Gigaword corpus, we trained a trigram language model using the SRI Language Modelling Toolkit (Stolcke, 2002). Following (Chiang, 2005), we used the version 11a NIST BLEU script with its default settings to calculate the BLEU scores (Papineni et al., 2002) based on case-insensitive ngram matching, where n is up to 4. First, we performed word alignment on the FBIS parallel corpus using GIZA++ (Och and Ney, 2000) in both directions. The word alignments of both directions are then combined into a single set of alignments using the “diag-and” method of Koehn et al. (2003). Based on these alignments, synchronous CFG rules are then extracted from the corpus. While Hiero is extracting grammar rules, we gathered WSD training data by following the procedure described in section 4. Using the MT 2002 test set, we ran the minimumerror rate training (MERT) (Och, 2003) with the decoder to tune the weights for each feature. The weights obtained are shown in the row Hiero of Table 2. Using these weights, we run Hiero’s decoder to perform the actual translation of the MT 2003 test sentences and obtained a BLEU score of 29.73, as shown in the row Hiero of Table 1. This is higher than the score of 28.77 reported in (Chiang, 2005), perhaps due to differences in word segmentation, etc. Note that comparing with the MT systems used in (Carpuat and Wu, 2005) and (Cabezas and Resnik, 2005), the Hiero system we are using represents a much stronger baseline MT system upon which the WSD system must improve. We then added the WSD features of Section 3.1 into Hiero and reran the experiment. The weights obtained by MERT are shown in the row Hiero+WSD of Table 2. We note that a negative weight is learnt for Ptywsd. This means that in general, the model prefers grammar rules having chunks that matches WSD translations. This matches our intuition. Using the weights obtained, we translated the test sentences and obtained a BLEU score of 30.30, as shown in the row Hiero+WSD of Table 1. The improvement of 0.57 is statistically significant at p < 0.05 using the sign-test as described by Collins et al. (2005), with 374 (+1), 318 (−1) and 227 (0). Using the bootstrap-sampling test described in (Koehn, 2004b), the improvement is statistically significant atp < 0.05. Though the improvement is modest, it is statistically significant and this positive result is important in view of the negative findings in (Carpuat and Wu, 2005) that WSD does not help MT. Furthermore, note that Hiero+WSD has higher n-gram precisions than Hiero. Ideally, the WSD system should be suggesting highquality translations which are frequently part of the reference sentences. To determine this, we note the set of grammar rules used in the best derivation for translating each test sentence. From the rules of each test sentence, we tabulated the set of translations proposed by the WSD system and check whether they are found in the associated reference sentences. On the entire set of NIST MT 2003 evaluation test sentences, an average of 10.36 translations proposed by the WSD system were used for each sentence. When limited to the set of 374 sentences which were judged by the Collins sign-test to have better translations from Hiero+WSD than from Hiero, a higher number (11.14) of proposed translations were used on average. Further, for the entire set of test sentences, 73.01% of the proposed translations are found in the reference sentences. This increased to a proportion of 73.22% when limited to the set of 374 sentences. These figures show that having more, and higher-quality proposed translations contributed to the set of 374 sentences being better translations than their respective original translations from Hiero. Table 3 gives a detailed breakdown of these figures according to the number of words in each proposed translation. For instance, over all the test sentences, the WSD module gave 7087 translations of single-word length, and 77.31% of these translations match their respective reference sentences. We note that although the proportion of matching 2word translations is slightly lower for the set of 374 sentences, the proportion increases for translations having more words. After the experiments in Section 6 were completed, we visually inspected the translation output of Hiero and Hiero+WSD to categorize the ways in which integrating WSD contributes to better translations. The first way in which WSD helps is when it enables the integrated Hiero+WSD system to output extra appropriate English words. For example, the translations for the Chinese sentence “...� 41t r -�.�3 li• J , 4 Ãm Rz Í õ WtPI be unable to obtain more aid and other concessions. Here, the Chinese words “Ã� Rz” are not translated by Hiero at all. By providing the correct translation of “unable to obtain” for “ÃM Rz”, the translation output of Hiero+WSD is more complete. A second way in which WSD helps is by correcting a previously incorrect translation. For example, for the Chinese sentence “.. . , ;i!�E -i-t- Q È & A F� ,... ”, the WSD system helps to correct Hiero’s original translation by providing the correct translation of “all ethnic groups” for the Chinese phrase “È &”: We also looked at the set of 318 sentences that were judged by the Collins sign-test to be worse translations. We found that in some situations, Hiero+WSD has provided extra appropriate English words, but those particular words are not used in the reference sentences. An interesting example is the translation of the Chinese sentence “A� i� IRR �� �� Ã~ Rz Í õ �� ”. This is similar to the example mentioned earlier. In this case however, those extra English words provided by Hiero+WSD, though appropriate, do not result in more n-gram matches as the reference sentences used phrases such as “will not gain”, “will not get”, etc. Since the BLEU metric is precision based, the longer sentence translation by Hiero+WSD gets a lower BLEU score instead. D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of ACL05, pages 263– 270. We have shown that WSD improves the translation performance of a state-of-the-art hierarchical phrase-based statistical MT system and this improvement is statistically significant. We have also demonstrated one way to integrate a WSD system into an MT system without introducing any rules that compete against existing rules, and where the feature-weight tuning and decoding place the WSD system on an equal footing with the other model components. For future work, an immediate step would be for the WSD classifier to provide translations for longer Chinese phrases. Also, different alternatives could be tried to match the translations provided by the WSD classifier against the chunks of rules. Finally, besides our proposed approach of integrating WSD into statistical MT via the introduction of two new features, we could explore other alternative ways of integration. Yee Seng Chan is supported by a Singapore Millennium Foundation Scholarship (ref no. SMF-20041076). David Chiang was partially supported under the GALE program of the Defense Advanced Research Projects Agency, contract HR0011-06-C0022.\"\n",
              " \"Exploiting Syntactic Structure for Language Modeling The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies. The model assigns probability to every joint sequence of words-binary-parse-structure with headword annotation and operates in a left-to-right manner — therefore usable for automatic speech recognition. The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved. The main goal of the present work is to develop a language model that uses syntactic structure to model long-distance dependencies. During the summer96 DoD Workshop a similar attempt was made by the dependency modeling group. The model we present is closely related to the one investigated in (Chelba et al., 1997), however different in a few important aspects: • our model operates in a left-to-right manner, allowing the decoding of word lattices, as opposed to the one referred to previously, where only whole sentences could be processed, thus reducing its applicability to n-best list re-scoring; the syntactic structure is developed as a model component; • our model is a factored version of the one in (Chelba et al., 1997), thus enabling the calculation of the joint probability of words and parse structure; this was not possible in the previous case due to the huge computational complexity of the model. Our model develops syntactic structure incrementally while traversing the sentence from left to right. This is the main difference between our approach and other approaches to statistical natural language parsing. Our parsing strategy is similar to the incremental syntax ones proposed relatively recently in the linguistic community (Philips, 1996). The probabilistic model, its parameterization and a few experiments that are meant to evaluate its potential for speech recognition are presented. Consider predicting the word after in the sentence: the contract ended with a loss of 7 cents after trading as low as 89 cents. A 3-gram approach would predict after from (7, cents) whereas it is intuitively clear that the strongest predictor would be ended which is outside the reach of even 7-grams. Our assumption is that what enables humans to make a good prediction of after is the syntactic structure in the past. The linguistically correct partial parse of the word history when predicting after is shown in Figure 1. The word ended is called the headword of the constituent (ended (with (...))) and ended is an exposed headword when predicting after — topmost headword in the largest constituent that contains it. The syntactic structure in the past filters out irrelevant words and points to the important ones, thus enabling the use of long distance information when predicting the next word. Our model will attempt to build the syntactic structure incrementally while traversing the sentence left-to-right. The model will assign a probability P(W,T) to every sentence W with every possible POStag assignment, binary branching parse, nonterminal label and headword annotation for every constituent of T. Let W be a sentence of length n words to which we have prepended <s> and appended </s> so that wo =<s> and w,,±1 =</s>. Let Wk be the word k-prefix wo wk of the sentence and Wk Tk the word-parse k-prefix. To stress this point, a word-parse k-prefix contains — for a given parse — only those binary subtrees whose span is completely included in the word k-prefix, excluding wo =<s>. Single words along with their POStag can be regarded as root-only trees. Figure 2 shows a word-parse k-prefix; h_O h_{-m} are the exposed heads, each head being a pair(headword, nonterminal label), or (word, POStag) in the case of a root-only tree. A complete parse — Figure 3 — is any binary parse of the (wi ti) • • • (Wn, to) (<Is> , SE) sequence with the restriction that (</s>, TOP') is the only allowed head. Note that ((wi, ti) . (w, t)) needn't be a constituent, but for the parses where it is, there is no restriction on which of its words is the headword or what is the non-terminal label that accompanies the headword. The model will operate by means of three modules: (unary, NTlabel), (adjoin-left, NTlabel) or (adj oin-right , NTlabel) until it passes control to the PREDICTOR by taking a null transition. NTlabel is the non-terminal label assigned to the newly built constituent and {left , right} specifies where the new headword is inherited from. The operations performed by the PARSER are illustrated in Figures 4-6 and they ensure that all possible binary branching parses with all possible headword and non-terminal label assignments for the w1 wk word sequence can be generated. The following algorithm formalizes the above description of the sequential generation of a sentence with a complete parse. The unary transition is allowed only when the most recent exposed head is a leaf of the tree — a regular word along with its POStag — hence it can be taken at most once at a given position in the input word string. The second subtree in Figure 2 provides an example of a unary transition followed by a null transition. It is easy to see that any given word sequence with a possible parse and headword annotation is generated by a unique sequence of model actions. This will prove very useful in initializing our model parameters from a treebank — see section 3.5. The probability P(W,T) of a word sequence W and a complete parse T can be broken into: where: As can be seen, (wk,tk,Wk--171-1, Pt • • • 14 1) is one of the Nk word-parse k-prefixes WkTk at position k in the sentence, i = I, Nk. To ensure a proper probabilistic model (1) we have to make sure that (2), (3) and (4) are well defined conditional probabilities and that the model halts with probability one. Consequently, certain PARSER and WORD-PREDICTOR probabilities must be given specific values: P((adjoin-right, TOP')/WkTk) = 1, if h_O = (</s>, TOP') and h_{-1}. word 0 <s> ensure that the parse generated by our model is consistent with the definition of a complete parse; The word-predictor model (2) predicts the next word based on the preceding 2 exposed heads, thus making the following equivalence classification: After experimenting with several equivalence classifications of the word-parse prefix for the tagger model, the conditioning part of model (3) was reduced to using the word to be tagged and the tags of the two most recent exposed heads: Model (4) assigns probability to different parses of the word k-prefix by chaining the elementary operations described above. The workings of the parser module are similar to those of Spatter (Jelinek et al., 1994). The equivalence classification of the WkTk word-parse we used for the parser model (4) was the same as the one used in (Collins, 1996): It is worth noting that if the binary branching structure developed by the parser were always rightbranching and we mapped the POStag and nonterminal label vocabularies to a single type then our model would be equivalent to a trigram language model. All model components — WORD-PREDICTOR, TAGGER, PARSER — are conditional probabilistic models of the type P(y/xi, x2, ,x,2) where y,x1,x2,...,xn, belong to a mixed bag of words, POStags, non-terminal labels and parser operations (y only). For simplicity, the modeling method we chose was deleted interpolation among relative frequency estimates of different orders f„(.) using a recursive mixing scheme: As can be seen, the context mixing scheme discards items in the context in right-to-left order. The A coefficients are tied based on the range of the count C(xi, , xn). The approach is a standard one which doesn't require an extensive description given the literature available on it (Jelinek and Mercer, 1980). Since the number of parses for a given word prefix Wk grows exponentially with k,I{Tk}1 0(2k), the state space of our model is huge even for relatively short sentences so we had to use a search strategy that prunes it. Our choice was a synchronous multistack search algorithm which is very similar to a beam search. Each stack contains hypotheses — partial parses — that have been constructed by the same number of predictor and the same number of parser operations. The hypotheses in each stack are ranked according to the ln(P(W, T)) score, highest on top. The width of the search is controlled by two parameters: above pruning strategy proved to be insufficient so we chose to also discard all hypotheses whose score is more than the log-probability threshold below the score of the topmost hypothesis. This additional pruning step is performed after all hypotheses in stage k' have been extended with the null parser transition and thus prepared for scanning a new word. The conditional perplexity calculated by assigning to a whole sentence the probability: where T* = argmaxTP(W,T), is not valid because it is not causal: when predicting wk+i we use T* which was determined by looking at the entire sentence. To be able to compare the perplexity of our model with that resulting from the standard trigram approach, we need to factor in the entropy of guessing the correct parse I': before predicting wk+i, based solely on the word prefix Wk. The probability assignment for the word at position k + 1 in the input sentence is made using: which ensures a proper probability over strings W*, where Sk is the set of all parses present in our stacks at the current stage k. Another possibility for evaluating the word level perplexity of our model is to approximate the probability of a whole sentence: where T(k) is one of the &quot;N-best&quot; — in the sense defined by our search — parses for W. This is a deficient probability assignment, however useful for justifying the model parameter re-estimation. The two estimates (8) and (10) are both consistent in the sense that if the sums are carried over all possible parses we get the correct value for the word level perplexity of our model. The major problem we face when trying to reestimate the model parameters is the huge state space of the model and the fact that dynamic programming techniques similar to those used in HMM parameter re-estimation cannot be used with our model. Our solution is inspired by an HMM re-estimation technique that works on pruned — N-best — trellises(Byrne et al., 1998). Let (W, T(k)), k = 1 . . . N be the set of hypotheses that survived our pruning strategy until the end of the parsing process for sentence W. Each of them was produced by a sequence of model actions, chained together as described in section 2; let us call the sequence of model actions that produced a given (W, T) the derivation(W,T). Let an elementary event in the derivation(W,T) = (word-to-tag, ho.tag , h_i.tag). The probability associated with each model action is determined as described in section 3.1, based on counts C(m)(y(m), x(m)), one set for each model component. Assuming that the deleted interpolation coefficients and the count ranges used for tying them stay fixed, these counts are the only parameters to be re-estimated in an eventual re-estimation procedure; indeed, once a set of counts C(m)(y(m), x(m)) is specified for a given model m, we can easily calculate: in ky /Xn ) for all context orders n = 0. . .maximum-order (mode/ (m)); This is all we need for calculating the probability of an elementary event and then the probability of an entire derivation. One training iteration of the re-estimation procedure we propose is described by the following algorithm: N-best parse development data; // counts.Ei // prepare counts.E(i+1) for each model component c{ gather_counts development model_c; } In the parsing stage we retain for each &quot;N-best&quot; hypothesis (W, T(k)), k = 1 . . . N, only the quantity 0(w77-0)) = p(w, T(0) / N-,N1 ' p k/w7 T(k)) and its derivation(W,T(k)). We then scan all the derivations in the &quot;development set&quot; and, for each occurrence of the elementary event (y(m) , x(m)) in derivation(W,T(k)) we accumulate the value 0(W, T(k)) in the C(m)(y(m), x(m)) counter to be used in the next iteration. The intuition behind this procedure is that 0(W, T(k)) is an approximation to the P(T(k)/W) probability which places all its mass on the parses that survived the parsing process; the above procedure simply accumulates the expected values of the counts Om) (y(m) , x(m)) under the 0(W, T(k)) conditional distribution. As explained previously, the Om) (y(m) , x(m)) counts are the parameters defining our model, making our procedure similar to a rigorous EM approach (Dempster et al., 1977). A particular — and very interesting — case is that of events which had count zero but get a non-zero count in the next iteration, caused by the &quot;N-best&quot; nature of the re-estimation process. Consider a given sentence in our &quot;development&quot; set. The &quot;N-best&quot; derivations for this sentence are trajectories through the state space of our model. They will change from one iteration to the other due to the smoothing involved in the probability estimation and the change of the parameters — event counts — defining our model, thus allowing new events to appear and discarding others through purging low probability events from the stacks. The higher the number of trajectories per sentence, the more dynamic this change is expected to be. The results we obtained are presented in the experiments section. All the perplexity evaluations were done using the left-to-right formula (8) (L2RPPL) for which the perplexity on the &quot;development set&quot; is not guaranteed to decrease from one iteration to another. However, we believe that our reestimation method should not increase the approximation to perplexity based on (10) (SUM-PPL) — again, on the &quot;development set&quot;; we rely on the consistency property outlined at the end of section 3.3 to correlate the desired decrease in L2R-PPL with that in SUM-PPL. No claim can be made about the change in either L2R-PPL or SUM-PPL on test data. Each model component — WORD-PREDICTOR, TAGGER, PARSER — is trained initially from a set of parsed sentences, after each parse tree (W, T) undergoes: These are the initial parameters used with the reestimation procedure described in the previous section. In order to get initial statistics for our model components we needed to binarize the UPenn Treebank (Marcus et al., 1995) parse trees and percolate headwords. The procedure we used was to first percolate headwords using a context-free (CF) rulebased approach and then binarize the parses by using a rule-based approach again. The headword of a phrase is the word that best represents the phrase, all the other words in the phrase being modifiers of the headword. Statistically speaking, we were satisfied with the output of an enhanced version of the procedure described in (Collins, 1996) — also known under the name &quot;Magerman Sz Black Headword Percolation Rules&quot;. Once the position of the headword within a constituent — equivalent with a CF production of the type Z -4 Y1... Y,2 , where Z, Yr, are nonterminal labels or POStags (only for Yi) — is identified to be k, we binarize the constituent as follows: depending on the Z identity, a fixed rule is used to decide which of the two binarization schemes in Figure 8 to apply. The intermediate nodes created by the above binarization schemes receive the nonterminal label Z'. Due to the low speed of the parser — 200 wds/min for stack depth 10 and log-probability threshold 6.91 nats (1/1000) — we could carry out the reestimation technique described in section 3.4 on only 1 Mwds of training data. For convenience we chose to work on the UPenn Treebank corpus. The vocabulary sizes were: The training data was split into &quot;development&quot; set — 929,564wds (sections 00-20) — and &quot;check set&quot; — 73,760wds (sections 21-22); the test set size was 82,430wds (sections 23-24). The &quot;check&quot; set has been used for estimating the interpolation weights and tuning the search parameters; the &quot;development&quot; set has been used for gathering/estimating counts; the test set has been used strictly for evaluating model performance. Table 1 shows the results of the re-estimation technique presented in section 3.4. We achieved a reduction in test-data perplexity bringing an improvement over a deleted interpolation trigram model whose perplexity was 167.14 on the same training-test data; the reduction is statistically significant according to a sign test. Simple linear interpolation between our model and the trigram model: yielded a further improvement in PPL, as shown in Table 2. The interpolation weight was estimated on check data to be A = 0.36. An overall relative reduction of 11% over the trigram model has been achieved. The large difference between the perplexity of our model calculated on the &quot;development&quot; set — used for model parameter estimation — and &quot;test&quot; set — unseen data — shows that the initial point we choose for the parameter values has already captured a lot of information from the training data. The same problem is encountered in standard n-gram language modeling; however, our approach has more flexibility in dealing with it due to the possibility of reestimating the model parameters. We believe that the above experiments show the potential of our approach for improved language models. Our future plans include: This research has been funded by the NSF IRI-19618874 grant (STIMULATE). The authors would like to thank Sanjeev Khudanpur for his insightful suggestions. Also to Harry Printz, Eric Ristad, Andreas Stolcke, Dekai Wu and all the other members of the dependency modeling group at the summer96 DoD Workshop for useful comments on the model, programming support and an extremely creative environment. Also thanks to Eric Brill, Sanjeev Khudanpur, David Yarowsky, Radu Florian, Lidia Mangu and Jun Wu for useful input during the meetings of the people working on our STIMULATE grant.\"\n",
              " 'SemEval-2007 Task-17: English Lexical Sample SRL and All Words This paper describes our experience in preparing the data and evaluating the results for three subtasks of SemEval-2007 Task-17 ? Lexical Sample, Semantic Role Labeling(SRL) and All-Words respectively. We tab ulate and analyze the results of participating systems. Correctly disambiguating words (WSD), and correctly identifying the semantic relationships be tween those words (SRL), is an important step forbuilding successful natural language processing applications, such as text summarization, question an swering, and machine translation. SemEval-2007Task-17 (English Lexical Sample, SRL and All Words) focuses on both of these challenges, WSD and SRL, using annotated English text taken from the Wall Street Journal and the Brown Corpus.It includes three subtasks: i) the traditional AllWords task comprising fine-grained word sense dis ambiguation using a 3,500 word section of the Wall Street Journal, annotated with WordNet 2.1 sense tags, ii) a Lexical Sample task for coarse-grainedword sense disambiguation on a selected set of lex emes, and iii) Semantic Role Labeling, using two different types of arguments, on the same subset of lexemes. 2.1 English fine-grained All-Words. In this task we measure the ability of systems to identify the correct fine-grained WordNet 2.1 wordsense for all the verbs and head words of their argu ments. 2.1.1 Data Preparation We began by selecting three articles wsj 0105.mrg (on homelessness), wsj 0186.mrg (about a book on corruption), and wsj 0239.mrg (about hot-air ballooning) from a section of the WSJ corpus that has been Treebanked and PropBanked. All instances of verbs were identified using theTreebank part-of-speech tags, and also the head words of their noun arguments (using the PropBank and standard headword rules). The locations of the sentences containing them as well as the locations of the verbs and the nouns within these sentences were recorded for subsequent sense-annotation. A total of 465 lemmas were selected from about 3500 words of text.We use a tool called STAMP written by Benjamin Snyder for sense-annotation of these instances. STAMP accepts a list of pointers to the in stances that need to be annotated. These pointers consist of the name of the file where the instance is located, the sentence number of the instance, and finally, the word number of the ambiguous word within that sentence. These pointers were obtained as described in the previous paragraph. STAMP also requires a sense inventory, which must be stored in XML format. This sense inventory was obtained by querying WordNet 2.1 and storing the output as a 87set of XML files (one for each word to be anno tated) prior to tagging. STAMP works by displayingto the user the sentence to be annotated with the tar get word highlighted along with the previous and the following sentences and the senses from the sense inventory. The user can select one of the senses and move on to the next instance. Two linguistics students annotated the words with WordNet 2.1 senses. Our annotators examined each instance upon which they disagreed and resolvedtheir disagreements. Finally, we converted the re sulting data to the Senseval format. For this dataset, we got an inter-annotator agreement (ITA) of 72% on verbs and 86% for nouns. 2.1.2 Results A total of 14 systems were evaluated on the All Words task. These results are shown in Table 1. We used the standard Senseval scorer ? scorer21 to score the systems. All the F-scores2 in this table as well as other tables in this paper are accompanied by a 95% confidence interval calculated using the bootstrap resampling procedure. 2.2 OntoNotes English Lexical Sample WSD. It is quite well accepted at this point that it is dif ficult to achieve high inter-annotator agreement onthe fine-grained WordNet style senses, and without a corpus with high annotator agreement, auto matic learning methods cannot perform at a levelthat would be acceptable for a downstream applica tion. OntoNotes (Hovy et al, 2006) is a project that has annotated several layers of semantic information ? including word senses, at a high inter-annotator agreement of over 90%. Therefore we decided to use this data for the lexical sample task. 2.2.1 Data All the data for this task comes from the 1M wordWSJ Treebank. For the convenience of the partici pants who wanted to use syntactic parse information as features using an off-the-shelf syntactic parser, we decided to compose the training data of Sections 02-21. For the test sets, we use data from Sections 1http://www.cse.unt.edu/?rada/senseval/senseval3/scoring/ 2scorer2 reports Precision and Recall scores for each system. For a sys tem that attempts all the words, both Precision and Recall are the same. Since a few systems had missing answers, they got different Precision and Recall scores. Therefore, for ranking purposes, we consolidated them into an F-score. Train Test Total Verb 8988 2292 11280 Noun 13293 2559 15852 Total 22281 4851 Table 2: The number of instances for Verbs andNouns in the Train and Test sets for the Lexical Sam ple WSD task. 01, 22, 23 and 24. Fortunately, the distribution ofwords was amenable to an acceptable number of in stances for each lemma in the test set. We selecteda total of 100 lemmas (65 verbs and 35 nouns) con sidering the degree of polysemy and total instances that were annotated. The average ITA for these is over 90%. The training and test set composition is described in Table 2. The distribution across all the verbs and nouns is displayed in Table 4 2.2.2 ResultsA total of 13 systems were evaluated on the Lexi cal Sample task. Table 3 shows the Precision/Recall for all these systems. The same scoring software was used to score this task as well. 2.2.3 Discussion For the all words task, the baseline performanceusing the most frequent WordNet sense for the lemmas is 51.4. The top-performing system was a supervised system that used a Maximum Entropy clas sifier, and got a Precision/Recall of 59.1% ? about 8 points higher than the baseline. Since the coarse and fine-grained disambiguation tasks have been part ofthe two previous Senseval competitions, and we happen to have access to that data, we can take this op portunity to look at the disambiguation performancetrend. Although different test sets were used for ev ery evaluation, we can get a rough indication of the trend. For the fine-grained All Words sense tagging task, which has always used WordNet, the systemperformance has ranged from our 59% to 65.2 (Sen seval3, (Decadt et al, 2004)) to 69% (Seneval2, (Chklovski and Mihalcea, 2002)). Because of time constraints on the data preparation, this year?s task has proportionally more verbs and fewer nouns thanprevious All-Words English tasks, which may ac count for the lower scores. As expected, the Lexical Sample task using coarse 88 Rank Participant System ID Classifier F 1 Stephen Tratz <stephen.tratz@pnl.gov> PNNL MaxEnt 59.1?4.5. 2 Hwee Tou Ng <nght@comp.nus.edu.sg> NUS-PT SVM 58.7?4.5. 3 Rada Mihalcea <rada@cs.unt.edu> UNT-Yahoo Memory-based 58.3?4.5. 4 Cai Junfu <caijunfu@gmail.com> NUS-ML naive Bayes 57.6?4.5. 5 Oier Lopez de Lacalle <jibloleo@si.ehu.es> UBC-ALM kNN 54.4?4.5. 6 David Martinez <davidm@csse.unimelb.edu.au> UBC-UMB-2 kNN 54.0?4.5. 7 Jonathan Chang <jcone@princeton.edu> PU-BCD Exponential Model 53.9?4.5. 8 Radu ION <radu@racai.ro> RACAI Unsupervised 52.7?4.5. 9 Most Frequent WordNet Sense Baseline N/A 51.4?4.5. 10 Davide Buscaldi <dbuscaldi@dsic.upv.es> UPV-WSD Unsupervised 46.9?4.5. 11 Sudip Kumar Naskar <sudip.naskar@gmail.com> JU-SKNSB Unsupervised 40.2?4.5. 12 David Martinez <davidm@csse.unimelb.edu.au> UBC-UMB-1 Unsupervised 39.9?4.5. 14 Rafael Berlanga <berlanga@uji.es> tkb-uo Unsupervised 32.5?4.5. 15 Jordan Boyd-Graber <jbg@princeton.edu> PUTOP Unsupervised 13.2?4.5. Table 1: System Performance for the All-Words task. Rank Participant System Classifier F 1 Cai Junfu <caijunfu@gmail.com> NUS-ML SVM 88.7?1.2. 2 Oier Lopez de Lacalle <jibloleo@si.ehu.es> UBC-ALM SVD+kNN 86.9?1.2. 5 Lucia Specia <lspecia@gmail.com> USP-IBM-1 ILP 85.1?1.2. 5 Deniz Yuret <dyuret@ku.edu.tr> KU Semi-supervised 85.1?1.2. 6 Saarikoski <harri.saarikoski@helsinki.fi> OE naive Bayes, SVM 83.8?1.2. 7 University of Technology Brno VUTBR naive Bayes 80.3?1.2. 8 Ana Zelaia <ana.zelaia@ehu.es> UBC-ZAS SVD+kNN 79.9?1.2. 9 Carlo Strapparava <strappa@itc.it> ITC-irst SVM 79.6?1.2. 10 Most frequent sense in training Baseline N/A 78.0?1.2. 11 Toby Hawker <toby@it.usyd.edu.au> USYD SVM 74.3?1.2. 12 Siddharth Patwardhan <sidd@cs.utah.edu> UMND1 Unsupervised 53.8?1.2. 13 Saif Mohammad <smm@cs.toronto.edu> Tor Unsupervised 52.1?1.2. - Toby Hawker <toby@it.usyd.edu.au> USYD? SVM 89.1?1.2 - Carlo Strapparava <strappa@itc.it> ITC? SVM 89.1?1.2 Table 3: System Performance for the OntoNotes Lexical Sample task. Systems marked with an * were post-competition bug-fix submissions.grained senses provides consistently higher per formance than previous more fine-grained LexicalSample Tasks. The high scores here were foreshad owed in an evaluation involving a subset of the data last summer (Chen et al, 2006). Note that the best system performance is now closely approaching the ITA for this data of over 90%. Table 4 shows theperformance of the top 8 systems on all the indi vidual verbs and nouns in the test set. Owing to space constraints we have removed some lemmas that have perfect or almost perfect accuracies. At theright are mentioned the average, minimum and max imum performances of the teams per lemma, and atthe bottom are the average scores per lemma (with out considering the lemma frequencies) and broken down by verbs and nouns. A gap of about 10 points between the verb and noun performance seems to indicate that in general the verbs were more difficult than the nouns. However, this might just be owing to this particular test sample having more verbs with higher perplexities, and maybe even ones that are indeed difficult to disambiguate ? in spite of highhuman agreement. The hope is that better knowledge sources can overcome the gap still existing between the system performance and human agree ment. Overall, however, this data indicates that theapproach suggested by (Palmer, 2000) and that is be ing adopted in the ongoing OntoNotes project (Hovyet al, 2006) does result in higher system perfor mance. Whether or not the more coarse-grained senses are effective in improving natural language processing applications remains to be seen. 89 Lemma S s T t 1 2 3 4 5 6 7 8 Average Min Max turn.v 13 8 340 62 58 61 40 55 52 53 27 44 49 27 61 go.v 12 6 244 61 64 69 38 66 43 46 31 39 49 31 69 come.v 10 9 186 43 49 46 56 60 37 23 23 49 43 23 60 set.v 9 5 174 42 62 50 52 57 50 57 36 50 52 36 62 hold.v 8 7 129 24 58 46 50 54 54 38 50 67 52 38 67 raise.v 7 6 147 34 50 44 29 26 44 26 24 12 32 12 50 work.v 7 5 230 43 74 65 65 65 72 67 46 65 65 46 74 keep.v 7 6 260 80 56 54 52 64 56 52 48 51 54 48 64 start.v 6 4 214 38 53 50 47 55 45 42 37 45 47 37 55 lead.v 6 6 165 39 69 69 85 69 51 69 36 46 62 36 85 see.v 6 5 158 54 56 54 46 54 57 52 48 48 52 46 57 ask.v 6 3 348 58 84 72 72 78 76 52 67 66 71 52 84 find.v 5 3 174 28 93 93 86 89 82 82 75 86 86 75 93 fix.v 5 3 32 2 50 50 50 50 50 0 0 50 38 0 50 buy.v 5 3 164 46 83 80 80 83 78 76 70 76 78 70 83 begin.v 4 2 114 48 83 65 75 69 79 56 50 56 67 50 83 kill.v 4 1 111 16 88 88 88 88 88 88 88 81 87 81 88 join.v 4 4 68 18 44 50 50 39 56 57 39 44 47 39 57 end.v 4 3 135 21 90 86 86 90 62 87 86 67 82 62 90 do.v 4 2 207 61 92 90 90 93 93 90 85 84 90 84 93 examine.v 3 2 26 3 100 100 67 100 100 67 100 33 83 33 100 report.v 3 2 128 35 89 91 91 91 91 91 91 86 90 86 91 regard.v 3 3 40 14 93 93 86 86 64 86 57 93 82 57 93 recall.v 3 1 49 15 100 100 87 87 93 87 87 87 91 87 100 prove.v 3 2 49 22 90 88 82 80 90 86 70 74 82 70 90 claim.v 3 2 54 15 67 73 80 80 80 80 80 87 78 67 87 build.v 3 3 119 46 74 67 74 61 54 74 61 72 67 54 74 feel.v 3 3 347 51 71 69 69 74 76 69 61 71 70 61 76 care.v 3 3 69 7 43 43 43 43 100 29 57 57 52 29 100 contribute.v 2 2 35 18 67 72 72 67 50 61 50 67 63 50 72 maintain.v 2 2 61 10 80 80 70 100 80 90 90 80 84 70 100 complain.v 2 1 32 14 93 86 86 86 86 86 86 79 86 79 93 propose.v 2 2 34 14 100 86 100 86 100 93 79 79 90 79 100 promise.v 2 2 50 8 88 88 75 88 75 75 62 88 80 62 88 produce.v 2 2 115 44 82 82 77 73 75 75 77 80 78 73 82 prepare.v 2 2 54 18 94 83 89 89 83 86 83 83 86 83 94 explain.v 2 2 85 18 94 89 94 89 94 89 89 94 92 89 94 believe.v 2 2 202 55 87 78 78 86 84 78 74 80 81 74 87 occur.v 2 2 47 22 86 73 91 96 86 96 86 82 87 73 96 grant.v 2 2 19 5 100 80 80 80 40 80 60 80 75 40 100 enjoy.v 2 2 56 14 50 57 57 50 64 57 50 57 55 50 64 need.v 2 2 195 56 89 82 86 89 86 78 70 70 81 70 89 disclose.v 1 1 55 14 93 93 93 93 93 93 93 93 93 93 93 point.n 9 6 469 150 91 91 89 91 92 87 84 79 88 79 92 position.n 7 6 268 45 78 78 78 53 56 65 58 64 66 53 78 defense.n 7 7 120 21 57 48 52 43 48 29 48 48 46 29 57 carrier.n 7 3 111 21 71 71 71 71 67 71 71 62 70 62 71 order.n 7 4 346 57 93 95 93 91 93 92 90 91 92 90 95 exchange.n 5 3 363 61 92 90 92 85 90 88 82 79 87 79 92 system.n 5 3 450 70 79 73 66 67 59 63 63 61 66 59 79 source.n 5 5 152 35 86 80 80 63 83 68 60 29 69 29 86 space.n 5 2 67 14 93 100 93 93 93 86 86 71 89 71 100 base.n 5 4 92 20 75 80 75 50 65 40 50 75 64 40 80 authority.n 4 3 90 21 86 86 81 62 71 33 71 81 71 33 86 people.n 4 4 754 115 96 96 95 96 95 90 91 91 94 90 96 chance.n 4 3 91 15 60 67 60 60 67 73 20 73 60 20 73 part.n 4 3 481 71 90 90 92 97 90 74 66 66 83 66 97 hour.n 4 2 187 48 83 85 92 83 77 90 58 92 83 58 92 development.n 3 3 180 29 100 79 86 79 76 62 79 62 78 62 100 president.n 3 3 879 177 98 97 98 97 93 96 97 85 95 85 98 network.n 3 3 152 55 91 87 98 89 84 88 87 82 88 82 98 future.n 3 3 350 146 97 96 94 97 83 98 89 85 92 83 98 effect.n 3 2 178 30 97 93 80 93 80 90 77 83 87 77 97 state.n 3 3 617 72 85 86 86 83 82 79 83 82 83 79 86 power.n 3 3 251 47 92 87 87 81 77 77 77 74 81 74 92 bill.n 3 3 404 102 98 99 98 96 90 96 96 22 87 22 99 area.n 3 3 326 37 89 73 65 68 84 70 68 65 73 65 89 job.n 3 3 188 39 85 80 77 90 80 82 69 82 80 69 90 management.n 2 2 284 45 89 78 87 73 98 76 67 64 79 64 98 condition.n 2 2 132 34 91 82 82 56 76 78 74 76 77 56 91 policy.n 2 2 331 39 95 97 97 87 95 97 90 64 90 64 97 rate.n 2 2 1009 145 90 88 92 81 92 89 88 91 89 81 92 drug.n 2 2 205 46 94 94 96 78 94 94 87 78 89 78 96 Average Overall 86 83 83 82 82 79 76 77 Verbs 78 75 73 76 73 70 65 70 Nouns 89 87 86 81 83 80 77 76 Table 4: All Supervised system performance per predicate. (Column legend ? S=number of senses in training; s=number senses appearing more than 3 times; T=instances in training; t=instances in test.; The numbers indicate system ranks.) 90 3 Semantic Role Labeling. Subtask 2 evaluates Semantic Role Labeling (SRL) systems, where the goal is to locate the constituents which are arguments of a given verb, and to assign them appropriate semantic roles that describe howthey relate to the verb. SRL systems are an important building block for many larger semantic systems. For example, in order to determine that ques tion (1a) is answered by sentence (1b), but not by sentence (1c), we must determine the relationships between the relevant verbs (eat and feed) and their arguments. (1) a. What do lobsters like to eat?b. Recent studies have shown that lobsters pri marily feed on live fish, dig for clams, sea urchins, and feed on algae and eel-grass. c. In the early 20th century, Mainers would only eat lobsters because the fish they caught was too valuable to eat themselves. Traditionally, SRL systems have been trained on either the PropBank corpus (Palmer et al, 2005) ? for two years, the CoNLL workshop (Carreras and Ma`rquez, 2004; Carreras and Ma`rquez, 2005) has made this their shared task, or the FrameNet corpus ? Senseval-3 used this for their shared task(Litkowski, 2004). However, there is still little con sensus in the linguistics and NLP communities about what set of role labels are most appropriate. ThePropBank corpus avoids this issue by using theory agnostic labels (ARG0, ARG1, . . . , ARG5), and by defining those labels to have only verb-specific meanings. Under this scheme, PropBank can avoidmaking any claims about how any one verb?s ar guments relate to other verbs? arguments, or aboutgeneral distinctions between verb arguments and ad juncts.However, there are several limitations to this ap proach. The first is that it can be difficult to make inferences and generalizations based on role labels that are only meaningful with respect to a single verb. Since each role label is verb-specific, we can not confidently determine when two different verbs? arguments have the same role; and since no encoded meaning is associated with each tag, we can notmake generalizations across verb classes. In con trast, the use of a shared set of role labels, such System Type Precision Recall F UBC-UPC Open 84.51 82.24 83.36?0.5 UBC-UPC Closed 85.04 82.07 83.52?0.5 RTV Closed 81.82 70.37 75.66?0.6 Without ?say? UBC-UPC Open 78.57 74.70 76.60?0.8 UBC-UPC Closed 78.67 73.94 76.23?0.8 RTV Closed 74.15 57.85 65.00?0.9Table 5: System performance on PropBank argu ments. as VerbNet roles, would facilitate both inferencingand generalization. VerbNet has more traditional la bels such as Agent, Patient, Theme, Beneficiary, etc. (Kipper et al, 2006).Therefore, we chose to annotate the corpus us ing two different role label sets: the PropBank role set and the VerbNet role set. VerbNet roles were generated using the SemLink mapping (Loper et al,2007), which provides a mapping between Prop Bank and VerbNet role labels. In a small number of cases, no VerbNet role was available (e.g., because VerbNet did not contain the appropriate sense of the verb). In those cases, the PropBank role label was used instead. We proposed two levels of participation in thistask: i) Closed ? the systems could use only the an notated data provided and nothing else. ii) Open ?where systems could use PropBank data from Sec tions 02-21, as well as any other resource for training their labelers. 3.1 Data. We selected 50 verbs from the 65 in the lexical sam ple task for the SRL task. The partitioning into train and test set was done in the same fashion as for the lexical sample task. Since PropBank does not tag any noun predicates, none of the 35 nouns from the lexical sample task were part of this data. 3.2 Results. For each system, we calculated the precision, re call, and F-measure for both role label sets. Scores were calculated using the srl-eval.pl script from the CoNLL-2005 scoring package (Carreras and Ma`rquez, 2005). Only two teams chose to perform the SRL subtask. The performance of these two teams is shown in Table 5 and Table 6. 91 System Type Precision Recall F UBC-UPC Open 85.31 82.08 83.66?0.5 UBC-UPC Closed 85.31 82.08 83.66?0.5 RTV Closed 81.58 70.16 75.44?0.6 Without ?say? UBC-UPC Open 79.23 73.88 76.46?0.8 UBC-UPC Closed 79.23 73.88 76.46?0.8 RTV Closed 73.63 57.44 64.53?0.9 Table 6: System performance on VerbNet roles. 3.3 Discussion. Given that only two systems participated in the task, it is difficult to form any strong conclusions. It should be noted that since there was no additional VerbNet role data to be used by the Open system, the performance of that on PropBank arguments as well as VerbNet roles is exactly identical. It can be seenthat there is almost no difference between the perfor mance of the Open and Closed systems for tagging PropBank arguments. The reason for this is the factthat all the instances of the lemma under consider ation was selected from the Propbank corpus, and probably the number of training instances for each lemma as well as the fact that the predicate is such an important feature combine to make the difference negligible. We also realized that more than half of the test instances were contributed by the predicate ?say? the performance over whose arguments is in the high 90s. To remove the effect of ?say? we alsocomputed the performances after excluding exam ples of ?say? from the test set. These numbers are shown in the bottom half of the two tables. Theseresults are not directly comparable to the CoNLL 2005 shared task since: i) this test set comprisesSections 01, 22, 23 and 24 as opposed to just Sec tion 23, and ii) this test set comprises data for only 50 predicates as opposed to all the verb predicates in the CoNLL-2005 shared task. The results in the previous discussion seem to confirm the hypothesis that there is a predictable correlation between human annotator agreement and sys tem performance. Given high enough ITA rates we can can hope to build sense disambiguation systemsthat perform at a level that might be of use to a con suming natural language processing application. Itis also encouraging that the more informative Verb Net roles which have better/direct applicability indownstream systems, can also be predicted with al most the same degree of accuracy as the PropBank arguments from which they are mapped. We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA/IPTO) under the GALE program, DARPA/CMO Contract No. HR0011-06-C-0022; National Science Foundation Grant NSF-0415923, Word Sense Disambiguation; the DTO-AQUAINT NBCHC040036 grant under the University of Illinois subcontract to University of Pennsylvania 2003-07911-01; and NSF-ITR-0325646: Domain-Independent Semantic Interpretation.']\n",
              "tfms - [TransformersTokenizer:\n",
              "encodes: (object,object) -> encodes\n",
              "decodes: (object,object) -> decodes\n",
              "]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOWQTwR9MxuF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df5af93d-642d-45bc-ba96-ceb5b789c4b5"
      },
      "source": [
        "tls.tfms(tls.train.items[0]).shape, tls.tfms(tls.valid.items[0]).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1024]), torch.Size([1024]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scg6ogue3o2r"
      },
      "source": [
        "bs, sl = 4,  256\n",
        "dls = tls.dataloaders(bs = bs, seq_len=sl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKaQZllM4W1Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6daffdd3-6d3c-4060-bd06-85c1de2caf9c"
      },
      "source": [
        "dls.show_batch(max_n=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Improving Unsupervised Dependency Parsing with Richer Contexts and Smoothing Unsupervised grammar induction models tend to employ relatively simple models of syntax when compared to their supervised counterparts. Traditionally, the unsupervised models have been kept simple due to tractability and data sparsity concerns. In this paper, we introduce basic valence frames and lexical information into an unsupervised dependency grammar inducer and show how this additional information can be leveraged via smoothing. Our model produces state-of-theart results on the task of unsupervised grammar induction, improving over the best previous work by almost 10 percentage points. The last decade has seen great strides in statistical natural language parsing. Supervised and semisupervised methods now provide highly accurate parsers for a number of languages, but require training from corpora hand-annotated with parse trees. Unfortunately, manually annotating corpora with parse trees is expensive and time consuming so for languages and domains with minimal resources it is</td>\n",
              "      <td>ving Unsupervised Dependency Parsing with Richer Contexts and Smoothing Unsupervised grammar induction models tend to employ relatively simple models of syntax when compared to their supervised counterparts. Traditionally, the unsupervised models have been kept simple due to tractability and data sparsity concerns. In this paper, we introduce basic valence frames and lexical information into an unsupervised dependency grammar inducer and show how this additional information can be leveraged via smoothing. Our model produces state-of-theart results on the task of unsupervised grammar induction, improving over the best previous work by almost 10 percentage points. The last decade has seen great strides in statistical natural language parsing. Supervised and semisupervised methods now provide highly accurate parsers for a number of languages, but require training from corpora hand-annotated with parse trees. Unfortunately, manually annotating corpora with parse trees is expensive and time consuming so for languages and domains with minimal resources it is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>these cases, is being used as a reliable source of information.&amp;quot; Following are examples of subjective and objective sentences: In sentence 4, there is no uncertainty or evaluation expressed toward the speaking event. Thus, from one point of view, one might have considered this sentence to be objective. However, the object of the sentence is not presented as material that is factual to the reporter, so the sentence is classified as subjective. Linguistic categorizations usually do not cover all instances perfectly. For example, sentences may fall on the borderline between two categories. To allow for uncertainty in the annotation process, the specific tags used in this work include certainty ratings, ranging from 0, for least certain, to 3, for most certain. As discussed below in section 3.2, the certainty ratings allow us to investigate whether a model positing additional categories provides a better description of the judges' annotations than</td>\n",
              "      <td>cases, is being used as a reliable source of information.&amp;quot; Following are examples of subjective and objective sentences: In sentence 4, there is no uncertainty or evaluation expressed toward the speaking event. Thus, from one point of view, one might have considered this sentence to be objective. However, the object of the sentence is not presented as material that is factual to the reporter, so the sentence is classified as subjective. Linguistic categorizations usually do not cover all instances perfectly. For example, sentences may fall on the borderline between two categories. To allow for uncertainty in the annotation process, the specific tags used in this work include certainty ratings, ranging from 0, for least certain, to 3, for most certain. As discussed below in section 3.2, the certainty ratings allow us to investigate whether a model positing additional categories provides a better description of the judges' annotations than a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>this expectation to leverage transliteration, and thus the identification of named entities across languages. Our idea is that the occurrence of a cluster of names in, say, an English text, should be useful if we find a cluster of what looks like the same names in a Chinese or Arabic text. An example of what we are referring to can be found in Figure 1. These are fragments of two stories from the June 8, 2001 Xinhua English and Chinese newswires, each covering an international women’s badminton championship. Though these two stories are from the same newswire source, and cover the same event, they are not translations of each other. Still, not surprisingly, a lot of the names that occur in one, also occur in the other. Thus (Camilla) Martin shows up in the Chinese version asAi'1: ma-er-ting; Judith Meulendijks is T - V fL' A A JW yu</td>\n",
              "      <td>expectation to leverage transliteration, and thus the identification of named entities across languages. Our idea is that the occurrence of a cluster of names in, say, an English text, should be useful if we find a cluster of what looks like the same names in a Chinese or Arabic text. An example of what we are referring to can be found in Figure 1. These are fragments of two stories from the June 8, 2001 Xinhua English and Chinese newswires, each covering an international women’s badminton championship. Though these two stories are from the same newswire source, and cover the same event, they are not translations of each other. Still, not surprisingly, a lot of the names that occur in one, also occur in the other. Thus (Camilla) Martin shows up in the Chinese version asAi'1: ma-er-ting; Judith Meulendijks is T - V fL' A A JW yu mo-lun-di-ke-si;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>generator realizes each set of combined propositions as a sentence, mapping from concepts to words and building syntactic structure. Our approach differs in the following ways: On 3th of September 1995, 120 hostages were released by Bosnian Serbs. Serbs were holding over 250 U.N. personnel. Bosnian serb leader Radovan Karadjic said he expected &amp;quot;a sign of goodwill&amp;quot; from the international community. U.S. F-16 fighter jet was shot down by Bosnian Serbs. Electronic beacon signals, which might have been transmitted by a downed U.S. fighter pilot in Bosnia, were no longer being received. After six days, O'Grady, downed pilot, was rescued by Marine force. The mission was carried out by CH-53 helicopters with an escort of missile- and rocket-armed Cobra helicopters. information needed for clarification (entity descriptions, temporal references, and newswire source references). We developed techniques to map predicateargument structure produced by the content-planner to the functional representation expected by</td>\n",
              "      <td>realizes each set of combined propositions as a sentence, mapping from concepts to words and building syntactic structure. Our approach differs in the following ways: On 3th of September 1995, 120 hostages were released by Bosnian Serbs. Serbs were holding over 250 U.N. personnel. Bosnian serb leader Radovan Karadjic said he expected &amp;quot;a sign of goodwill&amp;quot; from the international community. U.S. F-16 fighter jet was shot down by Bosnian Serbs. Electronic beacon signals, which might have been transmitted by a downed U.S. fighter pilot in Bosnia, were no longer being received. After six days, O'Grady, downed pilot, was rescued by Marine force. The mission was carried out by CH-53 helicopters with an escort of missile- and rocket-armed Cobra helicopters. information needed for clarification (entity descriptions, temporal references, and newswire source references). We developed techniques to map predicateargument structure produced by the content-planner to the functional representation expected by FUF/SURGE(Elhaklad,</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH0wV23i3yTA"
      },
      "source": [
        "class DropOutput(Callback):\n",
        "    def after_pred(self): self.learn.pred = self.pred[0]\n",
        "\n",
        "learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity()).to_fp16()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKf0F9oD37Wb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f078b21d-9f90-4ae9-afbe-52decb70ef85"
      },
      "source": [
        "learn.validate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#2) [10.029350280761719,22682.529296875]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V43rOgCT4C4y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "d4e5b105-2760-48ec-fa36-feae540825a5"
      },
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SuggestedLRs(valley=4.365158383734524e-05)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEMCAYAAADeYiHoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3yV5f3/8dcniyRkMRJGEgwge2NAwFHcoygqKo7WWa3aaqf9avur9Wu19fvtdFWhVbFfUYqoVesuoKgMCRgQmSGskEAWWYTsz++PnGhMT0LGOec+4/N8PPJIzn3uc+7PRUjeua/rvq9LVBVjjDGmrTCnCzDGGOOfLCCMMca4ZQFhjDHGLQsIY4wxbllAGGOMccsCwhhjjFteCwgReUZECkVki5vnfiIiKiL9vXV8Y4wxPePNM4hFwPltN4pIOnAusN+LxzbGGNNDEd56Y1VdJSIZbp76E/Az4LXOvlf//v01I8PdWxljjGnPhg0bilU1ubuv91pAuCMic4GDqrpJRDr9uoyMDLKysrxXmDHGBCER2deT1/ssIEQkFvg5zd1Lndn/VuBWgCFDhnixMmOMMe748iqm4cBQYJOI7AXSgI0iMtDdzqq6UFUzVTUzObnbZ0jGGGO6yWdnEKr6OZDS8tgVEpmqWuyrGowxxnSe1wJCRF4EZgP9RSQP+JWqPu2p96+vrycvL4+amhpPvWXAiY6OJi0tjcjISKdLMcYEIW9exXT1cZ7P6Mn75+XlER8fT0ZGBl0Z8A4WqkpJSQl5eXkMHTrU6XKMMUEoYO+krqmpoV+/fiEZDgAiQr9+/UL6DMoY410BGxBAyIZDi1BvvzHBrLKmnrc/L6CkqtaxGgI6IAJJXFwcAHv37mX8+PEOV2OM8Xdb8yu4ffFGPj9Y7lgNoRMQm5fCn8bD/UnNnzcvdboiY4xpV05RFQAnpsQ5VkNoBMTmpfDGXVB+ANDmz2/c1aOQuOeee3jiiSe+fHz//ffz4IMPctZZZzF16lQmTJjAa691PJtIY2Mjd999N9OmTWPixIksWLAAgOuuu45//vOfX+537bXXHve9jDHBJaewipjIcAYnxjhWQ2gExPIHoP7Y17fVH2ve3k3z589n6dKvAmbp0qVcf/31vPrqq2zcuJGVK1fyk5/8BFVt9z2efvppEhMTWb9+PevXr+evf/0re/bs4eabb2bRokUAlJeXs3r1ar75zW92u1ZjTODJKaxieEpvwsKcG2v06VxMjinP69r2TpgyZQqFhYXk5+dTVFREnz59GDhwID/60Y9YtWoVYWFhHDx4kMOHDzNwoNubxXnvvffYvHkzy5Ytay6nvJxdu3Zx7rnncscdd1BUVMTLL7/MvHnziIgIjW+VMabZ7sIqpg/t62gNofFbJzHN1b3kZnsPXHHFFSxbtoxDhw4xf/58Fi9eTFFRERs2bCAyMpKMjIwOL0NVVR577DHOO++8/3juuuuu4/nnn2fJkiU8++yzParTGBNYjtY2kF9e4+j4A4RKF9NZ90Fkm368yJjm7T0wf/58lixZwrJly7jiiisoLy8nJSWFyMhIVq5cyb59HU+keN555/Hkk09SX18PwM6dOzl69CgAN9xwA3/+858BGDt2bI/qNMYElt1+MEANoXIGMfHK5s/LH2juVkpMaw6Hlu3dNG7cOCorK0lNTWXQoEFce+21XHTRRUyYMIHMzExGjx7d4eu/853vsHfvXqZOnYqqkpyc/OXg9IABAxgzZgyXXHJJj2o0xgSenEL/CAjpaBDVX2RmZmrb9SC2bdvGmDFjHKrI+6qrq5kwYQIbN24kMTGx3f2C/d/BmFD0v+9sZ+GqXLb9+nwiw7vf0SMiG1Q1s7uvD40upgDz73//mzFjxnDnnXd2GA7GmOCUU1hFRv/ePQoHTwiNLqYAc/bZZx93/MIYE7xyiqoYmRLvdBl2BmGMMf6krqGJfSXVjo8/QIAHRCCMn3hTqLffmGC0r+QojU1qAdET0dHRlJSUhOwvyZb1IKKjo50uxRjjQf5yBRN4d0W5Z4A5QKGqjndt+zUwF2gCCoEbVDW/O++flpZGXl4eRUVFnio54LSsKGeMCR4tATEsubfDlXh3kHoR8Djw91bbfqeqvwQQkbuA+4DbuvPmkZGRtpKaMSbo5BRVkZoUQ2yU89cQea2LSVVXAaVttlW0etgbCM3+IWOMaceuw1V+0b0EDlzmKiIPAdcB5cAZvj6+Mcb4q6YmJbe4ipnD+zldCuDAILWq/kJV04HFwPfb209EbhWRLBHJCuVxBmNM6DhYdoya+ia/OYNw8iqmxcC89p5U1YWqmqmqmcnJyT4syxhjnOFPVzCBjwNCREa0ejgX2O7L4xtjjD/7MiCS/SMgvHmZ64vAbKC/iOQBvwIuFJFRNF/muo9uXsFkjDHBKKewiv5xUfTpHeV0KYAXA0JVr3az+WlvHc8YYwJdTlEVw/3k7AEC+E5qY4wJJqpKTqH/XOIKFhDGGOMXiqvqKD9WbwFhjDHm6/ztCiawgDDGGL+QU1gJWEAYY4xpY/3eI6TE92Jggv/M0GwBYYwxDlNV1uaWMHN4P0TE6XK+ZAFhjDEOyy0+SmFlLTOG+cccTC0sIIwxxmFrc0sALCCMMcZ83drcUgYmRJPRL9bpUr7GAsIYYxykqqzZXcKMYX39avwBLCCMMcZRu4uOUlzlf+MPYAFhjDGOahl/8JdFglqzgDDGGAetyS1hUGI0Q/r61/gDWEAYY4xjVJV1uSXMGOZf9z+0sIAwxhiH5BRWUVxVx0w/HH8ACwhjjHGMv97/0MJrASEiz4hIoYhsabXtdyKyXUQ2i8irIpLkreMbY4y/W5tbyuDEaNL7xjhdilvePINYBJzfZtv7wHhVnQjsBO714vGNMcZvtcy/NMPP5l9qzWsBoaqrgNI2295T1QbXw7VAmreOb4wx/mxXYRUlR+v8tnsJnB2DuAl428HjG2OMY768/8EC4utE5BdAA7C4g31uFZEsEckqKiryXXHGGOMD63JLSU2KId0P739o4fOAEJEbgDnAtaqq7e2nqgtVNVNVM5OTk31WnzHG+MJn+48w9YQ+TpfRIZ8GhIicD/wMuFhVq315bGOM8ReFFTXkl9cwKS3R6VI65M3LXF8E1gCjRCRPRG4GHgfigfdFJFtEnvLW8Y0xxl9lHygDYMoQ/77SP8Jbb6yqV7vZ/LS3jmeMMYFiU14ZEWHCuMEhegZhjDHGvewDZYweFE90ZLjTpXTIAsIYY3yoqUnZfKCcyen+3b0EFhDGGONTucVVVNY2MCnNAsIYY0wrn+0PjAFqsIAwxhif2pRXRnyvCIb1j3O6lOOygDDGGB/KPlDGxPREwsL8c4K+1iwgjDHGR2rqG9leUBkQ4w9gAWGMMT7zRX45DU0aEFcwgQWEMcb4TMsAtQWEMcaYr9mUV87gxGhSEqKdLqVTLCCMMcZHsg8cYVKAnD2ABYQxxvhESVUtB0qPBUz3ElhAGGOMT2zKax5/sDMIY4wxX5N9oJwwgQmp/j2Da2sWEMYY4wPZB8oYOSCe3r28tsqCx1lAGGOMlzU0NvHZ/iMBMf9Sa95cUe4ZESkUkS2ttl0hIl+ISJOIZHrr2MYY40827DtCZU0Dp49IdrqULvHmGcQi4Pw227YAlwGrvHhcY4zxKyt3FBERJpwyor/TpXSJN5ccXSUiGW22bQMQ8f9JqowxxlNWbi9kWkZfEqIjnS6lS2wMwhhjvOhg2TF2HK7kjNGB1b0EfhwQInKriGSJSFZRUZHT5RhjTLes3F4IwJmjUxyupOv8NiBUdaGqZqpqZnJy4CWvMcYAfLCjkPS+MQxP9v8Fgtry24AwxphAV1PfyCc5JZwxKiUgx169eZnri8AaYJSI5InIzSJyqYjkATOBN0XkXW8d3xhjnLZuTynH6hs5IwC7l8C7VzFd3c5Tr3rrmMYY409Wbi8kOjKMmcP6OV1Kt1gXkzHGeIGqsmJ7IbOG9yc6MtzpcrrFAsIYY7wgt/go+0urOWNU4F5kYwFhjDFe0HJ56+xRgTn+ABYQxhjjFSt3FDIiJY70vrFOl9JtFhDGGONhVbUNfLqnNCBvjmvNAsIYYzxs/Z5S6huV00cG7vgDWEAYY4zHrcktISo8jKlD+jhdSo9YQBhjjIetzS1hcnoSMVGBeXlrCwsIY4zxoIqaerYcLGfG8MC8Oa41CwhjjPGg9XtKaVKYMayv06X0mAWEMcZ40JrdJURFBP74A1hAGGOMR63JLWHqkKSAnV6jNQsIY4zxkPLqerYWVDAjQCfna8sCwhhjPGTdnhJUCdjZW9uygDDGGA9Zk1tCr4gwJg9JcroUj7CAMMYYD1mbW8pJJ/ShV0Tgjz+Ad1eUe0ZECkVkS6ttfUXkfRHZ5foc+MP8xhgDHDlax7aCiqDpXgLvnkEsAs5vs+0eYLmqjgCWux4bY0zAW7enFCAobpBr0amAEJHeIhLm+nqkiFwsIpEdvUZVVwGlbTbPBZ5zff0ccEkX6zXGGL+0NreEmMhwJqUFx/gDdP4MYhUQLSKpwHvAt2k+Q+iqAapa4Pr6EDCgG+9hjDF+Z83uEjIz+hAVETxDu51tiahqNXAZ8BdVvQIY15MDq6oC2u4BRW4VkSwRySoqKurJoYwxxqtKqmrZcbgyaO5/aNHpgBCRmcC1wJuubd0Zpj8sIoNcbzgIKGxvR1VdqKqZqpqZnBzYc6obY4LbxznFACEbED8E7gVeVdUvRGQYsLIbx3sduN719fXAa914D2OM8StLPj1AalIMk9ODZ/wBIKIzO6nqh8CHAK7B6mJVvauj14jIi8BsoL+I5AG/Ah4GlorIzcA+4Mrul26MMc7LKaxiTW4Jd583ivAwcbocj+pUQIjIC8BtQCOwHkgQkUdU9XftvUZVr27nqbO6XKUxxvipxev2ERkuXJmZ7nQpHtfZLqaxqlpB82WpbwNDab6SyRhjQlZ1XQPLNuRx/vhBJMf3crocj+tsQES67nu4BHhdVevp4AokY4wJBW9syqeypoFvzzjB6VK8olNdTMACYC+wCVglIicAFd4qylNeWLef9XtLuWxqKrOG9++wf7C2oZHVu0tYsa2Q8DBh9MB4Rg9KYOSAOGKjOvvPZIwJJc+v3c/IAXFMywjOWYM6O0j9KPBoq037ROQM75TkORU19SzfdphXPzvIwIRoLpmSynnjBhAeJlTXNXKsrpHSo3V8sLOID7YXUlnbQG/XIuNH6xoBEIGJaUncOCuDCycMCqqbYIwx3bfpQBmfHyzngbnjEAmuwekW0ny/2nF2Ekmk+Sqk012bPgQeUNVyL9b2pczMTM3KyurWa2vqG1m+rZBXNubxwc4iGpv+s739ekdxztgBnDduILNO7EdkWBh5R46x7VAFW/Mr+NfmfHYXHWVAQi+um5nBtScPISk2qqfNMsYEsJ++tIm3Pi9g3c/PIj66w5mHHCMiG1Q1s9uv72RAvAxs4at5lL4NTFLVy7p74K7oSUC0VlxVy/o9pUSGhxEbFU5MVDhxvSIYlhzXYfdTU5Py4a4inv5oDx/nFNMrIoyLJg3m2pOHMDk9qUd/PVTU1HPwyDEKK2upa2iiobGJ+ialobGJhialsc1Hy3dLValrbKKypoHKmnoqaxo4WttIfWMT9Y1NNDQ2P1/b0ERtQyO19c2fe0WEkxATSUJ0BIkxkfSJjaJfXBT943rRLy6KtD4xTEhNsjMlYzpQVl3Hyb9ZzryT0vjNpROcLqddPQ2IznauD1fVea0e/7eIZHf3oE7pH9eLCyYM6vLrwsKEM0alcMaoFLYfquC51ft4LfsgyzbkMWZQAvMz00jtE0uviDCiI8PpFRFGZHgYEeFCRJgQERZGaXUduwuryC2uYnfhUfaWHOVg2TEqaxp61KaIMCE+OoL46Eh694ogKly+PHZ8ZAT9I8KJjgyjV0Q4vSLDqK1vovxYPRU19ewvrSb7QBklR+u+dmYVGxXOjGH9OOXE/pw2oj8jUuKC9hTamO54ZeNBahua+NbJwTk43aKzAXFMRE5V1Y8BROQU4Jj3yvJfowcm8NvLJvDzC0fzWnY+i9ft5/43tnb69eFhwgl9Y8no35vpQ/uSmhRDap8YBiVG0ysi/GuhEhEuhIe5PqT5s+D6RS0QFR5GdGRYj395NzUp5cfqKa6qZXdRFR/nFPNJTgkrtjfPhJLeN4Zzxgzk7LEpTM/oS0S4nV2Y0JZ9oIy0PjGMHZzgdCle1dkupknA34FE16YjwPWqutmLtX3JU11M3qCq7CupprKmobkrp6GJmvpG6huVhqbmrp76xiYSYiIZnhzHkL6xAdN9k3ekmlU7i3l/6yE+2V1CXUMTSbGRzJk4iGumnxD0PxzGtOeKp1YjIiz97kynS+mQT7qYVHUTMElEElyPK0Tkh4BPAsKfiQgZ/Xs7XYZXpPWJ5ZqTh3DNyUM4WtvAqp1FvPPFIZZm5fH82v1MTk/imulDuGjSYGKigmOJRWM6o6C8hswTgvPS1ta69Kesqla47qgG+LEX6jF+qnevCC6YMIhHrprCpz8/i/vmjKWqtoGfvbyZGb9dzsNvbye/LCR7HU2IaWpSDlfUMCgpxulSvK4nd4DZqGWISoqN4qZTh3LjKRms33uERav3sHDVbv76US4XjB/IracPY2IQraplTGvFVbXUNyqDEqOdLsXrehIQNtVGiBMRpg/ty/Shfck7Us3f1+zjxU/38+bnBdw4ayh3nzfKup5M0MkvrwFgUGLwn0F02MUkIpUiUuHmoxIY7KMaTQBI6xPLzy8cw5p7z+K6GSfwzCd7OP+RVazLLXG6NGM8qsDVlRoKZxAdBoSqxqtqgpuPeFW1CYrMf4jrFcF/zx3Pi7fMQBXmL1zLr17bQlVtz+73MMZfFLjOIAaHwBhEYFxvaQLOzOH9eOeHp3HDrAz+vnYf5/7xQ1ZsP+x0Wcb0WEH5MXpFhNEn1j+n1/AkRwJCRH4gIltE5AvX5bImCMVGRXD/xeNYdttM4qIjuGlRFt97YSOFlTVOl2ZMt+WX1zAoMTokZhfweUCIyHjgFmA6MAmYIyIn+roO4zsnndCXf915Gj89dyTvbz3M2X/4kBfW7afJzcSJxvi7grJjITFADc6cQYwB1qlqtao20DwzrE8m/TPOiYoI4/tnjuCdH5zG2MEJ/PzVz7lywRp2Hq50ujRjuuRQeQ2DkoJ/gBqcCYgtwGki0k9EYoELgeBbzNW4NSw5jhdvmcHvLp9ITlEV33z0I37/7g5q6hudLs2Y42psUg5X1jLYziC8Q1W3Af8DvAe8A2QD//HbQURuFZEsEckqKirycZXGm0SEKzLTWf7jb3DRxME8vjKHuY9/Qk5hldOlGdOhwsoaGpuUgSFwiSs4NEitqk+r6kmqejrNE//tdLPPQlXNVNXM5ORk3xdpvK5fXC/+OH8yz944jaKqWi5+/GNeyz7odFnGtCu/rOUSVwsIrxGRFNfnITSPP7zgRB3GP5wxKoU37zqVcYMT+MGSbO595XPrcjJ+6VAI3UUNzt0H8bKIbAXeAL6nqmUO1WH8xKDEGF64ZQbf/cYwXvx0P3Mf/4TP9h9xuixjvqagvPkuahuD8CJVPU1Vx6rqJFVd7kQNxv9Ehodx7wVjePbGaZQfq+eyJ1dz/+tf2F3Yxm/kl9UQExlOQkxoTCRhd1Ibv3PGqBTe//HpXDfjBJ5bs5dz/vghy7fZXdjGeQXlxxiUFBo3yYEFhPFT8dGR/Pfc8bx8+ywSoiO5+bks7n1lM9V1djZhnFNQXhMy3UtgAWH83NQhfXjjzlO5Y/Zwlqw/wJxHP+bzvHKnyzIhqqD8WEjM4trCAsL4vaiIMH52/mhe+M4MqusauezJT3jqw902VYfxqfrGJgoray0gjPFHLTPEnj1mAA+/vZ0bF62n9Gid02WZEHG4ogZVQmKp0RYWECagJMVG8Zdrp/LgJeNZk1vChY98RNbeUqfLMiHgq3sg7AzCGL8lInxrxgm8cvssekWGMX/hWhau2o2qdTkZ78kPoYWCWlhAmIA1PjWRN+48lfPGDeA3b23nzhc/o7bB7sA23tGy1GiozMMEFhAmwCVER/LENVO554LR/GtzAdc9/Snl1fVOl2WCUEF5DXG9IkiIDv6V5FpYQJiAJyLc9o3hPHLVZDbuP8IVC1aT7/przxhPCbVLXMECwgSRuZNTee6m6RSU1XDpXz5ha36F0yWZIFJQXhNSVzCBBYQJMrOG9+el22ciCJc/tZp3thQ4XZIJEvllNQxKsDMIYwLa6IEJvPb9UxgxIJ7bnt/IH9/faTfVmR6pbWikuKo2ZJYabWEBYYLSgIRo/nHrDOZNTePR5bv47vMbbFZY022FFbVA6Ezz3cICwgSt6Mhwfn/FRO6bM5YV2wu59IlP2FN81OmyTABquejBziCMCSIiwk2nDuX/bppOsWtZ0xXbbepw0zUFIbaSXAunlhz9kYh8ISJbRORFEQmtWDY+N+vE/rz+/VNJ7xPLzc9l8djyXTYuYTot37WSnF3m6mUikgrcBWSq6nggHLjK13WY0JPeN5aXb5/F3EmD+cP7O7l98QaO2riE6YSCshoSoiPo3Ss0VpJr4VQXUwQQIyIRQCyQ71AdJsTERIXzp/mT+eWcsby/9TBXLljz5SRsxrSnoLwmpOZgauHzgFDVg8Dvgf1AAVCuqu/5ug4TukSEm08dytM3TGNv8VEueeITDny4CP40Hu5Pav68eanTZRo/0dik7DhcQaoFhPeJSB9gLjAUGAz0FpFvudnvVhHJEpGsoqIiX5dpQsAZo1J46bZZnNe4iv4r7obyA4A2f37jLgsJA8Abm/I5UHqMeSelOV2KzznRxXQ2sEdVi1S1HngFmNV2J1VdqKqZqpqZnJzs8yJNaBg7OIH7YpcRI20WHqo/BssfcKYo4zcaGpt4ZPkuRg+M5/xxA50ux+ecCIj9wAwRiRURAc4CtjlQhzEAhFcedP9EeZ5vCzF+55/Z+ewpPsqPzhlJWJg4XY7POTEGsQ5YBmwEPnfVsNDXdRjzpUT3XQdNCak+LsT4k/rGJh5dvovxqQmcO3aA0+U4wpGrmFT1V6o6WlXHq+q3VbXWiTqMAeCs+yDy6wOQ1RrFn/Vqiqvsv2aoenlDHvtLq/nxOSNp7uwIPXYntTETr4SLHoXEdEAgMZ1dJz/EgiMnMe/J1ewrsek5Qk1dQxOPrchhUnoSZ4xKcbocx4TWXR/GtGfilc0fLpOAF8cf4eZF67nsL6t59sZpTExLcq4+41NLsw5wsOwYv7lsQsiePYCdQRjTrqlD+rDs9lnERIVz1cK1rNxR6HRJxgeqaht4fEUOJ53Qh9NH9He6HEdZQBjTgeHJcbxy+ywy+vXmO89l8VLWAadLMl5232tbKKys4ecXjg7pswewgDDmuFISovnHd2cwa3g/7l62mUeX70LVJvoLRq9lH+SVjQe588wRnHRCX6fLcZwFhDGdEB8dydPXT+Oyqan88f2d3PvK5zQ0NjldlvGgA6XV/L9Xt3DSCX2488wTnS7HL9ggtTGdFBURxh+umERqUgyPrcjhUEUNT1wzNeRm+AxGDY1N/GDJZwD8ef5kIsLtb2ewMwhjukRE+Mm5o/jNpRNYtbOIqxaupajS7pUIdI+uyGHj/jIeumwC6X1jnS7Hb1hAGNMN15w8hL9dn0lOYRXznlzNXlvKNGBtK6jg8RW7mDc1jYsnDXa6HL9iAWFMN505egAv3HIylTX1zHtyNZvzypwuyXTDkk/3Exkexn1zxjpdit+xgDCmB6YM6cPLre6V+MDulQgotQ2NvLYpn3PHDSQxNtLpcvyOBYQxPTQsOY5X7vjqXoml6+1eiUCxcnshZdX1zJtqEzO6YwFhjAekxDffKzFzeD9+9vJmfvv2Npqa7F4Jf7dsw0FS4ntx2ghbc8YdCwhjPCQ+OpJnb5jGt2YMYcGHudyxeCPH6hqdLsu0o7iqlg92FHLp1FTCQ3Cth86wgDDGgyLCw/j13PHcN2cs7249xJUL1nC4osbpsowbr2Xn09CkXD419JYS7SwLCGM8TES46dSh/O26THYXVXHpE5+w41Cl02WZNl7ekMfEtERGDIh3uhS/5fOAEJFRIpLd6qNCRH7o6zqM8bazxgxg6Xdn0qjK5U+u5uNdxU6XZFy2FVSwtaCCeXb20CEnlhzdoaqTVXUycBJQDbzq6zqM8YXxqYm8escppPaJ4YZnP2WpzQbrF17ekEdkuNiNccfhdBfTWcBuVd3ncB3GeM3gpBiW3jaz+QqnZZv5w3s77AonB9U3NvHP7IOcNXoAfXpHOV2OX3M6IK4CXnS4BmO8LiE6kmdumMb8zHQeW5HDHYs3Ul3X4HRZIWnVziKKq+qYd5J1Lx2PYwEhIlHAxcBL7Tx/q4hkiUhWUVGRb4szxgsiw8N4eN4E/t83x/De1kPMe3INB8uOOV1WyFm2IY9+vaOYPcrufTgeJ88gLgA2quphd0+q6kJVzVTVzORk+0aa4CAifOe0YTxzwzTySquZ+/jHbNhX6nRZIaP0aB3/3naYS6akEmlTeh+Xk/9CV2PdSyZEzR6Vwqvfm0VcrwiuXriOd7YccrqkkPB69kHqG5XLrXupUxwJCBHpDZwDvOLE8Y3xByemxPPP753CuNQE7li8weZw8oGXNuQxPjWBMYMSnC4lIDgSEKp6VFX7qWq5E8c3xl8kxUax+Dsnc+qIZH728mae+nC30yUFra35FXyRX8EVJ6U7XUrAsE44YxwWGxXB367LZM7EQTz89nZ++9Y2VO0yWE9btiGPqPAwu/ehC2wxXWP8QFREGI9cNYU+sVEsWJVL6dE6fnvZBFsb2UPqGprvfTh7bIrd+9AFFhDG+InwMOGBuePo2zuKR5bv4kh1PY9fM4XoyHCnSwt4K7YXUnq0zrqXusj+PDHGj4gIPzpnJL+eO47l2w9z3dOfUn6s3umyAt6yDXmudR/6O11KQLGAMMYPfXtmBo9dPYXPDhxhvk0Z3iNFlbWsdK37YF12XWP/Wsb4qTkTB/PsDdM5UFptU4Z3k6qy5NP9NDYpV9i9D11mAWGMHzt1RH/+8d2ZzQvbPLmaT3JsyjSyK9QAAAyeSURBVPDOaGhs4s3NBVz6l9X84f2dzBzWjxNTbN2HrrKAMMbPjU9N5NXvncKgpGiuf+ZTlm3Ic7okv1XX0MSiT/Yw+/cf8L0XNlJWXcev547j6RsynS4tINlVTMYEgNSkGJbdPovbn9/AT1/axP7San541gjCbC1loLkr6d0vDvPw29vYW1LNtIw+/HLOWM4eM8DWm+4BCwhjAkRCdCTP3jCdX7z6OY8u38WOQxX88crJ9O4V2j/Gm/PKePBf2/h0bykjB8Sx6MZpzB6V4nRZQSG0/2cZE2CiIsL438snMnpQAg+9uZV5T67mr9dlkt431unSfK6mvpHfvbuDZz7ZQ9/YKB66dDzzM9PtSiUPsoAwJsCICDefOpQRKXF8/4WNXPz4x/z5qimcPqI/IqHRnbI5r4wfL91ETmEV35oxhP86fzTx0ZFOlxV0JBDmfMnMzNSsrCynyzDG7+wpPsotf88ip7CKESlxXJmZzqVTU+kf18vp0ryivrGJJ1bm8NiKHJLjevG/l0/k9JG2Xkx7RGSDqnZ7hN4CwpgAV13XwGvZ+SzNOsBn+8uICBNmj0rh3HEDOGNUCsnxwREWa3aXcN9rW9hVWMWlU1K5/6JxJMbaWUNHLCCMMV/adbiSlzbk8Xp2Podcd19PSkvkzNEDuObkIQEZFoUVNTz01jZey84nrU8M9180jrPHDnC6rIBgAWGM+Q+qytaCClZuL2T59kKyD5QRFxXBXWeN4PpZGURF+P9A7rG6Rhat3ssTK3Ooa2jitm8M444zTrTJC7sgIANCRJKAvwHjAQVuUtU17e1vAWFMz+QWVfHgm9tYsb2QYcm9+eWcsZzhp5eC1jU08Y/1+3l0RQ5FlbWcOTqF++aMJaN/b6dLCziBGhDPAR+p6t9EJAqIVdWy9va3gDDGM1ZuL+SBf21lT/FRZg3vxx2zT+SUE/v5xdVPhZU1vPvFYRZ8uJu8I8eYltGHu88bzfShfZ0uLWAFXECISCKQDQzTTh7cAsIYz6lraOLva/aycFUuhZW1TExL5PZvDOfccQN9ftfxzsOVvL/1MO9vPUz2gea/EcenJvDTc0fxjZHJfhFcgSwQA2IysBDYCkwCNgA/UNWj7b3GAsIYz6ttaOSVjQdZ8OFu9pZUM3pgPA/Pm8jk9CSvHnd/STWvbzrI65vy2Xm4CoBJ6UmcMyaFs8cOYNSAeAsGDwnEgMgE1gKnqOo6EXkEqFDVX7bZ71bgVoAhQ4actG/fPp/WaUyoaGxS3vq8gIfe3EZhZQ03njKUn5w7ktgoz91HW1lTz+ub8nkpK+/LM4VpGX24eNJgzhs3kJSEaI8dy3wlEANiILBWVTNcj08D7lHVb7b3GjuDMMb7Kmvq+Z93tvP82v2k9YnhgbnjmD0ypdsTAqoqnx0oY8mn+3ljUwHH6hsZPTCeS6akctGkwaQmxXi4BaatngaEz6faUNVDInJAREap6g7gLJq7m4wxDoqPjuTBSyZw8aRU7nl5MzctyiIlvhfnjRvIBeMHMn1o3w7nOcotquLTPaVsP1TJtoIKth+qpPxYPbFR4Vw8aTBXnzyESWmJ1n0UQJy6imkyzZe5RgG5wI2qeqS9/e0Mwhjfqqlv5N0vDvH254f4YGchNfVN9O0dxexRyZw5OoXTRyaTEB1JdV0Db31+iH+s38/6vc0/wr2jwhk1MJ7RgxKYnJbEBRMG2jxJDgm4LqbusIAwxjnVdQ18uKOId784xAc7iyirriciTJiUnsTOQ5VU1jYwtH9v5k9L5/xxAxnSN9bWqfATAdfFZIwJLLFREVwwYRAXTBhEQ2MT2QfKWL69kNU5xZw9dgBXTUtn+tC+1nUUhCwgjDGdFhEeRmZGXzIz7Oa1UOD/E7IYY4xxhAWEMcYYtywgjDHGuGUBYYwxxi0LCGOMMW5ZQBhjjHHLAsIYY4xbFhDGGGPcCoipNkSkCGiZ7zsRKG/1dOvH7r7uDxT34PBtj9fVfdw911Eb2j4O5ja13mZt6lq9ndmnq23qzNc9aVNn2tPRfp1pT9tt/vCz1NF+3v79cIKqJneiPvdUNaA+gIXtPXb3NZDlyeN1dR93z3XUhlBqU5tt1iaH29TJr7vdps60p6P9OtOerrbJF9+jnrbJ178fWn8EYhfTGx08bu9rTx6vq/u4e66jNrR9HMxt8lR7Ovte1qbu/V/z5f+7jvbrTHvabguGNvn698OXAqKLqSdEJEt7MJuhP7I2BQZrk/8LtvaAZ9sUiGcQXbXQ6QK8wNoUGKxN/i/Y2gMebFPQn0EYY4zpnlA4gzDGGNMNFhDGGGPcsoAwxhjjVkgHhIicJiJPicjfRGS10/V4goiEichDIvKYiFzvdD2eICKzReQj1/dqttP1eIKI9BaRLBGZ43QtniAiY1zfn2UicrvT9XiCiFwiIn8VkX+IyLlO1+MJIjJMRJ4WkWWd2T9gA0JEnhGRQhHZ0mb7+SKyQ0RyROSejt5DVT9S1duAfwHPebPezvBEm4C5QBpQD+R5q9bO8lCbFKgConG4TR5qD8B/AUu9U2XXeOhnaZvrZ+lK4BRv1tsZHmrTP1X1FuA2YL436+0MD7UpV1Vv7vQxA/UqJhE5neZfGn9X1fGubeHATuAcmn+RrAeuBsKB37Z5i5tUtdD1uqXAzapa6aPy3fJEm1wfR1R1gYgsU9XLfVW/Ox5qU7GqNonIAOCPqnqtr+pvy0PtmQT0oznwilX1X76p3j1P/SyJyMXA7cD/qeoLvqrfHQ//fvgDsFhVN/qofLc83KZO/W6I8Fz5vqWqq0Qko83m6UCOquYCiMgSYK6q/hZweyovIkOAcqfDATzTJhHJA+pcDxu9V23neOr75HIE6OWNOjvLQ9+j2UBvYCxwTETeUtUmb9bdEU99j1T1deB1EXkTcDQgPPR9EuBh4G2nwwE8/rPUKQEbEO1IBQ60epwHnHyc19wMPOu1inquq216BXhMRE4DVnmzsB7oUptE5DLgPCAJeNy7pXVLl9qjqr8AEJEbcJ0debW67unq92g2cBnNAf6WVyvrvq7+LN0JnA0kisiJqvqUN4vrpq5+n/oBDwFTROReV5C0K9gCostU9VdO1+BJqlpNc+gFDVV9hebgCyqqusjpGjxFVT8APnC4DI9S1UeBR52uw5NUtYTmMZVOCdhB6nYcBNJbPU5zbQtk1ib/F2ztAWtToPBqm4ItINYDI0RkqIhEAVcBrztcU09Zm/xfsLUHrE2Bwrtt8tS84b7+AF4ECvjqcs6bXdsvpHlUfzfwC6frtDYFV5uCrT3WJudr9ec2BexlrsYYY7wr2LqYjDHGeIgFhDHGGLcsIIwxxrhlAWGMMcYtCwhjjDFuWUAYY4xxywLCBCwRqfLx8TyyZog0r29RLiLZIrJdRH7fiddcIiJjPXF8YzrLAsIYFxHpcG4yVZ3lwcN9pKqTgSnAHBE53hoKl9A8+6sxPmMBYYKKiAwXkXdEZIM0r0I32rX9IhFZJyKfici/XWtLICL3i8j/icgnwP+5Hj8jIh+ISK6I3NXqvatcn2e7nl/mOgNY7JoaGhG50LVtg4g8KiIdrvWgqseAbJpn5UREbhGR9SKySUReFpFYEZkFXAz8znXWMby9dhrjSRYQJtgsBO5U1ZOAnwJ/cW3/GJihqlOAJcDPWr1mLHC2ql7tejya5unFpwO/EpFIN8eZAvzQ9dphwCkiEg0sAC5wHT/5eMWKSB9gBF9Nzf6Kqk5T1UnANpqnU1hN8/w6d6vqZFXd3UE7jfGYkJ/u2wQPEYkDZgEvuf6gh68WGEoD/iEig4AoYE+rl77u+ku+xZuqWgvUikghMID/XOr0U1XNcx03G8igebWvXFVtee8XgVvbKfc0EdlEczj8WVUPubaPF5EHaV77Ig54t4vtNMZjLCBMMAkDylx9+209RvNypa+7Fre5v9VzR9vsW9vq60bc/5x0Zp+OfKSqc0RkKLBWRJaqajawCLhEVTe5FhSa7ea1HbXTGI+xLiYTNFS1AtgjIldA85KRIjLJ9XQiX82Tf72XStgBDGu1LORxF7p3nW08DPyXa1M8UODq1mq99nal67njtdMYj7GAMIEsVkTyWn38mOZfqje7um++AOa69r2f5i6ZDUCxN4pxdVPdAbzjOk4lUN6Jlz4FnO4Kll8C64BPgO2t9lkC3O0aZB9O++00xmNsum9jPEhE4lS1ynVV0xPALlX9k9N1GdMddgZhjGfd4hq0/oLmbq0FDtdjTLfZGYQxxhi37AzCGGOMWxYQxhhj3LKAMMYY45YFhDHGGLcsIIwxxrhlAWGMMcat/w+dQRAhZaFtFAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O80g59wl4OWP"
      },
      "source": [
        "learn.fit_one_cycle(1, 1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jLdK185aS-Rj",
        "outputId": "53e90c14-c3af-4854-d28d-bd046d488619"
      },
      "source": [
        "learn.validate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#2) [0.061653994023799896,1.0635943412780762]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ykj6ZRKtRXJ4"
      },
      "source": [
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf8VJ90FVhHi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492,
          "referenced_widgets": [
            "d76f27eb79524f0abeacb732099e08cd",
            "d16fddf0261c483bad5b7ed72afd66b8",
            "dbe21da0cf124579975a44dd5cbc8b85",
            "fff56ae7ca8649faa9ef8a0f920f8f1f",
            "d12261660a8f4b62a76d0edee9892ba0",
            "d08b79ce299145539b4f293e6c174221",
            "1ed545a19c0342b6b155354bf8cd82bb",
            "19235ce6da0c49a4b0bc3f26c8f425e6"
          ]
        },
        "outputId": "b938e53c-264b-4723-f66d-5809b5de6ba0"
      },
      "source": [
        "gen_summs = []\n",
        "#mod:sharmaa4 for paper in tqdm(scis_df.text.values[:500]):\n",
        "for paper in tqdm(scis_df.text.values[:5]):\n",
        "\n",
        "  ARTICLE_TO_SUMMARIZE = paper\n",
        "  inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt', truncation=True)\n",
        "\n",
        "  # Generate Summary\n",
        "  summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=500, early_stopping=True)\n",
        "  gen_summs.append([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d76f27eb79524f0abeacb732099e08cd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-4c3536e93b9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# Generate Summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0msummary_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mgen_summs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msummary_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0;31m# add encoder_outputs to model_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m             \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_encoder_decoder_kwargs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;31m# set input_ids as decoder_input_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, input_ids, model_kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"decoder_\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cross_attn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             }\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument index in method wrapper_index_select)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46ajA2DMQmo0"
      },
      "source": [
        "gen_df = pd.DataFrame({'generated_summary':gen_summs})\n",
        "gen_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJDc8ZK6QnN0"
      },
      "source": [
        "gen_df.generated_summary = gen_df.generated_summary.apply(lambda x: x[0])\n",
        "gen_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hxtRgzlQsYV"
      },
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge2', 'rouge3'])\n",
        "scores = []\n",
        "#mod: sharmaa4 for i in range(500):\n",
        "for i in range(5):\n",
        "  \n",
        "  scores.append(scorer.score(test_df.iloc[i].summary, gen_df.iloc[i].generated_summary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkMob8DWQyCz"
      },
      "source": [
        "r2r = []\n",
        "r2f = []\n",
        "r3f = []\n",
        "#mod:sharmaa4 for i in range(500):\n",
        "for i in range(5):\n",
        "  r2r.append(scores[i]['rouge2'].recall)\n",
        "  r2f.append(scores[i]['rouge2'].fmeasure)\n",
        "  r3f.append(scores[i]['rouge3'].fmeasure)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxziegVWQ2Be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3918633a-102d-44cd-88fd-f184cffb6f6a"
      },
      "source": [
        "print('rouge2 - recall: '+ str(np.mean(r2r)))\n",
        "print('rouge2 - fmeasure: '+ str(np.mean(r2f)))\n",
        "print('rouge3 - fmeasure: '+ str(np.mean(r3f)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rouge2 - recall: 0.4764333337127982\n",
            "rouge2 - fmeasure: 0.585565164569248\n",
            "rouge3 - fmeasure: 0.569256056991675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G40vKT3rQtrz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}